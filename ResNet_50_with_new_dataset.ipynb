{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaysinha1804/nailDiseasesAnalysis/blob/main/ResNet_50_with_new_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2cYkLHWItWw"
      },
      "source": [
        "# Importing Dependecies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rrjdaImGf4M"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D,GlobalAveragePooling2D\n",
        "from keras import regularizers, optimizers\n",
        "import numpy as np\n",
        "from keras.layers import Add\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.layers import Flatten\n",
        "from keras.utils import plot_model\n",
        "from keras.preprocessing import image\n",
        "import os\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from sklearn.utils import shuffle\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger,EarlyStopping,ModelCheckpoint\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras.utils as image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYxfkOzEIsN0",
        "outputId": "fa0d4054-dd5c-4fda-fbae-ac74937be822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTlPzPa6P0-t"
      },
      "source": [
        "# Define Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YPGLHfkGmtc",
        "outputId": "a90e9529-3935-46a4-d3fc-bbd223aa1a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PWD /content/drive/MyDrive/datasetnew/\n"
          ]
        }
      ],
      "source": [
        "PATH = \"/content/drive/MyDrive/datasetnew/\"\n",
        "METRICS_PATH=\"/content/drive/MyDrive/metro/\"\n",
        "MODEL_NAME=\"ResNet\"\n",
        "print(\"PWD\", PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj6JkdnQKH-6"
      },
      "source": [
        "# Define Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZskICu_8Gopr"
      },
      "outputs": [],
      "source": [
        "def sensitivity(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NUnI6O4Goni"
      },
      "outputs": [],
      "source": [
        "def specificity(y_true, y_pred):\n",
        "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
        "    false_positives = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
        "    return true_negatives / (true_negatives+false_positives + K.epsilon())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwzrX00JGolR"
      },
      "outputs": [],
      "source": [
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX_hZ-ODNAHz"
      },
      "source": [
        "# Read Images from Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgGgQB1BG4Od"
      },
      "outputs": [],
      "source": [
        "def sorted_alphanumeric(data):\n",
        "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
        "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
        "    return sorted(data, key=alphanum_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDGRsXoiN9Wf",
        "outputId": "a3b251fd-a0e0-423e-bedb-ca2fc8ee6b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Acral Lentiginous Melanoma', 'Bluish Nail', 'Clubbing', 'Healthy Nail', 'Koilonychia', 'Nail Pitting', 'Onychogryphosis', 'onycholysis']\n"
          ]
        }
      ],
      "source": [
        "data_path = PATH\n",
        "data_dir_list = sorted_alphanumeric(os.listdir(data_path))\n",
        "print(data_dir_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQMNxx5yG4HY",
        "outputId": "7318394a-8ba9-4ed3-da23-624c539b61b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded the images of dataset-Acral Lentiginous Melanoma\n",
            "\n",
            "Loaded the images of dataset-Bluish Nail\n",
            "\n",
            "Loaded the images of dataset-Clubbing\n",
            "\n",
            "Loaded the images of dataset-Healthy Nail\n",
            "\n",
            "Loaded the images of dataset-Koilonychia\n",
            "\n",
            "Loaded the images of dataset-Nail Pitting\n",
            "\n",
            "Loaded the images of dataset-Onychogryphosis\n",
            "\n",
            "Loaded the images of dataset-onycholysis\n",
            "\n"
          ]
        }
      ],
      "source": [
        "img_data_list = []\n",
        "\n",
        "for dataset in data_dir_list:\n",
        "    img_list = sorted_alphanumeric(os.listdir(data_path + '/' + dataset))\n",
        "    print('Loaded the images of dataset-' + '{}\\n'.format(dataset))\n",
        "    for img in img_list:\n",
        "        # print(img)\n",
        "        img_path = data_path + '/' + dataset + '/' + img\n",
        "        img = image.load_img(img_path, target_size=(32,32))\n",
        "        x = image.img_to_array(img)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "        x = preprocess_input(x)\n",
        "        #     x = x/255\n",
        "        # print('Input image shape:', x.shape)\n",
        "        img_data_list.append(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DnkHKm5G4FS",
        "outputId": "4b9a717d-f87a-4e75-c061-5660b02acf65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8033, 1, 32, 32, 3)\n",
            "(1, 8033, 32, 32, 3)\n",
            "(8033, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "img_data = np.array(img_data_list)\n",
        "# img_data = img_data.astype('float32')\n",
        "print(img_data.shape)\n",
        "img_data = np.rollaxis(img_data, 1, 0)\n",
        "print(img_data.shape)\n",
        "img_data = img_data[0]\n",
        "print(img_data.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny8pdS-NNEhL"
      },
      "source": [
        "# Train & Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEymJW76JQTv",
        "outputId": "e4cd8cae-ef03-4802-d7aa-6a0e0356113e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample 8033\n"
          ]
        }
      ],
      "source": [
        "num_classes = 8\n",
        "num_of_samples = img_data.shape[0]\n",
        "print(\"sample\", num_of_samples)\n",
        "labels = np.ones((num_of_samples,), dtype='int64')\n",
        "labels[0:859] = 0\n",
        "labels[859:2191] = 1\n",
        "labels[2191:3380] = 2\n",
        "labels[3380:4222] = 3\n",
        "labels[4222:5121] = 4\n",
        "labels[5121:5865] = 5\n",
        "labels[5865:7269] = 6\n",
        "labels[7269:8033] = 7\n",
        "\n",
        "names = ['Acral Lentiginous Melanoma', 'Bluish Nail', 'Clubbing', 'Healthy Nail', 'Koilonychia', 'Nail Pitting', 'Onychogryphosis', 'onycholysis']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2i6ejV-HEl4"
      },
      "outputs": [],
      "source": [
        "Y = to_categorical(labels, num_classes)\n",
        "x, y = shuffle(img_data, Y, random_state=2)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yetl9cqpNLCp"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7BpPGd-HXzC"
      },
      "outputs": [],
      "source": [
        "def initial_conv(Input, filters, stride=1, kernel_size=7):\n",
        "    x = Conv2D(filters, kernel_size=(kernel_size, kernel_size), strides=(stride, stride), padding=\"same\")(Input)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv_basic_block(Input, filters, stride=1, dropout=0.0):\n",
        "    Init = Input\n",
        "\n",
        "    # First conv which is used to downsample the image\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(Input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Optional Dropout layer\n",
        "    if (dropout > 0.0):\n",
        "        x = Dropout(dropout)(x)\n",
        "\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Projection shortcut to make skip connection(Paper terminology)\n",
        "    skip_conv = Conv2D(filters, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n",
        "    skip = BatchNormalization()(skip_conv)\n",
        "\n",
        "    # Skip connection\n",
        "    x = Add()([x, skip])\n",
        "    return x\n",
        "\n",
        "\n",
        "def normal_conv_basic_block(Input, filters, stride=1, dropout=0.0):\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(Input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Optional Dropout layer\n",
        "    if (dropout > 0.0):\n",
        "        x = Dropout(dropout)(x)\n",
        "\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Identity skip connection\n",
        "    x = Add()([x, Input])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv_bottleneck_block(Input, filters, stride=1, dropout=0.0):\n",
        "    # Contracting 1*1 conv\n",
        "    x = Conv2D(filters, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # if(dropout > 0.0):\n",
        "    #   x = Dropout(dropout)(x)\n",
        "\n",
        "    # Depth preserving 3*3 conv\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # if(Dropout > 0.0):\n",
        "    #   x = Dropout(dropout)(x)\n",
        "\n",
        "    # Expanding 1*1 Conv\n",
        "    x = Conv2D(filters * 4, kernel_size=(1, 1), strides=(1, 1), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Projection shortcut\n",
        "    skip_conv = Conv2D(filters * 4, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n",
        "    skip = BatchNormalization()(skip_conv)\n",
        "\n",
        "    # Skip connection\n",
        "    x = Add()([x, skip])\n",
        "\n",
        "    return x\n",
        "\n",
        "def normal_conv_bottleneck_block(Input, filters, stride=1, dropout=0.0):\n",
        "    # Contracting 1*1 conv\n",
        "    x = Conv2D(filters, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # if(dropout > 0.0):\n",
        "    #   x = Dropout(dropout)(x)\n",
        "\n",
        "    # Depth preserving 3*3 Conv\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # if(Dropout > 0.0):\n",
        "    #    x = Dropout(dropout)(x)\n",
        "\n",
        "    # Expanding 1*1 Conv\n",
        "    x = Conv2D(filters * 4, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Identity skip connection\n",
        "    x = Add()([x, Input])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_basic_resnet(h, w, no_of_outputs, r1, r2, r3, r4, first_conv_stride=2, first_max_pool=True,\n",
        "                       first_conv_kernel_size=7):\n",
        "    # Creating input tensor\n",
        "    inputs = Input(shape=(h, w, 3), name=\"image_input\")\n",
        "\n",
        "    # Inital Conv block\n",
        "    x = initial_conv(inputs, 64, first_conv_stride, first_conv_kernel_size)\n",
        "\n",
        "    # Optional Max pooling layer\n",
        "    if (first_max_pool):\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Expanding block1 with projection shortcut\n",
        "    x = expand_conv_basic_block(x, 64, 1)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv1\n",
        "    for i in range(r1 - 1):\n",
        "        x = normal_conv_basic_block(x, 64)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    # Expanding block2 with projection shortcut\n",
        "    x = expand_conv_basic_block(x, 128, 2)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv2\n",
        "    for i in range(r2 - 1):\n",
        "        x = normal_conv_basic_block(x, 128)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    # Expanding block3 with projection shortcut\n",
        "    x = expand_conv_basic_block(x, 256, 2)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv3\n",
        "    for i in range(r3 - 1):\n",
        "        x = normal_conv_basic_block(x, 256)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    # Expanding block4 with projection shortcut\n",
        "    x = expand_conv_basic_block(x, 512, 2)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv3\n",
        "    for i in range(r4 - 1):\n",
        "        x = normal_conv_basic_block(x, 512)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    shape = K.int_shape(x)\n",
        "\n",
        "    # Average pooling layer\n",
        "    x = AveragePooling2D(pool_size=(shape[1], shape[2]),\n",
        "                         strides=(1, 1))(x)\n",
        "    # x = GlobalAveragePooling2D()(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # Classifier Block\n",
        "    x = Dense(no_of_outputs, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_bottleneck_resnet(h, w, no_of_outputs, r1, r2, r3, r4, first_conv_stride=2, first_max_pool=True,\n",
        "                            first_conv_kernel_size=7):\n",
        "    # Creating input tensor\n",
        "    inputs = Input(shape=(h, w, 3), name=\"image_input\")\n",
        "\n",
        "    # Inital Conv block\n",
        "    x = initial_conv(inputs, 64, first_conv_stride, first_conv_kernel_size)\n",
        "\n",
        "    # Optional Max pooling layer\n",
        "    if (first_max_pool):\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Expanding block1 with projection shortcut\n",
        "    x = expand_conv_bottleneck_block(x, 64, 1)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv1\n",
        "    for i in range(r1 - 1):\n",
        "        x = normal_conv_bottleneck_block(x, 64)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    # Expanding block2 with projection shortcut\n",
        "    x = expand_conv_bottleneck_block(x, 128, 2)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv2\n",
        "    for i in range(r2 - 1):\n",
        "        x = normal_conv_bottleneck_block(x, 128)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    # Expanding block3 with projection shortcut\n",
        "    x = expand_conv_bottleneck_block(x, 256, 2)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv3\n",
        "    for i in range(r3 - 1):\n",
        "        x = normal_conv_bottleneck_block(x, 256)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    # Expanding block4 with projection shortcut\n",
        "    x = expand_conv_bottleneck_block(x, 512, 2)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv4\n",
        "    for i in range(r4 - 1):\n",
        "        x = normal_conv_bottleneck_block(x, 512)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    shape = K.int_shape(x)\n",
        "\n",
        "    # Average pooling layer\n",
        "    x = AveragePooling2D(pool_size=(shape[1], shape[2]),\n",
        "                         strides=(1, 1))(x)\n",
        "    # x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Classifier Block\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(no_of_outputs, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZfeGbU4J3aB",
        "outputId": "2c3dd1e2-a192-4bdf-dc00-2d4bdefe5ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " image_input (InputLayer)    [(None, 32, 32, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)          (None, 16, 16, 64)           9472      ['image_input[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_53 (Ba  (None, 16, 16, 64)           256       ['conv2d_53[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_49 (Activation)  (None, 16, 16, 64)           0         ['batch_normalization_53[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 64)             0         ['activation_49[0][0]']       \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)          (None, 8, 8, 64)             4160      ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_54 (Ba  (None, 8, 8, 64)             256       ['conv2d_54[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_50 (Activation)  (None, 8, 8, 64)             0         ['batch_normalization_54[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)          (None, 8, 8, 64)             36928     ['activation_50[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_55 (Ba  (None, 8, 8, 64)             256       ['conv2d_55[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_51 (Activation)  (None, 8, 8, 64)             0         ['batch_normalization_55[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)          (None, 8, 8, 256)            16640     ['activation_51[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)          (None, 8, 8, 256)            16640     ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_56 (Ba  (None, 8, 8, 256)            1024      ['conv2d_56[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_57 (Ba  (None, 8, 8, 256)            1024      ['conv2d_57[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_16 (Add)                (None, 8, 8, 256)            0         ['batch_normalization_56[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'batch_normalization_57[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_52 (Activation)  (None, 8, 8, 256)            0         ['add_16[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)          (None, 8, 8, 64)             16448     ['activation_52[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_58 (Ba  (None, 8, 8, 64)             256       ['conv2d_58[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_53 (Activation)  (None, 8, 8, 64)             0         ['batch_normalization_58[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)          (None, 8, 8, 64)             36928     ['activation_53[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_59 (Ba  (None, 8, 8, 64)             256       ['conv2d_59[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_54 (Activation)  (None, 8, 8, 64)             0         ['batch_normalization_59[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)          (None, 8, 8, 256)            16640     ['activation_54[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_60 (Ba  (None, 8, 8, 256)            1024      ['conv2d_60[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_17 (Add)                (None, 8, 8, 256)            0         ['batch_normalization_60[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_52[0][0]']       \n",
            "                                                                                                  \n",
            " activation_55 (Activation)  (None, 8, 8, 256)            0         ['add_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)          (None, 8, 8, 64)             16448     ['activation_55[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_61 (Ba  (None, 8, 8, 64)             256       ['conv2d_61[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_56 (Activation)  (None, 8, 8, 64)             0         ['batch_normalization_61[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)          (None, 8, 8, 64)             36928     ['activation_56[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_62 (Ba  (None, 8, 8, 64)             256       ['conv2d_62[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_57 (Activation)  (None, 8, 8, 64)             0         ['batch_normalization_62[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)          (None, 8, 8, 256)            16640     ['activation_57[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_63 (Ba  (None, 8, 8, 256)            1024      ['conv2d_63[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_18 (Add)                (None, 8, 8, 256)            0         ['batch_normalization_63[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_55[0][0]']       \n",
            "                                                                                                  \n",
            " activation_58 (Activation)  (None, 8, 8, 256)            0         ['add_18[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)          (None, 4, 4, 128)            32896     ['activation_58[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_64 (Ba  (None, 4, 4, 128)            512       ['conv2d_64[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_59 (Activation)  (None, 4, 4, 128)            0         ['batch_normalization_64[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)          (None, 4, 4, 128)            147584    ['activation_59[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_65 (Ba  (None, 4, 4, 128)            512       ['conv2d_65[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_60 (Activation)  (None, 4, 4, 128)            0         ['batch_normalization_65[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)          (None, 4, 4, 512)            66048     ['activation_60[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)          (None, 4, 4, 512)            131584    ['activation_58[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_66 (Ba  (None, 4, 4, 512)            2048      ['conv2d_66[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_67 (Ba  (None, 4, 4, 512)            2048      ['conv2d_67[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_19 (Add)                (None, 4, 4, 512)            0         ['batch_normalization_66[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'batch_normalization_67[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_61 (Activation)  (None, 4, 4, 512)            0         ['add_19[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)          (None, 4, 4, 128)            65664     ['activation_61[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_68 (Ba  (None, 4, 4, 128)            512       ['conv2d_68[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_62 (Activation)  (None, 4, 4, 128)            0         ['batch_normalization_68[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_69 (Conv2D)          (None, 4, 4, 128)            147584    ['activation_62[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_69 (Ba  (None, 4, 4, 128)            512       ['conv2d_69[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_63 (Activation)  (None, 4, 4, 128)            0         ['batch_normalization_69[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_70 (Conv2D)          (None, 4, 4, 512)            66048     ['activation_63[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_70 (Ba  (None, 4, 4, 512)            2048      ['conv2d_70[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_20 (Add)                (None, 4, 4, 512)            0         ['batch_normalization_70[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_61[0][0]']       \n",
            "                                                                                                  \n",
            " activation_64 (Activation)  (None, 4, 4, 512)            0         ['add_20[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_71 (Conv2D)          (None, 4, 4, 128)            65664     ['activation_64[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_71 (Ba  (None, 4, 4, 128)            512       ['conv2d_71[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_65 (Activation)  (None, 4, 4, 128)            0         ['batch_normalization_71[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_72 (Conv2D)          (None, 4, 4, 128)            147584    ['activation_65[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_72 (Ba  (None, 4, 4, 128)            512       ['conv2d_72[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_66 (Activation)  (None, 4, 4, 128)            0         ['batch_normalization_72[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_73 (Conv2D)          (None, 4, 4, 512)            66048     ['activation_66[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_73 (Ba  (None, 4, 4, 512)            2048      ['conv2d_73[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_21 (Add)                (None, 4, 4, 512)            0         ['batch_normalization_73[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_64[0][0]']       \n",
            "                                                                                                  \n",
            " activation_67 (Activation)  (None, 4, 4, 512)            0         ['add_21[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_74 (Conv2D)          (None, 4, 4, 128)            65664     ['activation_67[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_74 (Ba  (None, 4, 4, 128)            512       ['conv2d_74[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_68 (Activation)  (None, 4, 4, 128)            0         ['batch_normalization_74[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_75 (Conv2D)          (None, 4, 4, 128)            147584    ['activation_68[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_75 (Ba  (None, 4, 4, 128)            512       ['conv2d_75[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_69 (Activation)  (None, 4, 4, 128)            0         ['batch_normalization_75[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_76 (Conv2D)          (None, 4, 4, 512)            66048     ['activation_69[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_76 (Ba  (None, 4, 4, 512)            2048      ['conv2d_76[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_22 (Add)                (None, 4, 4, 512)            0         ['batch_normalization_76[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_67[0][0]']       \n",
            "                                                                                                  \n",
            " activation_70 (Activation)  (None, 4, 4, 512)            0         ['add_22[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_77 (Conv2D)          (None, 2, 2, 256)            131328    ['activation_70[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_77 (Ba  (None, 2, 2, 256)            1024      ['conv2d_77[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_71 (Activation)  (None, 2, 2, 256)            0         ['batch_normalization_77[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_78 (Conv2D)          (None, 2, 2, 256)            590080    ['activation_71[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_78 (Ba  (None, 2, 2, 256)            1024      ['conv2d_78[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_72 (Activation)  (None, 2, 2, 256)            0         ['batch_normalization_78[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_79 (Conv2D)          (None, 2, 2, 1024)           263168    ['activation_72[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_80 (Conv2D)          (None, 2, 2, 1024)           525312    ['activation_70[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_79 (Ba  (None, 2, 2, 1024)           4096      ['conv2d_79[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_80 (Ba  (None, 2, 2, 1024)           4096      ['conv2d_80[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_23 (Add)                (None, 2, 2, 1024)           0         ['batch_normalization_79[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'batch_normalization_80[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_73 (Activation)  (None, 2, 2, 1024)           0         ['add_23[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_81 (Conv2D)          (None, 2, 2, 256)            262400    ['activation_73[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_81 (Ba  (None, 2, 2, 256)            1024      ['conv2d_81[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_74 (Activation)  (None, 2, 2, 256)            0         ['batch_normalization_81[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_82 (Conv2D)          (None, 2, 2, 256)            590080    ['activation_74[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_82 (Ba  (None, 2, 2, 256)            1024      ['conv2d_82[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_75 (Activation)  (None, 2, 2, 256)            0         ['batch_normalization_82[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_83 (Conv2D)          (None, 2, 2, 1024)           263168    ['activation_75[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_83 (Ba  (None, 2, 2, 1024)           4096      ['conv2d_83[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_24 (Add)                (None, 2, 2, 1024)           0         ['batch_normalization_83[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_73[0][0]']       \n",
            "                                                                                                  \n",
            " activation_76 (Activation)  (None, 2, 2, 1024)           0         ['add_24[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_84 (Conv2D)          (None, 2, 2, 256)            262400    ['activation_76[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_84 (Ba  (None, 2, 2, 256)            1024      ['conv2d_84[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_77 (Activation)  (None, 2, 2, 256)            0         ['batch_normalization_84[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_85 (Conv2D)          (None, 2, 2, 256)            590080    ['activation_77[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_85 (Ba  (None, 2, 2, 256)            1024      ['conv2d_85[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_78 (Activation)  (None, 2, 2, 256)            0         ['batch_normalization_85[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_86 (Conv2D)          (None, 2, 2, 1024)           263168    ['activation_78[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_86 (Ba  (None, 2, 2, 1024)           4096      ['conv2d_86[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_25 (Add)                (None, 2, 2, 1024)           0         ['batch_normalization_86[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_76[0][0]']       \n",
            "                                                                                                  \n",
            " activation_79 (Activation)  (None, 2, 2, 1024)           0         ['add_25[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_87 (Conv2D)          (None, 2, 2, 256)            262400    ['activation_79[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_87 (Ba  (None, 2, 2, 256)            1024      ['conv2d_87[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_80 (Activation)  (None, 2, 2, 256)            0         ['batch_normalization_87[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_88 (Conv2D)          (None, 2, 2, 256)            590080    ['activation_80[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_88 (Ba  (None, 2, 2, 256)            1024      ['conv2d_88[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_81 (Activation)  (None, 2, 2, 256)            0         ['batch_normalization_88[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_89 (Conv2D)          (None, 2, 2, 1024)           263168    ['activation_81[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_89 (Ba  (None, 2, 2, 1024)           4096      ['conv2d_89[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_26 (Add)                (None, 2, 2, 1024)           0         ['batch_normalization_89[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_79[0][0]']       \n",
            "                                                                                                  \n",
            " activation_82 (Activation)  (None, 2, 2, 1024)           0         ['add_26[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_90 (Conv2D)          (None, 2, 2, 256)            262400    ['activation_82[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_90 (Ba  (None, 2, 2, 256)            1024      ['conv2d_90[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_83 (Activation)  (None, 2, 2, 256)            0         ['batch_normalization_90[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_91 (Conv2D)          (None, 2, 2, 256)            590080    ['activation_83[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_91 (Ba  (None, 2, 2, 256)            1024      ['conv2d_91[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_84 (Activation)  (None, 2, 2, 256)            0         ['batch_normalization_91[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_92 (Conv2D)          (None, 2, 2, 1024)           263168    ['activation_84[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_92 (Ba  (None, 2, 2, 1024)           4096      ['conv2d_92[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_27 (Add)                (None, 2, 2, 1024)           0         ['batch_normalization_92[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_82[0][0]']       \n",
            "                                                                                                  \n",
            " activation_85 (Activation)  (None, 2, 2, 1024)           0         ['add_27[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_93 (Conv2D)          (None, 2, 2, 256)            262400    ['activation_85[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_93 (Ba  (None, 2, 2, 256)            1024      ['conv2d_93[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_86 (Activation)  (None, 2, 2, 256)            0         ['batch_normalization_93[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_94 (Conv2D)          (None, 2, 2, 256)            590080    ['activation_86[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_94 (Ba  (None, 2, 2, 256)            1024      ['conv2d_94[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_87 (Activation)  (None, 2, 2, 256)            0         ['batch_normalization_94[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_95 (Conv2D)          (None, 2, 2, 1024)           263168    ['activation_87[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_95 (Ba  (None, 2, 2, 1024)           4096      ['conv2d_95[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_28 (Add)                (None, 2, 2, 1024)           0         ['batch_normalization_95[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'activation_85[0][0]']       \n",
            "                                                                                                  \n",
            " activation_88 (Activation)  (None, 2, 2, 1024)           0         ['add_28[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_96 (Conv2D)          (None, 1, 1, 512)            524800    ['activation_88[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_96 (Ba  (None, 1, 1, 512)            2048      ['conv2d_96[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_89 (Activation)  (None, 1, 1, 512)            0         ['batch_normalization_96[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_97 (Conv2D)          (None, 1, 1, 512)            2359808   ['activation_89[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_97 (Ba  (None, 1, 1, 512)            2048      ['conv2d_97[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_90 (Activation)  (None, 1, 1, 512)            0         ['batch_normalization_97[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " conv2d_98 (Conv2D)          (None, 1, 1, 2048)           1050624   ['activation_90[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_99 (Conv2D)          (None, 1, 1, 2048)           2099200   ['activation_88[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_98 (Ba  (None, 1, 1, 2048)           8192      ['conv2d_98[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_99 (Ba  (None, 1, 1, 2048)           8192      ['conv2d_99[0][0]']           \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " add_29 (Add)                (None, 1, 1, 2048)           0         ['batch_normalization_98[0][0]\n",
            "                                                                    ',                            \n",
            "                                                                     'batch_normalization_99[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_91 (Activation)  (None, 1, 1, 2048)           0         ['add_29[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_100 (Conv2D)         (None, 1, 1, 512)            1049088   ['activation_91[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_100 (B  (None, 1, 1, 512)            2048      ['conv2d_100[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_92 (Activation)  (None, 1, 1, 512)            0         ['batch_normalization_100[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " conv2d_101 (Conv2D)         (None, 1, 1, 512)            2359808   ['activation_92[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_101 (B  (None, 1, 1, 512)            2048      ['conv2d_101[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_93 (Activation)  (None, 1, 1, 512)            0         ['batch_normalization_101[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " conv2d_102 (Conv2D)         (None, 1, 1, 2048)           1050624   ['activation_93[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_102 (B  (None, 1, 1, 2048)           8192      ['conv2d_102[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_30 (Add)                (None, 1, 1, 2048)           0         ['batch_normalization_102[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'activation_91[0][0]']       \n",
            "                                                                                                  \n",
            " activation_94 (Activation)  (None, 1, 1, 2048)           0         ['add_30[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_103 (Conv2D)         (None, 1, 1, 512)            1049088   ['activation_94[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_103 (B  (None, 1, 1, 512)            2048      ['conv2d_103[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_95 (Activation)  (None, 1, 1, 512)            0         ['batch_normalization_103[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " conv2d_104 (Conv2D)         (None, 1, 1, 512)            2359808   ['activation_95[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_104 (B  (None, 1, 1, 512)            2048      ['conv2d_104[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_96 (Activation)  (None, 1, 1, 512)            0         ['batch_normalization_104[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " conv2d_105 (Conv2D)         (None, 1, 1, 2048)           1050624   ['activation_96[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_105 (B  (None, 1, 1, 2048)           8192      ['conv2d_105[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_31 (Add)                (None, 1, 1, 2048)           0         ['batch_normalization_105[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'activation_94[0][0]']       \n",
            "                                                                                                  \n",
            " activation_97 (Activation)  (None, 1, 1, 2048)           0         ['add_31[0][0]']              \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (Avera  (None, 1, 1, 2048)           0         ['activation_97[0][0]']       \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)         (None, 2048)                 0         ['average_pooling2d_1[0][0]'] \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 8)                    16392     ['flatten_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23604104 (90.04 MB)\n",
            "Trainable params: 23550984 (89.84 MB)\n",
            "Non-trainable params: 53120 (207.50 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = build_bottleneck_resnet(32,32,8,3,4,6,3,2,True,7)\n",
        "model.summary()\n",
        "plot_model(model,\"ResNet50.png\",show_shapes=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "        optimizer=\"Adam\",\n",
        "        metrics=['accuracy',f1,sensitivity,specificity])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS_PATH+MODEL_NAME+\".csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "baZJgX6_PrSq",
        "outputId": "b643e424-e7f5-47a5-ee62-b98db61c9a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/metro/ResNet.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_GW2KEVHXvh"
      },
      "outputs": [],
      "source": [
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.5e-6)\n",
        "csv_logger = CSVLogger(METRICS_PATH+MODEL_NAME+\".csv\")\n",
        "\n",
        "model_chekpoint = ModelCheckpoint(\"ResNet50_DA_aug.hdf5\",monitor = 'val_loss',verbose = 1,save_best_only=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vavqPMAvNWQQ"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMTEO8kRHEq3"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "data_augmentation = True\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m2mVaSaHxVN",
        "outputId": "fbace7b5-612b-46a0-cc8c-47a749756ecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------Using Data augmentation------------\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-2c13fe4e579f>:14: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "401/401 [==============================] - ETA: 0s - loss: 2.6334 - accuracy: 0.2796 - f1: 0.1283 - sensitivity: 0.0854 - specificity: 0.9779\n",
            "Epoch 1: val_loss improved from inf to 2.68616, saving model to ResNet50_DA_aug.hdf5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r401/401 [==============================] - 73s 83ms/step - loss: 2.6334 - accuracy: 0.2796 - f1: 0.1283 - sensitivity: 0.0854 - specificity: 0.9779 - val_loss: 2.6862 - val_accuracy: 0.3528 - val_f1: 0.2973 - val_sensitivity: 0.2222 - val_specificity: 0.9626 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 2.3968 - accuracy: 0.2573 - f1: 0.1123 - sensitivity: 0.0733 - specificity: 0.9839\n",
            "Epoch 2: val_loss did not improve from 2.68616\n",
            "401/401 [==============================] - 24s 61ms/step - loss: 2.3951 - accuracy: 0.2576 - f1: 0.1123 - sensitivity: 0.0733 - specificity: 0.9839 - val_loss: 3.5196 - val_accuracy: 0.2576 - val_f1: 0.1328 - val_sensitivity: 0.0804 - val_specificity: 0.9849 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 2.4488 - accuracy: 0.2159 - f1: 0.0385 - sensitivity: 0.0223 - specificity: 0.9889\n",
            "Epoch 3: val_loss improved from 2.68616 to 2.15302, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 26s 65ms/step - loss: 2.4488 - accuracy: 0.2159 - f1: 0.0385 - sensitivity: 0.0223 - specificity: 0.9889 - val_loss: 2.1530 - val_accuracy: 0.1425 - val_f1: 0.0012 - val_sensitivity: 6.1275e-04 - val_specificity: 0.9998 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 2.4472 - accuracy: 0.1863 - f1: 0.0205 - sensitivity: 0.0115 - specificity: 0.9903\n",
            "Epoch 4: val_loss did not improve from 2.15302\n",
            "401/401 [==============================] - 23s 58ms/step - loss: 2.4472 - accuracy: 0.1863 - f1: 0.0205 - sensitivity: 0.0115 - specificity: 0.9903 - val_loss: 11.3425 - val_accuracy: 0.1512 - val_f1: 0.1233 - val_sensitivity: 0.0882 - val_specificity: 0.9540 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 2.4699 - accuracy: 0.2002 - f1: 0.0253 - sensitivity: 0.0142 - specificity: 0.9906\n",
            "Epoch 5: val_loss improved from 2.15302 to 2.03709, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 26s 66ms/step - loss: 2.4699 - accuracy: 0.2002 - f1: 0.0253 - sensitivity: 0.0142 - specificity: 0.9906 - val_loss: 2.0371 - val_accuracy: 0.2159 - val_f1: 0.0491 - val_sensitivity: 0.0276 - val_specificity: 0.9910 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 2.4349 - accuracy: 0.2192 - f1: 0.0297 - sensitivity: 0.0169 - specificity: 0.9898\n",
            "Epoch 6: val_loss did not improve from 2.03709\n",
            "401/401 [==============================] - 25s 62ms/step - loss: 2.4335 - accuracy: 0.2195 - f1: 0.0301 - sensitivity: 0.0171 - specificity: 0.9898 - val_loss: 3.9163 - val_accuracy: 0.2502 - val_f1: 0.1566 - val_sensitivity: 0.1055 - val_specificity: 0.9684 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 2.3833 - accuracy: 0.2206 - f1: 0.0412 - sensitivity: 0.0232 - specificity: 0.9912\n",
            "Epoch 7: val_loss did not improve from 2.03709\n",
            "401/401 [==============================] - 23s 57ms/step - loss: 2.3833 - accuracy: 0.2206 - f1: 0.0412 - sensitivity: 0.0232 - specificity: 0.9912 - val_loss: 3.9669 - val_accuracy: 0.2371 - val_f1: 0.0148 - val_sensitivity: 0.0080 - val_specificity: 0.9930 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 2.0141 - accuracy: 0.2526 - f1: 0.0316 - sensitivity: 0.0171 - specificity: 0.9946\n",
            "Epoch 8: val_loss improved from 2.03709 to 2.01167, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 25s 63ms/step - loss: 2.0141 - accuracy: 0.2526 - f1: 0.0316 - sensitivity: 0.0171 - specificity: 0.9946 - val_loss: 2.0117 - val_accuracy: 0.2638 - val_f1: 0.0351 - val_sensitivity: 0.0190 - val_specificity: 0.9946 - lr: 3.1623e-04\n",
            "Epoch 9/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.9868 - accuracy: 0.2676 - f1: 0.0385 - sensitivity: 0.0213 - specificity: 0.9937\n",
            "Epoch 9: val_loss did not improve from 2.01167\n",
            "401/401 [==============================] - 23s 57ms/step - loss: 1.9868 - accuracy: 0.2676 - f1: 0.0385 - sensitivity: 0.0213 - specificity: 0.9937 - val_loss: 3.4985 - val_accuracy: 0.2601 - val_f1: 0.0259 - val_sensitivity: 0.0147 - val_specificity: 0.9846 - lr: 3.1623e-04\n",
            "Epoch 10/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.9451 - accuracy: 0.2922 - f1: 0.0459 - sensitivity: 0.0256 - specificity: 0.9939\n",
            "Epoch 10: val_loss did not improve from 2.01167\n",
            "401/401 [==============================] - 24s 59ms/step - loss: 1.9451 - accuracy: 0.2922 - f1: 0.0459 - sensitivity: 0.0256 - specificity: 0.9939 - val_loss: 2.0403 - val_accuracy: 0.2850 - val_f1: 0.0149 - val_sensitivity: 0.0080 - val_specificity: 0.9944 - lr: 3.1623e-04\n",
            "Epoch 11/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.8555 - accuracy: 0.3098 - f1: 0.0624 - sensitivity: 0.0355 - specificity: 0.9942\n",
            "Epoch 11: val_loss did not improve from 2.01167\n",
            "401/401 [==============================] - 25s 62ms/step - loss: 1.8555 - accuracy: 0.3098 - f1: 0.0624 - sensitivity: 0.0355 - specificity: 0.9942 - val_loss: 2.1055 - val_accuracy: 0.3192 - val_f1: 0.0986 - val_sensitivity: 0.0571 - val_specificity: 0.9896 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.8126 - accuracy: 0.3269 - f1: 0.0894 - sensitivity: 0.0512 - specificity: 0.9935\n",
            "Epoch 12: val_loss improved from 2.01167 to 1.82144, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 26s 64ms/step - loss: 1.8127 - accuracy: 0.3268 - f1: 0.0892 - sensitivity: 0.0511 - specificity: 0.9935 - val_loss: 1.8214 - val_accuracy: 0.3485 - val_f1: 0.1321 - val_sensitivity: 0.0789 - val_specificity: 0.9893 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.7819 - accuracy: 0.3395 - f1: 0.1161 - sensitivity: 0.0683 - specificity: 0.9914\n",
            "Epoch 13: val_loss did not improve from 1.82144\n",
            "401/401 [==============================] - 22s 56ms/step - loss: 1.7819 - accuracy: 0.3395 - f1: 0.1161 - sensitivity: 0.0683 - specificity: 0.9914 - val_loss: 1.8315 - val_accuracy: 0.3752 - val_f1: 0.1841 - val_sensitivity: 0.1156 - val_specificity: 0.9855 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.7244 - accuracy: 0.3729 - f1: 0.1665 - sensitivity: 0.1024 - specificity: 0.9880\n",
            "Epoch 14: val_loss did not improve from 1.82144\n",
            "401/401 [==============================] - 23s 58ms/step - loss: 1.7244 - accuracy: 0.3729 - f1: 0.1665 - sensitivity: 0.1024 - specificity: 0.9880 - val_loss: 2.0408 - val_accuracy: 0.3634 - val_f1: 0.2314 - val_sensitivity: 0.1524 - val_specificity: 0.9804 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.6220 - accuracy: 0.4051 - f1: 0.2280 - sensitivity: 0.1449 - specificity: 0.9864\n",
            "Epoch 15: val_loss improved from 1.82144 to 1.63755, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 25s 64ms/step - loss: 1.6212 - accuracy: 0.4053 - f1: 0.2282 - sensitivity: 0.1450 - specificity: 0.9865 - val_loss: 1.6376 - val_accuracy: 0.4169 - val_f1: 0.2715 - val_sensitivity: 0.1781 - val_specificity: 0.9842 - lr: 3.1623e-05\n",
            "Epoch 16/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.5932 - accuracy: 0.4276 - f1: 0.2649 - sensitivity: 0.1740 - specificity: 0.9847\n",
            "Epoch 16: val_loss improved from 1.63755 to 1.56529, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 26s 65ms/step - loss: 1.5932 - accuracy: 0.4276 - f1: 0.2649 - sensitivity: 0.1740 - specificity: 0.9847 - val_loss: 1.5653 - val_accuracy: 0.4574 - val_f1: 0.2976 - val_sensitivity: 0.1959 - val_specificity: 0.9859 - lr: 3.1623e-05\n",
            "Epoch 17/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.5514 - accuracy: 0.4346 - f1: 0.2913 - sensitivity: 0.1948 - specificity: 0.9842\n",
            "Epoch 17: val_loss improved from 1.56529 to 1.53611, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 24s 60ms/step - loss: 1.5514 - accuracy: 0.4346 - f1: 0.2913 - sensitivity: 0.1948 - specificity: 0.9842 - val_loss: 1.5361 - val_accuracy: 0.4630 - val_f1: 0.3083 - val_sensitivity: 0.2018 - val_specificity: 0.9889 - lr: 3.1623e-05\n",
            "Epoch 18/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.5263 - accuracy: 0.4542 - f1: 0.3140 - sensitivity: 0.2117 - specificity: 0.9841\n",
            "Epoch 18: val_loss did not improve from 1.53611\n",
            "401/401 [==============================] - 26s 64ms/step - loss: 1.5257 - accuracy: 0.4543 - f1: 0.3143 - sensitivity: 0.2119 - specificity: 0.9841 - val_loss: 1.6750 - val_accuracy: 0.4636 - val_f1: 0.3352 - val_sensitivity: 0.2293 - val_specificity: 0.9819 - lr: 3.1623e-05\n",
            "Epoch 19/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.5151 - accuracy: 0.4463 - f1: 0.3162 - sensitivity: 0.2158 - specificity: 0.9831\n",
            "Epoch 19: val_loss did not improve from 1.53611\n",
            "401/401 [==============================] - 24s 59ms/step - loss: 1.5151 - accuracy: 0.4463 - f1: 0.3162 - sensitivity: 0.2158 - specificity: 0.9831 - val_loss: 1.6838 - val_accuracy: 0.4748 - val_f1: 0.3408 - val_sensitivity: 0.2379 - val_specificity: 0.9804 - lr: 3.1623e-05\n",
            "Epoch 20/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.4869 - accuracy: 0.4589 - f1: 0.3362 - sensitivity: 0.2304 - specificity: 0.9835\n",
            "Epoch 20: val_loss improved from 1.53611 to 1.48611, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 26s 65ms/step - loss: 1.4861 - accuracy: 0.4593 - f1: 0.3365 - sensitivity: 0.2306 - specificity: 0.9835 - val_loss: 1.4861 - val_accuracy: 0.5028 - val_f1: 0.3790 - val_sensitivity: 0.2692 - val_specificity: 0.9806 - lr: 1.0000e-05\n",
            "Epoch 21/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.4688 - accuracy: 0.4748 - f1: 0.3447 - sensitivity: 0.2367 - specificity: 0.9836\n",
            "Epoch 21: val_loss did not improve from 1.48611\n",
            "401/401 [==============================] - 24s 59ms/step - loss: 1.4689 - accuracy: 0.4747 - f1: 0.3450 - sensitivity: 0.2371 - specificity: 0.9836 - val_loss: 1.5544 - val_accuracy: 0.5047 - val_f1: 0.3835 - val_sensitivity: 0.2704 - val_specificity: 0.9821 - lr: 1.0000e-05\n",
            "Epoch 22/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.4572 - accuracy: 0.4690 - f1: 0.3527 - sensitivity: 0.2443 - specificity: 0.9834\n",
            "Epoch 22: val_loss improved from 1.48611 to 1.47102, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 28s 69ms/step - loss: 1.4573 - accuracy: 0.4691 - f1: 0.3529 - sensitivity: 0.2445 - specificity: 0.9834 - val_loss: 1.4710 - val_accuracy: 0.5165 - val_f1: 0.4010 - val_sensitivity: 0.2912 - val_specificity: 0.9792 - lr: 1.0000e-05\n",
            "Epoch 23/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.4488 - accuracy: 0.4810 - f1: 0.3571 - sensitivity: 0.2487 - specificity: 0.9827\n",
            "Epoch 23: val_loss did not improve from 1.47102\n",
            "401/401 [==============================] - 23s 58ms/step - loss: 1.4488 - accuracy: 0.4810 - f1: 0.3571 - sensitivity: 0.2487 - specificity: 0.9827 - val_loss: 1.5229 - val_accuracy: 0.5128 - val_f1: 0.3977 - val_sensitivity: 0.2839 - val_specificity: 0.9814 - lr: 1.0000e-05\n",
            "Epoch 24/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.4327 - accuracy: 0.4850 - f1: 0.3664 - sensitivity: 0.2556 - specificity: 0.9830\n",
            "Epoch 24: val_loss did not improve from 1.47102\n",
            "401/401 [==============================] - 25s 63ms/step - loss: 1.4327 - accuracy: 0.4850 - f1: 0.3664 - sensitivity: 0.2556 - specificity: 0.9830 - val_loss: 1.5829 - val_accuracy: 0.5152 - val_f1: 0.3978 - val_sensitivity: 0.2876 - val_specificity: 0.9796 - lr: 1.0000e-05\n",
            "Epoch 25/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.4190 - accuracy: 0.4941 - f1: 0.3847 - sensitivity: 0.2704 - specificity: 0.9834\n",
            "Epoch 25: val_loss did not improve from 1.47102\n",
            "401/401 [==============================] - 23s 57ms/step - loss: 1.4190 - accuracy: 0.4941 - f1: 0.3847 - sensitivity: 0.2704 - specificity: 0.9834 - val_loss: 1.5353 - val_accuracy: 0.5196 - val_f1: 0.4068 - val_sensitivity: 0.2961 - val_specificity: 0.9794 - lr: 3.1623e-06\n",
            "Epoch 26/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.4070 - accuracy: 0.4922 - f1: 0.3956 - sensitivity: 0.2818 - specificity: 0.9830\n",
            "Epoch 26: val_loss did not improve from 1.47102\n",
            "401/401 [==============================] - 25s 63ms/step - loss: 1.4070 - accuracy: 0.4922 - f1: 0.3956 - sensitivity: 0.2818 - specificity: 0.9830 - val_loss: 1.5169 - val_accuracy: 0.5215 - val_f1: 0.4172 - val_sensitivity: 0.3072 - val_specificity: 0.9782 - lr: 3.1623e-06\n",
            "Epoch 27/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3960 - accuracy: 0.4895 - f1: 0.3920 - sensitivity: 0.2803 - specificity: 0.9817\n",
            "Epoch 27: val_loss did not improve from 1.47102\n",
            "401/401 [==============================] - 23s 58ms/step - loss: 1.3960 - accuracy: 0.4895 - f1: 0.3920 - sensitivity: 0.2803 - specificity: 0.9817 - val_loss: 1.4896 - val_accuracy: 0.5196 - val_f1: 0.4065 - val_sensitivity: 0.2949 - val_specificity: 0.9796 - lr: 1.0000e-06\n",
            "Epoch 28/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3962 - accuracy: 0.5025 - f1: 0.3940 - sensitivity: 0.2802 - specificity: 0.9827\n",
            "Epoch 28: val_loss did not improve from 1.47102\n",
            "401/401 [==============================] - 24s 59ms/step - loss: 1.3961 - accuracy: 0.5028 - f1: 0.3942 - sensitivity: 0.2803 - specificity: 0.9827 - val_loss: 1.5033 - val_accuracy: 0.5227 - val_f1: 0.4104 - val_sensitivity: 0.2998 - val_specificity: 0.9791 - lr: 1.0000e-06\n",
            "Epoch 29/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.4149 - accuracy: 0.4944 - f1: 0.3820 - sensitivity: 0.2722 - specificity: 0.9816\n",
            "Epoch 29: val_loss did not improve from 1.47102\n",
            "401/401 [==============================] - 26s 64ms/step - loss: 1.4149 - accuracy: 0.4944 - f1: 0.3820 - sensitivity: 0.2722 - specificity: 0.9816 - val_loss: 1.4834 - val_accuracy: 0.5264 - val_f1: 0.4168 - val_sensitivity: 0.3059 - val_specificity: 0.9788 - lr: 5.0000e-07\n",
            "Epoch 30/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3978 - accuracy: 0.4992 - f1: 0.3936 - sensitivity: 0.2807 - specificity: 0.9818\n",
            "Epoch 30: val_loss improved from 1.47102 to 1.45529, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 28s 70ms/step - loss: 1.3978 - accuracy: 0.4992 - f1: 0.3936 - sensitivity: 0.2807 - specificity: 0.9818 - val_loss: 1.4553 - val_accuracy: 0.5289 - val_f1: 0.4167 - val_sensitivity: 0.3035 - val_specificity: 0.9802 - lr: 5.0000e-07\n",
            "Epoch 31/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.4055 - accuracy: 0.4936 - f1: 0.3971 - sensitivity: 0.2835 - specificity: 0.9822\n",
            "Epoch 31: val_loss did not improve from 1.45529\n",
            "401/401 [==============================] - 24s 59ms/step - loss: 1.4060 - accuracy: 0.4933 - f1: 0.3966 - sensitivity: 0.2831 - specificity: 0.9822 - val_loss: 1.4739 - val_accuracy: 0.5208 - val_f1: 0.4150 - val_sensitivity: 0.3035 - val_specificity: 0.9790 - lr: 5.0000e-07\n",
            "Epoch 32/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3901 - accuracy: 0.4998 - f1: 0.3988 - sensitivity: 0.2849 - specificity: 0.9825\n",
            "Epoch 32: val_loss did not improve from 1.45529\n",
            "401/401 [==============================] - 22s 56ms/step - loss: 1.3898 - accuracy: 0.5002 - f1: 0.3989 - sensitivity: 0.2849 - specificity: 0.9825 - val_loss: 1.4958 - val_accuracy: 0.5215 - val_f1: 0.4134 - val_sensitivity: 0.3010 - val_specificity: 0.9798 - lr: 5.0000e-07\n",
            "Epoch 33/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3931 - accuracy: 0.4958 - f1: 0.3885 - sensitivity: 0.2762 - specificity: 0.9820\n",
            "Epoch 33: val_loss improved from 1.45529 to 1.44691, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 24s 61ms/step - loss: 1.3931 - accuracy: 0.4958 - f1: 0.3885 - sensitivity: 0.2762 - specificity: 0.9820 - val_loss: 1.4469 - val_accuracy: 0.5271 - val_f1: 0.4121 - val_sensitivity: 0.2998 - val_specificity: 0.9799 - lr: 5.0000e-07\n",
            "Epoch 34/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3994 - accuracy: 0.4945 - f1: 0.3953 - sensitivity: 0.2835 - specificity: 0.9818\n",
            "Epoch 34: val_loss did not improve from 1.44691\n",
            "401/401 [==============================] - 23s 57ms/step - loss: 1.3994 - accuracy: 0.4945 - f1: 0.3953 - sensitivity: 0.2835 - specificity: 0.9818 - val_loss: 1.4569 - val_accuracy: 0.5271 - val_f1: 0.4140 - val_sensitivity: 0.3041 - val_specificity: 0.9785 - lr: 5.0000e-07\n",
            "Epoch 35/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3913 - accuracy: 0.5009 - f1: 0.3952 - sensitivity: 0.2834 - specificity: 0.9818\n",
            "Epoch 35: val_loss improved from 1.44691 to 1.44031, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 28s 71ms/step - loss: 1.3913 - accuracy: 0.5011 - f1: 0.3953 - sensitivity: 0.2835 - specificity: 0.9818 - val_loss: 1.4403 - val_accuracy: 0.5202 - val_f1: 0.4188 - val_sensitivity: 0.3041 - val_specificity: 0.9809 - lr: 5.0000e-07\n",
            "Epoch 36/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3887 - accuracy: 0.5027 - f1: 0.3970 - sensitivity: 0.2846 - specificity: 0.9821\n",
            "Epoch 36: val_loss improved from 1.44031 to 1.43260, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 25s 62ms/step - loss: 1.3887 - accuracy: 0.5027 - f1: 0.3970 - sensitivity: 0.2846 - specificity: 0.9821 - val_loss: 1.4326 - val_accuracy: 0.5258 - val_f1: 0.4206 - val_sensitivity: 0.3078 - val_specificity: 0.9797 - lr: 5.0000e-07\n",
            "Epoch 37/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3986 - accuracy: 0.4955 - f1: 0.3970 - sensitivity: 0.2839 - specificity: 0.9823\n",
            "Epoch 37: val_loss did not improve from 1.43260\n",
            "401/401 [==============================] - 27s 66ms/step - loss: 1.3984 - accuracy: 0.4958 - f1: 0.3965 - sensitivity: 0.2835 - specificity: 0.9823 - val_loss: 1.4421 - val_accuracy: 0.5264 - val_f1: 0.4176 - val_sensitivity: 0.3053 - val_specificity: 0.9795 - lr: 5.0000e-07\n",
            "Epoch 38/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3971 - accuracy: 0.4989 - f1: 0.3920 - sensitivity: 0.2802 - specificity: 0.9819\n",
            "Epoch 38: val_loss did not improve from 1.43260\n",
            "401/401 [==============================] - 24s 59ms/step - loss: 1.3968 - accuracy: 0.4989 - f1: 0.3923 - sensitivity: 0.2804 - specificity: 0.9819 - val_loss: 1.4499 - val_accuracy: 0.5246 - val_f1: 0.4159 - val_sensitivity: 0.3029 - val_specificity: 0.9801 - lr: 5.0000e-07\n",
            "Epoch 39/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3987 - accuracy: 0.4908 - f1: 0.3920 - sensitivity: 0.2786 - specificity: 0.9826\n",
            "Epoch 39: val_loss improved from 1.43260 to 1.43220, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 25s 63ms/step - loss: 1.4004 - accuracy: 0.4905 - f1: 0.3918 - sensitivity: 0.2785 - specificity: 0.9826 - val_loss: 1.4322 - val_accuracy: 0.5283 - val_f1: 0.4122 - val_sensitivity: 0.2986 - val_specificity: 0.9806 - lr: 5.0000e-07\n",
            "Epoch 40/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3851 - accuracy: 0.5033 - f1: 0.3995 - sensitivity: 0.2857 - specificity: 0.9824\n",
            "Epoch 40: val_loss did not improve from 1.43220\n",
            "401/401 [==============================] - 24s 59ms/step - loss: 1.3847 - accuracy: 0.5034 - f1: 0.3999 - sensitivity: 0.2859 - specificity: 0.9824 - val_loss: 1.4606 - val_accuracy: 0.5246 - val_f1: 0.4190 - val_sensitivity: 0.3065 - val_specificity: 0.9798 - lr: 5.0000e-07\n",
            "Epoch 41/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3978 - accuracy: 0.4980 - f1: 0.3922 - sensitivity: 0.2805 - specificity: 0.9815\n",
            "Epoch 41: val_loss did not improve from 1.43220\n",
            "401/401 [==============================] - 24s 59ms/step - loss: 1.3978 - accuracy: 0.4980 - f1: 0.3921 - sensitivity: 0.2804 - specificity: 0.9815 - val_loss: 1.4626 - val_accuracy: 0.5277 - val_f1: 0.4148 - val_sensitivity: 0.3029 - val_specificity: 0.9795 - lr: 5.0000e-07\n",
            "Epoch 42/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3828 - accuracy: 0.5000 - f1: 0.3996 - sensitivity: 0.2869 - specificity: 0.9815\n",
            "Epoch 42: val_loss did not improve from 1.43220\n",
            "401/401 [==============================] - 25s 62ms/step - loss: 1.3823 - accuracy: 0.5000 - f1: 0.3996 - sensitivity: 0.2868 - specificity: 0.9815 - val_loss: 1.4509 - val_accuracy: 0.5264 - val_f1: 0.4243 - val_sensitivity: 0.3127 - val_specificity: 0.9790 - lr: 5.0000e-07\n",
            "Epoch 43/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3733 - accuracy: 0.5067 - f1: 0.4011 - sensitivity: 0.2849 - specificity: 0.9833\n",
            "Epoch 43: val_loss improved from 1.43220 to 1.40898, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 25s 62ms/step - loss: 1.3733 - accuracy: 0.5067 - f1: 0.4011 - sensitivity: 0.2849 - specificity: 0.9833 - val_loss: 1.4090 - val_accuracy: 0.5296 - val_f1: 0.4194 - val_sensitivity: 0.3059 - val_specificity: 0.9803 - lr: 5.0000e-07\n",
            "Epoch 44/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.4001 - accuracy: 0.4975 - f1: 0.3964 - sensitivity: 0.2837 - specificity: 0.9817\n",
            "Epoch 44: val_loss did not improve from 1.40898\n",
            "401/401 [==============================] - 25s 63ms/step - loss: 1.4003 - accuracy: 0.4977 - f1: 0.3966 - sensitivity: 0.2839 - specificity: 0.9817 - val_loss: 1.4482 - val_accuracy: 0.5327 - val_f1: 0.4176 - val_sensitivity: 0.3047 - val_specificity: 0.9801 - lr: 5.0000e-07\n",
            "Epoch 45/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3755 - accuracy: 0.5019 - f1: 0.3978 - sensitivity: 0.2839 - specificity: 0.9824\n",
            "Epoch 45: val_loss did not improve from 1.40898\n",
            "401/401 [==============================] - 24s 59ms/step - loss: 1.3755 - accuracy: 0.5019 - f1: 0.3978 - sensitivity: 0.2839 - specificity: 0.9824 - val_loss: 1.4742 - val_accuracy: 0.5240 - val_f1: 0.4166 - val_sensitivity: 0.3053 - val_specificity: 0.9788 - lr: 5.0000e-07\n",
            "Epoch 46/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3854 - accuracy: 0.5008 - f1: 0.4036 - sensitivity: 0.2909 - specificity: 0.9814\n",
            "Epoch 46: val_loss did not improve from 1.40898\n",
            "401/401 [==============================] - 23s 57ms/step - loss: 1.3854 - accuracy: 0.5008 - f1: 0.4036 - sensitivity: 0.2909 - specificity: 0.9814 - val_loss: 1.4216 - val_accuracy: 0.5308 - val_f1: 0.4212 - val_sensitivity: 0.3084 - val_specificity: 0.9796 - lr: 5.0000e-07\n",
            "Epoch 47/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3932 - accuracy: 0.5006 - f1: 0.3918 - sensitivity: 0.2806 - specificity: 0.9818\n",
            "Epoch 47: val_loss did not improve from 1.40898\n",
            "401/401 [==============================] - 24s 59ms/step - loss: 1.3933 - accuracy: 0.5002 - f1: 0.3916 - sensitivity: 0.2803 - specificity: 0.9818 - val_loss: 1.4490 - val_accuracy: 0.5252 - val_f1: 0.4122 - val_sensitivity: 0.3004 - val_specificity: 0.9795 - lr: 5.0000e-07\n",
            "Epoch 48/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3786 - accuracy: 0.4992 - f1: 0.4048 - sensitivity: 0.2897 - specificity: 0.9827\n",
            "Epoch 48: val_loss did not improve from 1.40898\n",
            "401/401 [==============================] - 26s 66ms/step - loss: 1.3781 - accuracy: 0.4994 - f1: 0.4054 - sensitivity: 0.2902 - specificity: 0.9827 - val_loss: 1.4621 - val_accuracy: 0.5264 - val_f1: 0.4172 - val_sensitivity: 0.3047 - val_specificity: 0.9797 - lr: 5.0000e-07\n",
            "Epoch 49/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3693 - accuracy: 0.5056 - f1: 0.4051 - sensitivity: 0.2898 - specificity: 0.9823\n",
            "Epoch 49: val_loss did not improve from 1.40898\n",
            "401/401 [==============================] - 24s 60ms/step - loss: 1.3693 - accuracy: 0.5056 - f1: 0.4051 - sensitivity: 0.2898 - specificity: 0.9823 - val_loss: 1.4445 - val_accuracy: 0.5320 - val_f1: 0.4212 - val_sensitivity: 0.3084 - val_specificity: 0.9798 - lr: 5.0000e-07\n",
            "Epoch 50/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3804 - accuracy: 0.5044 - f1: 0.4009 - sensitivity: 0.2867 - specificity: 0.9822\n",
            "Epoch 50: val_loss improved from 1.40898 to 1.40015, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 26s 64ms/step - loss: 1.3798 - accuracy: 0.5048 - f1: 0.4011 - sensitivity: 0.2868 - specificity: 0.9822 - val_loss: 1.4002 - val_accuracy: 0.5352 - val_f1: 0.4273 - val_sensitivity: 0.3133 - val_specificity: 0.9798 - lr: 5.0000e-07\n",
            "Epoch 51/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3803 - accuracy: 0.5036 - f1: 0.4009 - sensitivity: 0.2876 - specificity: 0.9817\n",
            "Epoch 51: val_loss did not improve from 1.40015\n",
            "401/401 [==============================] - 25s 63ms/step - loss: 1.3803 - accuracy: 0.5036 - f1: 0.4009 - sensitivity: 0.2876 - specificity: 0.9817 - val_loss: 1.4736 - val_accuracy: 0.5283 - val_f1: 0.4234 - val_sensitivity: 0.3090 - val_specificity: 0.9805 - lr: 5.0000e-07\n",
            "Epoch 52/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.4049 - accuracy: 0.4961 - f1: 0.3946 - sensitivity: 0.2826 - specificity: 0.9813\n",
            "Epoch 52: val_loss did not improve from 1.40015\n",
            "401/401 [==============================] - 23s 58ms/step - loss: 1.4038 - accuracy: 0.4967 - f1: 0.3947 - sensitivity: 0.2827 - specificity: 0.9813 - val_loss: 1.4434 - val_accuracy: 0.5289 - val_f1: 0.4245 - val_sensitivity: 0.3114 - val_specificity: 0.9799 - lr: 5.0000e-07\n",
            "Epoch 53/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3980 - accuracy: 0.4900 - f1: 0.3959 - sensitivity: 0.2836 - specificity: 0.9820\n",
            "Epoch 53: val_loss did not improve from 1.40015\n",
            "401/401 [==============================] - 32s 81ms/step - loss: 1.3974 - accuracy: 0.4902 - f1: 0.3960 - sensitivity: 0.2837 - specificity: 0.9819 - val_loss: 1.4732 - val_accuracy: 0.5320 - val_f1: 0.4208 - val_sensitivity: 0.3090 - val_specificity: 0.9790 - lr: 5.0000e-07\n",
            "Epoch 54/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3804 - accuracy: 0.5025 - f1: 0.4004 - sensitivity: 0.2861 - specificity: 0.9828\n",
            "Epoch 54: val_loss did not improve from 1.40015\n",
            "401/401 [==============================] - 24s 60ms/step - loss: 1.3794 - accuracy: 0.5028 - f1: 0.4008 - sensitivity: 0.2865 - specificity: 0.9828 - val_loss: 1.4584 - val_accuracy: 0.5327 - val_f1: 0.4267 - val_sensitivity: 0.3145 - val_specificity: 0.9791 - lr: 5.0000e-07\n",
            "Epoch 55/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3711 - accuracy: 0.5134 - f1: 0.4047 - sensitivity: 0.2904 - specificity: 0.9820\n",
            "Epoch 55: val_loss did not improve from 1.40015\n",
            "401/401 [==============================] - 27s 66ms/step - loss: 1.3711 - accuracy: 0.5134 - f1: 0.4047 - sensitivity: 0.2904 - specificity: 0.9820 - val_loss: 1.4010 - val_accuracy: 0.5383 - val_f1: 0.4235 - val_sensitivity: 0.3096 - val_specificity: 0.9800 - lr: 5.0000e-07\n",
            "Epoch 56/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3925 - accuracy: 0.4994 - f1: 0.3962 - sensitivity: 0.2836 - specificity: 0.9817\n",
            "Epoch 56: val_loss improved from 1.40015 to 1.38863, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 31s 77ms/step - loss: 1.3932 - accuracy: 0.4991 - f1: 0.3960 - sensitivity: 0.2835 - specificity: 0.9816 - val_loss: 1.3886 - val_accuracy: 0.5389 - val_f1: 0.4287 - val_sensitivity: 0.3127 - val_specificity: 0.9812 - lr: 5.0000e-07\n",
            "Epoch 57/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3806 - accuracy: 0.5022 - f1: 0.4102 - sensitivity: 0.2979 - specificity: 0.9809\n",
            "Epoch 57: val_loss did not improve from 1.38863\n",
            "401/401 [==============================] - 26s 64ms/step - loss: 1.3804 - accuracy: 0.5020 - f1: 0.4098 - sensitivity: 0.2976 - specificity: 0.9809 - val_loss: 1.4233 - val_accuracy: 0.5320 - val_f1: 0.4222 - val_sensitivity: 0.3084 - val_specificity: 0.9802 - lr: 5.0000e-07\n",
            "Epoch 58/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3866 - accuracy: 0.5039 - f1: 0.4072 - sensitivity: 0.2925 - specificity: 0.9820\n",
            "Epoch 58: val_loss did not improve from 1.38863\n",
            "401/401 [==============================] - 24s 60ms/step - loss: 1.3867 - accuracy: 0.5037 - f1: 0.4073 - sensitivity: 0.2926 - specificity: 0.9820 - val_loss: 1.4539 - val_accuracy: 0.5345 - val_f1: 0.4216 - val_sensitivity: 0.3096 - val_specificity: 0.9794 - lr: 5.0000e-07\n",
            "Epoch 59/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3762 - accuracy: 0.5000 - f1: 0.4048 - sensitivity: 0.2917 - specificity: 0.9814\n",
            "Epoch 59: val_loss did not improve from 1.38863\n",
            "401/401 [==============================] - 26s 66ms/step - loss: 1.3765 - accuracy: 0.5003 - f1: 0.4047 - sensitivity: 0.2916 - specificity: 0.9814 - val_loss: 1.4277 - val_accuracy: 0.5358 - val_f1: 0.4223 - val_sensitivity: 0.3090 - val_specificity: 0.9797 - lr: 5.0000e-07\n",
            "Epoch 60/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3799 - accuracy: 0.5050 - f1: 0.4082 - sensitivity: 0.2965 - specificity: 0.9805\n",
            "Epoch 60: val_loss did not improve from 1.38863\n",
            "401/401 [==============================] - 26s 64ms/step - loss: 1.3799 - accuracy: 0.5050 - f1: 0.4082 - sensitivity: 0.2965 - specificity: 0.9805 - val_loss: 1.4312 - val_accuracy: 0.5333 - val_f1: 0.4235 - val_sensitivity: 0.3090 - val_specificity: 0.9803 - lr: 5.0000e-07\n",
            "Epoch 61/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3765 - accuracy: 0.5081 - f1: 0.4024 - sensitivity: 0.2890 - specificity: 0.9823\n",
            "Epoch 61: val_loss did not improve from 1.38863\n",
            "401/401 [==============================] - 24s 59ms/step - loss: 1.3770 - accuracy: 0.5076 - f1: 0.4019 - sensitivity: 0.2886 - specificity: 0.9823 - val_loss: 1.4483 - val_accuracy: 0.5320 - val_f1: 0.4229 - val_sensitivity: 0.3084 - val_specificity: 0.9802 - lr: 5.0000e-07\n",
            "Epoch 62/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3749 - accuracy: 0.5081 - f1: 0.4047 - sensitivity: 0.2909 - specificity: 0.9818\n",
            "Epoch 62: val_loss did not improve from 1.38863\n",
            "401/401 [==============================] - 25s 63ms/step - loss: 1.3749 - accuracy: 0.5081 - f1: 0.4047 - sensitivity: 0.2909 - specificity: 0.9818 - val_loss: 1.4731 - val_accuracy: 0.5302 - val_f1: 0.4270 - val_sensitivity: 0.3139 - val_specificity: 0.9798 - lr: 5.0000e-07\n",
            "Epoch 63/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3811 - accuracy: 0.5014 - f1: 0.4031 - sensitivity: 0.2905 - specificity: 0.9819\n",
            "Epoch 63: val_loss did not improve from 1.38863\n",
            "401/401 [==============================] - 23s 57ms/step - loss: 1.3811 - accuracy: 0.5014 - f1: 0.4031 - sensitivity: 0.2905 - specificity: 0.9819 - val_loss: 1.4333 - val_accuracy: 0.5327 - val_f1: 0.4280 - val_sensitivity: 0.3133 - val_specificity: 0.9805 - lr: 5.0000e-07\n",
            "Epoch 64/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3702 - accuracy: 0.5030 - f1: 0.4067 - sensitivity: 0.2922 - specificity: 0.9820\n",
            "Epoch 64: val_loss did not improve from 1.38863\n",
            "401/401 [==============================] - 27s 67ms/step - loss: 1.3704 - accuracy: 0.5028 - f1: 0.4067 - sensitivity: 0.2922 - specificity: 0.9820 - val_loss: 1.4276 - val_accuracy: 0.5345 - val_f1: 0.4295 - val_sensitivity: 0.3164 - val_specificity: 0.9796 - lr: 5.0000e-07\n",
            "Epoch 65/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3790 - accuracy: 0.4994 - f1: 0.4062 - sensitivity: 0.2932 - specificity: 0.9813\n",
            "Epoch 65: val_loss improved from 1.38863 to 1.38697, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 27s 67ms/step - loss: 1.3792 - accuracy: 0.4992 - f1: 0.4059 - sensitivity: 0.2930 - specificity: 0.9813 - val_loss: 1.3870 - val_accuracy: 0.5345 - val_f1: 0.4231 - val_sensitivity: 0.3065 - val_specificity: 0.9816 - lr: 5.0000e-07\n",
            "Epoch 66/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3756 - accuracy: 0.5064 - f1: 0.4051 - sensitivity: 0.2929 - specificity: 0.9812\n",
            "Epoch 66: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 25s 63ms/step - loss: 1.3758 - accuracy: 0.5062 - f1: 0.4048 - sensitivity: 0.2926 - specificity: 0.9813 - val_loss: 1.4314 - val_accuracy: 0.5339 - val_f1: 0.4313 - val_sensitivity: 0.3188 - val_specificity: 0.9791 - lr: 5.0000e-07\n",
            "Epoch 67/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3673 - accuracy: 0.5150 - f1: 0.4141 - sensitivity: 0.2968 - specificity: 0.9829\n",
            "Epoch 67: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 24s 59ms/step - loss: 1.3673 - accuracy: 0.5150 - f1: 0.4141 - sensitivity: 0.2968 - specificity: 0.9829 - val_loss: 1.4256 - val_accuracy: 0.5308 - val_f1: 0.4268 - val_sensitivity: 0.3133 - val_specificity: 0.9799 - lr: 5.0000e-07\n",
            "Epoch 68/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3659 - accuracy: 0.5125 - f1: 0.4140 - sensitivity: 0.3003 - specificity: 0.9820\n",
            "Epoch 68: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 26s 64ms/step - loss: 1.3659 - accuracy: 0.5125 - f1: 0.4140 - sensitivity: 0.3003 - specificity: 0.9820 - val_loss: 1.4364 - val_accuracy: 0.5289 - val_f1: 0.4336 - val_sensitivity: 0.3206 - val_specificity: 0.9792 - lr: 5.0000e-07\n",
            "Epoch 69/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3616 - accuracy: 0.5106 - f1: 0.4114 - sensitivity: 0.2981 - specificity: 0.9808\n",
            "Epoch 69: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 25s 62ms/step - loss: 1.3616 - accuracy: 0.5106 - f1: 0.4114 - sensitivity: 0.2981 - specificity: 0.9808 - val_loss: 1.4194 - val_accuracy: 0.5376 - val_f1: 0.4372 - val_sensitivity: 0.3213 - val_specificity: 0.9805 - lr: 5.0000e-07\n",
            "Epoch 70/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3617 - accuracy: 0.5172 - f1: 0.4118 - sensitivity: 0.2968 - specificity: 0.9822\n",
            "Epoch 70: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 24s 61ms/step - loss: 1.3626 - accuracy: 0.5170 - f1: 0.4114 - sensitivity: 0.2965 - specificity: 0.9821 - val_loss: 1.4488 - val_accuracy: 0.5358 - val_f1: 0.4314 - val_sensitivity: 0.3164 - val_specificity: 0.9802 - lr: 5.0000e-07\n",
            "Epoch 71/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3692 - accuracy: 0.5077 - f1: 0.4082 - sensitivity: 0.2946 - specificity: 0.9820\n",
            "Epoch 71: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 31s 76ms/step - loss: 1.3691 - accuracy: 0.5080 - f1: 0.4085 - sensitivity: 0.2948 - specificity: 0.9820 - val_loss: 1.4506 - val_accuracy: 0.5395 - val_f1: 0.4344 - val_sensitivity: 0.3219 - val_specificity: 0.9790 - lr: 5.0000e-07\n",
            "Epoch 72/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3773 - accuracy: 0.5022 - f1: 0.4029 - sensitivity: 0.2911 - specificity: 0.9813\n",
            "Epoch 72: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 24s 61ms/step - loss: 1.3773 - accuracy: 0.5022 - f1: 0.4029 - sensitivity: 0.2911 - specificity: 0.9813 - val_loss: 1.4704 - val_accuracy: 0.5333 - val_f1: 0.4278 - val_sensitivity: 0.3133 - val_specificity: 0.9801 - lr: 5.0000e-07\n",
            "Epoch 73/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3697 - accuracy: 0.5070 - f1: 0.4160 - sensitivity: 0.3012 - specificity: 0.9819\n",
            "Epoch 73: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 25s 63ms/step - loss: 1.3697 - accuracy: 0.5070 - f1: 0.4160 - sensitivity: 0.3012 - specificity: 0.9819 - val_loss: 1.4483 - val_accuracy: 0.5333 - val_f1: 0.4373 - val_sensitivity: 0.3237 - val_specificity: 0.9793 - lr: 5.0000e-07\n",
            "Epoch 74/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3668 - accuracy: 0.5150 - f1: 0.4153 - sensitivity: 0.2996 - specificity: 0.9821\n",
            "Epoch 74: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 24s 61ms/step - loss: 1.3668 - accuracy: 0.5150 - f1: 0.4153 - sensitivity: 0.2996 - specificity: 0.9820 - val_loss: 1.4098 - val_accuracy: 0.5333 - val_f1: 0.4335 - val_sensitivity: 0.3182 - val_specificity: 0.9805 - lr: 5.0000e-07\n",
            "Epoch 75/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3634 - accuracy: 0.5149 - f1: 0.4157 - sensitivity: 0.3002 - specificity: 0.9825\n",
            "Epoch 75: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 25s 63ms/step - loss: 1.3628 - accuracy: 0.5151 - f1: 0.4161 - sensitivity: 0.3005 - specificity: 0.9825 - val_loss: 1.4018 - val_accuracy: 0.5376 - val_f1: 0.4358 - val_sensitivity: 0.3194 - val_specificity: 0.9807 - lr: 5.0000e-07\n",
            "Epoch 76/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3670 - accuracy: 0.5120 - f1: 0.4162 - sensitivity: 0.3021 - specificity: 0.9814\n",
            "Epoch 76: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 25s 62ms/step - loss: 1.3667 - accuracy: 0.5120 - f1: 0.4158 - sensitivity: 0.3018 - specificity: 0.9814 - val_loss: 1.4162 - val_accuracy: 0.5364 - val_f1: 0.4317 - val_sensitivity: 0.3164 - val_specificity: 0.9803 - lr: 5.0000e-07\n",
            "Epoch 77/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3585 - accuracy: 0.5125 - f1: 0.4146 - sensitivity: 0.2986 - specificity: 0.9822\n",
            "Epoch 77: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 24s 61ms/step - loss: 1.3579 - accuracy: 0.5125 - f1: 0.4148 - sensitivity: 0.2986 - specificity: 0.9822 - val_loss: 1.4735 - val_accuracy: 0.5364 - val_f1: 0.4372 - val_sensitivity: 0.3243 - val_specificity: 0.9787 - lr: 5.0000e-07\n",
            "Epoch 78/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3612 - accuracy: 0.5063 - f1: 0.4116 - sensitivity: 0.2969 - specificity: 0.9818\n",
            "Epoch 78: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 26s 66ms/step - loss: 1.3603 - accuracy: 0.5066 - f1: 0.4120 - sensitivity: 0.2973 - specificity: 0.9818 - val_loss: 1.4123 - val_accuracy: 0.5339 - val_f1: 0.4340 - val_sensitivity: 0.3200 - val_specificity: 0.9797 - lr: 5.0000e-07\n",
            "Epoch 79/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3589 - accuracy: 0.5125 - f1: 0.4210 - sensitivity: 0.3058 - specificity: 0.9820\n",
            "Epoch 79: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 27s 68ms/step - loss: 1.3589 - accuracy: 0.5125 - f1: 0.4210 - sensitivity: 0.3058 - specificity: 0.9820 - val_loss: 1.4352 - val_accuracy: 0.5395 - val_f1: 0.4366 - val_sensitivity: 0.3225 - val_specificity: 0.9798 - lr: 5.0000e-07\n",
            "Epoch 80/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3771 - accuracy: 0.5011 - f1: 0.4094 - sensitivity: 0.2979 - specificity: 0.9806\n",
            "Epoch 80: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 25s 61ms/step - loss: 1.3771 - accuracy: 0.5011 - f1: 0.4094 - sensitivity: 0.2979 - specificity: 0.9806 - val_loss: 1.4456 - val_accuracy: 0.5358 - val_f1: 0.4383 - val_sensitivity: 0.3237 - val_specificity: 0.9799 - lr: 5.0000e-07\n",
            "Epoch 81/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3428 - accuracy: 0.5219 - f1: 0.4215 - sensitivity: 0.3059 - specificity: 0.9818\n",
            "Epoch 81: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 25s 62ms/step - loss: 1.3429 - accuracy: 0.5218 - f1: 0.4216 - sensitivity: 0.3059 - specificity: 0.9819 - val_loss: 1.4118 - val_accuracy: 0.5345 - val_f1: 0.4380 - val_sensitivity: 0.3237 - val_specificity: 0.9799 - lr: 5.0000e-07\n",
            "Epoch 82/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3701 - accuracy: 0.5111 - f1: 0.4094 - sensitivity: 0.2954 - specificity: 0.9813\n",
            "Epoch 82: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 27s 66ms/step - loss: 1.3701 - accuracy: 0.5111 - f1: 0.4094 - sensitivity: 0.2954 - specificity: 0.9813 - val_loss: 1.4251 - val_accuracy: 0.5358 - val_f1: 0.4349 - val_sensitivity: 0.3200 - val_specificity: 0.9800 - lr: 5.0000e-07\n",
            "Epoch 83/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3625 - accuracy: 0.5108 - f1: 0.4203 - sensitivity: 0.3059 - specificity: 0.9812\n",
            "Epoch 83: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 27s 68ms/step - loss: 1.3624 - accuracy: 0.5106 - f1: 0.4203 - sensitivity: 0.3059 - specificity: 0.9811 - val_loss: 1.4166 - val_accuracy: 0.5339 - val_f1: 0.4433 - val_sensitivity: 0.3286 - val_specificity: 0.9794 - lr: 5.0000e-07\n",
            "Epoch 84/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3627 - accuracy: 0.5147 - f1: 0.4148 - sensitivity: 0.3011 - specificity: 0.9814\n",
            "Epoch 84: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 25s 62ms/step - loss: 1.3620 - accuracy: 0.5147 - f1: 0.4153 - sensitivity: 0.3014 - specificity: 0.9814 - val_loss: 1.4041 - val_accuracy: 0.5358 - val_f1: 0.4328 - val_sensitivity: 0.3188 - val_specificity: 0.9796 - lr: 5.0000e-07\n",
            "Epoch 85/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3528 - accuracy: 0.5144 - f1: 0.4184 - sensitivity: 0.3054 - specificity: 0.9812\n",
            "Epoch 85: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 27s 68ms/step - loss: 1.3538 - accuracy: 0.5139 - f1: 0.4184 - sensitivity: 0.3054 - specificity: 0.9811 - val_loss: 1.4134 - val_accuracy: 0.5320 - val_f1: 0.4374 - val_sensitivity: 0.3213 - val_specificity: 0.9807 - lr: 5.0000e-07\n",
            "Epoch 86/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3676 - accuracy: 0.5094 - f1: 0.4123 - sensitivity: 0.3008 - specificity: 0.9811\n",
            "Epoch 86: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 25s 62ms/step - loss: 1.3676 - accuracy: 0.5094 - f1: 0.4123 - sensitivity: 0.3008 - specificity: 0.9811 - val_loss: 1.4015 - val_accuracy: 0.5358 - val_f1: 0.4376 - val_sensitivity: 0.3225 - val_specificity: 0.9801 - lr: 5.0000e-07\n",
            "Epoch 87/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3739 - accuracy: 0.5055 - f1: 0.4151 - sensitivity: 0.3029 - specificity: 0.9809\n",
            "Epoch 87: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 28s 71ms/step - loss: 1.3736 - accuracy: 0.5058 - f1: 0.4153 - sensitivity: 0.3031 - specificity: 0.9808 - val_loss: 1.3872 - val_accuracy: 0.5383 - val_f1: 0.4394 - val_sensitivity: 0.3255 - val_specificity: 0.9793 - lr: 5.0000e-07\n",
            "Epoch 88/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3633 - accuracy: 0.5122 - f1: 0.4183 - sensitivity: 0.3039 - specificity: 0.9818\n",
            "Epoch 88: val_loss did not improve from 1.38697\n",
            "401/401 [==============================] - 25s 62ms/step - loss: 1.3625 - accuracy: 0.5123 - f1: 0.4186 - sensitivity: 0.3041 - specificity: 0.9819 - val_loss: 1.4059 - val_accuracy: 0.5364 - val_f1: 0.4399 - val_sensitivity: 0.3262 - val_specificity: 0.9791 - lr: 5.0000e-07\n",
            "Epoch 89/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3637 - accuracy: 0.5123 - f1: 0.4151 - sensitivity: 0.3014 - specificity: 0.9813\n",
            "Epoch 89: val_loss improved from 1.38697 to 1.37345, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 27s 66ms/step - loss: 1.3637 - accuracy: 0.5123 - f1: 0.4151 - sensitivity: 0.3014 - specificity: 0.9813 - val_loss: 1.3734 - val_accuracy: 0.5395 - val_f1: 0.4444 - val_sensitivity: 0.3292 - val_specificity: 0.9799 - lr: 5.0000e-07\n",
            "Epoch 90/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3662 - accuracy: 0.5084 - f1: 0.4128 - sensitivity: 0.2981 - specificity: 0.9820\n",
            "Epoch 90: val_loss did not improve from 1.37345\n",
            "401/401 [==============================] - 25s 62ms/step - loss: 1.3662 - accuracy: 0.5086 - f1: 0.4127 - sensitivity: 0.2979 - specificity: 0.9820 - val_loss: 1.3803 - val_accuracy: 0.5395 - val_f1: 0.4417 - val_sensitivity: 0.3280 - val_specificity: 0.9793 - lr: 5.0000e-07\n",
            "Epoch 91/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3580 - accuracy: 0.5103 - f1: 0.4137 - sensitivity: 0.3018 - specificity: 0.9806\n",
            "Epoch 91: val_loss did not improve from 1.37345\n",
            "401/401 [==============================] - 30s 75ms/step - loss: 1.3580 - accuracy: 0.5103 - f1: 0.4137 - sensitivity: 0.3018 - specificity: 0.9806 - val_loss: 1.3784 - val_accuracy: 0.5370 - val_f1: 0.4410 - val_sensitivity: 0.3255 - val_specificity: 0.9801 - lr: 5.0000e-07\n",
            "Epoch 92/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3692 - accuracy: 0.5059 - f1: 0.4091 - sensitivity: 0.2946 - specificity: 0.9818\n",
            "Epoch 92: val_loss did not improve from 1.37345\n",
            "401/401 [==============================] - 26s 65ms/step - loss: 1.3692 - accuracy: 0.5059 - f1: 0.4091 - sensitivity: 0.2946 - specificity: 0.9818 - val_loss: 1.4004 - val_accuracy: 0.5370 - val_f1: 0.4412 - val_sensitivity: 0.3262 - val_specificity: 0.9800 - lr: 5.0000e-07\n",
            "Epoch 93/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3473 - accuracy: 0.5111 - f1: 0.4187 - sensitivity: 0.3056 - specificity: 0.9814\n",
            "Epoch 93: val_loss did not improve from 1.37345\n",
            "401/401 [==============================] - 25s 62ms/step - loss: 1.3473 - accuracy: 0.5114 - f1: 0.4188 - sensitivity: 0.3056 - specificity: 0.9814 - val_loss: 1.3852 - val_accuracy: 0.5389 - val_f1: 0.4447 - val_sensitivity: 0.3298 - val_specificity: 0.9798 - lr: 5.0000e-07\n",
            "Epoch 94/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3633 - accuracy: 0.5147 - f1: 0.4156 - sensitivity: 0.3006 - specificity: 0.9817\n",
            "Epoch 94: val_loss did not improve from 1.37345\n",
            "401/401 [==============================] - 24s 60ms/step - loss: 1.3633 - accuracy: 0.5147 - f1: 0.4156 - sensitivity: 0.3006 - specificity: 0.9817 - val_loss: 1.3944 - val_accuracy: 0.5395 - val_f1: 0.4441 - val_sensitivity: 0.3268 - val_specificity: 0.9810 - lr: 5.0000e-07\n",
            "Epoch 95/100\n",
            "400/401 [============================>.] - ETA: 0s - loss: 1.3547 - accuracy: 0.5155 - f1: 0.4224 - sensitivity: 0.3077 - specificity: 0.9816\n",
            "Epoch 95: val_loss did not improve from 1.37345\n",
            "401/401 [==============================] - 25s 62ms/step - loss: 1.3556 - accuracy: 0.5150 - f1: 0.4223 - sensitivity: 0.3077 - specificity: 0.9815 - val_loss: 1.3976 - val_accuracy: 0.5414 - val_f1: 0.4421 - val_sensitivity: 0.3249 - val_specificity: 0.9807 - lr: 5.0000e-07\n",
            "Epoch 96/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3460 - accuracy: 0.5201 - f1: 0.4195 - sensitivity: 0.3044 - specificity: 0.9818\n",
            "Epoch 96: val_loss did not improve from 1.37345\n",
            "401/401 [==============================] - 26s 66ms/step - loss: 1.3460 - accuracy: 0.5201 - f1: 0.4195 - sensitivity: 0.3044 - specificity: 0.9818 - val_loss: 1.4050 - val_accuracy: 0.5370 - val_f1: 0.4397 - val_sensitivity: 0.3255 - val_specificity: 0.9795 - lr: 5.0000e-07\n",
            "Epoch 97/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3589 - accuracy: 0.5137 - f1: 0.4164 - sensitivity: 0.3033 - specificity: 0.9810\n",
            "Epoch 97: val_loss did not improve from 1.37345\n",
            "401/401 [==============================] - 27s 67ms/step - loss: 1.3589 - accuracy: 0.5137 - f1: 0.4164 - sensitivity: 0.3033 - specificity: 0.9810 - val_loss: 1.3873 - val_accuracy: 0.5408 - val_f1: 0.4424 - val_sensitivity: 0.3268 - val_specificity: 0.9802 - lr: 5.0000e-07\n",
            "Epoch 98/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3709 - accuracy: 0.5119 - f1: 0.4184 - sensitivity: 0.3041 - specificity: 0.9811\n",
            "Epoch 98: val_loss did not improve from 1.37345\n",
            "401/401 [==============================] - 27s 67ms/step - loss: 1.3709 - accuracy: 0.5119 - f1: 0.4184 - sensitivity: 0.3041 - specificity: 0.9811 - val_loss: 1.3829 - val_accuracy: 0.5414 - val_f1: 0.4398 - val_sensitivity: 0.3249 - val_specificity: 0.9799 - lr: 5.0000e-07\n",
            "Epoch 99/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3543 - accuracy: 0.5181 - f1: 0.4258 - sensitivity: 0.3116 - specificity: 0.9813\n",
            "Epoch 99: val_loss did not improve from 1.37345\n",
            "401/401 [==============================] - 29s 71ms/step - loss: 1.3543 - accuracy: 0.5181 - f1: 0.4258 - sensitivity: 0.3116 - specificity: 0.9813 - val_loss: 1.3998 - val_accuracy: 0.5395 - val_f1: 0.4396 - val_sensitivity: 0.3237 - val_specificity: 0.9803 - lr: 5.0000e-07\n",
            "Epoch 100/100\n",
            "401/401 [==============================] - ETA: 0s - loss: 1.3639 - accuracy: 0.5140 - f1: 0.4175 - sensitivity: 0.3041 - specificity: 0.9814\n",
            "Epoch 100: val_loss improved from 1.37345 to 1.37007, saving model to ResNet50_DA_aug.hdf5\n",
            "401/401 [==============================] - 30s 75ms/step - loss: 1.3639 - accuracy: 0.5140 - f1: 0.4175 - sensitivity: 0.3041 - specificity: 0.9814 - val_loss: 1.3701 - val_accuracy: 0.5426 - val_f1: 0.4508 - val_sensitivity: 0.3335 - val_specificity: 0.9807 - lr: 5.0000e-07\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "if data_augmentation:\n",
        "    print(\"-------------Using Data augmentation------------\")\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True)  # randomly flip images\n",
        "\n",
        "    datagen.fit(x_train)\n",
        "    history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                        epochs=epochs, verbose=1, validation_data=(x_test, y_test),\n",
        "                        callbacks=[lr_reducer,  csv_logger, model_chekpoint])\n",
        "\n",
        "else:\n",
        "    print(\"-----Not Using Data augmentation---------------\")\n",
        "    history = model.fit(x_train, y_train,\n",
        "              batch_size=batch_size * 4,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True, callbacks=[lr_reducer,  csv_logger, model_chekpoint])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcNrHJTYNZVl"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knp-ULKtHxS3",
        "outputId": "64885073-9cfe-42ff-885a-31344f78d9b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------Training time is seconds:%s 3302.1891515254974\n",
            "51/51 [==============================] - 1s 14ms/step - loss: 1.3701 - accuracy: 0.5426 - f1: 0.4508 - sensitivity: 0.3335 - specificity: 0.9807\n",
            "dict_keys(['loss', 'accuracy', 'f1', 'sensitivity', 'specificity', 'val_loss', 'val_accuracy', 'val_f1', 'val_sensitivity', 'val_specificity', 'lr'])\n",
            "dict_values([[2.633359432220459, 2.3950555324554443, 2.4488022327423096, 2.447218894958496, 2.4699039459228516, 2.433532953262329, 2.3833234310150146, 2.014056921005249, 1.9867945909500122, 1.945122241973877, 1.8554577827453613, 1.8127127885818481, 1.7819085121154785, 1.72444486618042, 1.621240258216858, 1.593187689781189, 1.5513842105865479, 1.5257184505462646, 1.5151028633117676, 1.486080527305603, 1.468904972076416, 1.457303524017334, 1.448814868927002, 1.4327421188354492, 1.418986439704895, 1.4069701433181763, 1.3960468769073486, 1.3960728645324707, 1.4148533344268799, 1.397768497467041, 1.4059710502624512, 1.3897771835327148, 1.3931310176849365, 1.3993662595748901, 1.3912807703018188, 1.3887436389923096, 1.3984044790267944, 1.3967642784118652, 1.400431513786316, 1.3847156763076782, 1.3978211879730225, 1.3822578191757202, 1.3733234405517578, 1.4003411531448364, 1.3755112886428833, 1.385384440422058, 1.3933199644088745, 1.3780887126922607, 1.3692885637283325, 1.3798340559005737, 1.3802943229675293, 1.4038448333740234, 1.39739191532135, 1.3794169425964355, 1.3711206912994385, 1.3931928873062134, 1.380441427230835, 1.3867018222808838, 1.3764528036117554, 1.3798704147338867, 1.3770382404327393, 1.374943733215332, 1.3810975551605225, 1.3703606128692627, 1.3792074918746948, 1.3757926225662231, 1.3672895431518555, 1.3659400939941406, 1.3615562915802002, 1.3626477718353271, 1.3690556287765503, 1.3772820234298706, 1.369686484336853, 1.3668471574783325, 1.3627983331680298, 1.3667056560516357, 1.3578568696975708, 1.3603192567825317, 1.3589376211166382, 1.3770993947982788, 1.342943787574768, 1.3701316118240356, 1.3624149560928345, 1.3619588613510132, 1.3538049459457397, 1.367596983909607, 1.3736279010772705, 1.3625391721725464, 1.3637183904647827, 1.3661632537841797, 1.3580188751220703, 1.3692139387130737, 1.3472530841827393, 1.363289475440979, 1.3555618524551392, 1.345978856086731, 1.3588643074035645, 1.3708992004394531, 1.3543435335159302, 1.3639190196990967], [0.27956318855285645, 0.25756630301475525, 0.21591264009475708, 0.18627144396305084, 0.20015600323677063, 0.21950078010559082, 0.22059282660484314, 0.2525741159915924, 0.2675507068634033, 0.2921997010707855, 0.30982840061187744, 0.3268330693244934, 0.3394695818424225, 0.37285491824150085, 0.4053042232990265, 0.42761310935020447, 0.43463337421417236, 0.4542901813983917, 0.4463338553905487, 0.45928236842155457, 0.47472697496414185, 0.46911075711250305, 0.480967253446579, 0.4850234091281891, 0.49407175183296204, 0.49219968914985657, 0.48954758048057556, 0.5028080940246582, 0.4943837821483612, 0.49921995401382446, 0.4932917356491089, 0.5001559853553772, 0.4957878291606903, 0.4945397675037384, 0.5010920166969299, 0.502652108669281, 0.4957878291606903, 0.4989079535007477, 0.4904836118221283, 0.5034321546554565, 0.49797192215919495, 0.5, 0.5067082643508911, 0.4976598918437958, 0.5018720626831055, 0.5007800459861755, 0.5001559853553772, 0.49937596917152405, 0.5056162476539612, 0.5048362016677856, 0.5035881400108337, 0.49672386050224304, 0.4901716113090515, 0.5028080940246582, 0.5134165287017822, 0.49906396865844727, 0.5020281076431274, 0.5037441253662109, 0.5003120303153992, 0.5049921870231628, 0.5076442956924438, 0.5081123113632202, 0.5014040470123291, 0.5028080940246582, 0.49921995401382446, 0.5062402486801147, 0.5149766206741333, 0.5124804973602295, 0.510608434677124, 0.517004668712616, 0.507956326007843, 0.5021840929985046, 0.5070202946662903, 0.5149766206741333, 0.5151326060295105, 0.5120124816894531, 0.5124804973602295, 0.5065522789955139, 0.5124804973602295, 0.5010920166969299, 0.5218408703804016, 0.5110764503479004, 0.510608434677124, 0.5146645903587341, 0.5138845443725586, 0.5093603730201721, 0.5057722330093384, 0.5123245120048523, 0.5123245120048523, 0.5085803270339966, 0.5102964043617249, 0.5059282183647156, 0.5113884806632996, 0.5146645903587341, 0.5149766206741333, 0.5201247930526733, 0.5137285590171814, 0.5118564963340759, 0.5180967450141907, 0.5140405893325806], [0.1283387690782547, 0.11230381578207016, 0.038545817136764526, 0.02045615017414093, 0.025278614833950996, 0.03007582016289234, 0.04121438041329384, 0.03155922517180443, 0.03848093003034592, 0.04587459936738014, 0.0624290332198143, 0.08916664868593216, 0.1160932183265686, 0.1664784550666809, 0.228215754032135, 0.2649061679840088, 0.29125189781188965, 0.3142849802970886, 0.3161638081073761, 0.33647242188453674, 0.3450421392917633, 0.3528652489185333, 0.35706984996795654, 0.3664487302303314, 0.3847127854824066, 0.39556142687797546, 0.39200443029403687, 0.39420509338378906, 0.3819652497768402, 0.3936280608177185, 0.3966071605682373, 0.39890801906585693, 0.38850662112236023, 0.3952503204345703, 0.39528149366378784, 0.3970130980014801, 0.39654719829559326, 0.3923395872116089, 0.39180606603622437, 0.399874746799469, 0.3920513987541199, 0.39959126710891724, 0.40105557441711426, 0.3966289460659027, 0.39783695340156555, 0.40358251333236694, 0.3915706276893616, 0.40539467334747314, 0.4050726294517517, 0.40110352635383606, 0.4008808732032776, 0.39468327164649963, 0.395991712808609, 0.4007737636566162, 0.40466639399528503, 0.39601054787635803, 0.40982669591903687, 0.40727469325065613, 0.40474361181259155, 0.40823614597320557, 0.40186917781829834, 0.40474414825439453, 0.40306997299194336, 0.40673044323921204, 0.40590545535087585, 0.4048442244529724, 0.41408199071884155, 0.41402262449264526, 0.4114198088645935, 0.4114185571670532, 0.4085274934768677, 0.40293455123901367, 0.4159826636314392, 0.41528621315956116, 0.4160853624343872, 0.41583192348480225, 0.4147520661354065, 0.4119910001754761, 0.42101436853408813, 0.409396231174469, 0.42160627245903015, 0.4093550741672516, 0.4202503263950348, 0.4152511656284332, 0.4184392988681793, 0.4123362898826599, 0.41530656814575195, 0.4185643792152405, 0.4150792062282562, 0.41272521018981934, 0.4136521816253662, 0.40912437438964844, 0.41879674792289734, 0.4156469702720642, 0.4223378896713257, 0.4194801449775696, 0.41639113426208496, 0.4184282124042511, 0.42581894993782043, 0.417512446641922], [0.08538030087947845, 0.07325436174869537, 0.02228803001344204, 0.011471321806311607, 0.014183292165398598, 0.017082294449210167, 0.023223191499710083, 0.017082294449210167, 0.021290523931384087, 0.025561098009347916, 0.035473816096782684, 0.05112219601869583, 0.06829800456762314, 0.10240025073289871, 0.1449812948703766, 0.17397131025791168, 0.19479426741600037, 0.21190772950649261, 0.21580423414707184, 0.2306421548128128, 0.23709475994110107, 0.24451372027397156, 0.24869076907634735, 0.25557979941368103, 0.27044886350631714, 0.2817955017089844, 0.2802680730819702, 0.28029924631118774, 0.27222567796707153, 0.28073564171791077, 0.2830735445022583, 0.2849438786506653, 0.27621570229530334, 0.28354114294052124, 0.2835099697113037, 0.28460100293159485, 0.2834787964820862, 0.28039276599884033, 0.2784912884235382, 0.28594139218330383, 0.28042393922805786, 0.2868453860282898, 0.2848815619945526, 0.2839152216911316, 0.28388404846191406, 0.2909289300441742, 0.2803304195404053, 0.29024314880371094, 0.28983789682388306, 0.2867518663406372, 0.2875935137271881, 0.28269949555397034, 0.2836970090866089, 0.28647130727767944, 0.2903989851474762, 0.2834787964820862, 0.29759976267814636, 0.29258105158805847, 0.29161471128463745, 0.29653990268707275, 0.28855985403060913, 0.29086658358573914, 0.29046133160591125, 0.2922070026397705, 0.29295510053634644, 0.292612224817276, 0.2967892587184906, 0.30028054118156433, 0.2980673313140869, 0.29653990268707275, 0.2948254346847534, 0.2911471426486969, 0.3011845350265503, 0.2996259331703186, 0.30052992701530457, 0.301839143037796, 0.2985972762107849, 0.29725685715675354, 0.30576685070991516, 0.29788029193878174, 0.3059226870536804, 0.2954176962375641, 0.3058915138244629, 0.301402747631073, 0.30542394518852234, 0.3007793128490448, 0.3030860424041748, 0.3040523827075958, 0.3014339208602905, 0.2979426383972168, 0.30177679657936096, 0.2946072220802307, 0.30557981133461, 0.30062344670295715, 0.3076995015144348, 0.30436408519744873, 0.30327308177948, 0.3041146993637085, 0.3115648329257965, 0.3041146993637085], [0.9779127836227417, 0.9839097261428833, 0.9888517260551453, 0.990290105342865, 0.9905887842178345, 0.9897645115852356, 0.9912259578704834, 0.9946100115776062, 0.9936966300010681, 0.9939420819282532, 0.9941917061805725, 0.9934744238853455, 0.9914262890815735, 0.9879622459411621, 0.9864571690559387, 0.98468017578125, 0.9842084050178528, 0.9840525388717651, 0.9830862879753113, 0.983522891998291, 0.9836163520812988, 0.983420193195343, 0.9826726317405701, 0.9829754829406738, 0.9833984375, 0.9829885363578796, 0.9817373752593994, 0.9826946258544922, 0.9816127419471741, 0.9818084836006165, 0.9822404980659485, 0.982476532459259, 0.9819607734680176, 0.9818172454833984, 0.9818129539489746, 0.982093334197998, 0.9822708964347839, 0.9819284081459045, 0.9825524687767029, 0.9824185371398926, 0.9815056920051575, 0.9815365076065063, 0.9833314418792725, 0.9817328453063965, 0.9824320673942566, 0.9813947081565857, 0.9817639589309692, 0.9826946258544922, 0.982329249382019, 0.9822311401367188, 0.981661319732666, 0.9812830090522766, 0.9819421768188477, 0.9827523231506348, 0.9819599390029907, 0.9816126823425293, 0.9808871150016785, 0.9819689989089966, 0.9813855886459351, 0.9805259704589844, 0.982276439666748, 0.9818265438079834, 0.9818522334098816, 0.9819954633712769, 0.9813230037689209, 0.9812602400779724, 0.9829174876213074, 0.9819644093513489, 0.9808374047279358, 0.9821292757987976, 0.9820396900177002, 0.9812737107276917, 0.9818535447120667, 0.9820307493209839, 0.9825385212898254, 0.9814032912254333, 0.9822311401367188, 0.9818354249000549, 0.9820358753204346, 0.9806147813796997, 0.9818708300590515, 0.9813006520271301, 0.9811135530471802, 0.9814301133155823, 0.9811492562294006, 0.9810606241226196, 0.9808337688446045, 0.9818706512451172, 0.981309711933136, 0.9820176959037781, 0.9805573225021362, 0.981839656829834, 0.9814119935035706, 0.9816704392433167, 0.9815276861190796, 0.9818258881568909, 0.9809800982475281, 0.9811403155326843, 0.9812827706336975, 0.9814254641532898], [2.686155080795288, 3.5196497440338135, 2.1530165672302246, 11.342479705810547, 2.037087917327881, 3.9163286685943604, 3.9669177532196045, 2.0116679668426514, 3.4984705448150635, 2.040318012237549, 2.1054835319519043, 1.8214430809020996, 1.83152174949646, 2.0408377647399902, 1.637551188468933, 1.5652941465377808, 1.5361078977584839, 1.6749672889709473, 1.6838053464889526, 1.4861083030700684, 1.5543657541275024, 1.4710164070129395, 1.5228557586669922, 1.5829136371612549, 1.535282015800476, 1.5169062614440918, 1.489600658416748, 1.5033060312271118, 1.4834240674972534, 1.4552873373031616, 1.4739412069320679, 1.4957863092422485, 1.446912407875061, 1.4568778276443481, 1.4403067827224731, 1.4325977563858032, 1.4421130418777466, 1.4499096870422363, 1.4321954250335693, 1.4605770111083984, 1.4626072645187378, 1.450904369354248, 1.4089791774749756, 1.4481968879699707, 1.474194884300232, 1.4215532541275024, 1.449036717414856, 1.4621151685714722, 1.4445046186447144, 1.4001519680023193, 1.4736279249191284, 1.4433661699295044, 1.473205327987671, 1.4584137201309204, 1.4010385274887085, 1.38862943649292, 1.4233475923538208, 1.4538812637329102, 1.427689552307129, 1.4312254190444946, 1.4483413696289062, 1.473104476928711, 1.4333189725875854, 1.4276270866394043, 1.3869739770889282, 1.4314252138137817, 1.4256092309951782, 1.4364012479782104, 1.419412612915039, 1.448809266090393, 1.4505647420883179, 1.4704012870788574, 1.4483323097229004, 1.4098080396652222, 1.4018224477767944, 1.4162448644638062, 1.4734777212142944, 1.4122885465621948, 1.4351708889007568, 1.4455609321594238, 1.4118133783340454, 1.4251478910446167, 1.416571855545044, 1.4041098356246948, 1.4133819341659546, 1.401456356048584, 1.387204647064209, 1.4058564901351929, 1.373448133468628, 1.3803448677062988, 1.3784247636795044, 1.40038001537323, 1.3852494955062866, 1.3944019079208374, 1.3975903987884521, 1.405045509338379, 1.3873211145401, 1.3828516006469727, 1.3997743129730225, 1.3700655698776245], [0.3528313636779785, 0.2576228976249695, 0.142501562833786, 0.15121343731880188, 0.2159302979707718, 0.25015556812286377, 0.23708774149417877, 0.26384568214416504, 0.2601120173931122, 0.285003125667572, 0.3192283809185028, 0.3484754264354706, 0.37523335218429565, 0.363410085439682, 0.41692593693733215, 0.4573739767074585, 0.462974488735199, 0.46359676122665405, 0.4747977554798126, 0.5028002262115479, 0.5046671032905579, 0.5164903402328491, 0.5127567052841187, 0.515245795249939, 0.5196017622947693, 0.5214685797691345, 0.5196017622947693, 0.5227131247520447, 0.5264468193054199, 0.5289359092712402, 0.5208463072776794, 0.5214685797691345, 0.527069091796875, 0.527069091796875, 0.5202240347862244, 0.5258245468139648, 0.5264468193054199, 0.5245799422264099, 0.5283136367797852, 0.5245799422264099, 0.5276913642883301, 0.5264468193054199, 0.5295581817626953, 0.5326695442199707, 0.5239576697349548, 0.5308027267456055, 0.525202214717865, 0.5264468193054199, 0.5320472717285156, 0.5351586937904358, 0.5283136367797852, 0.5289359092712402, 0.5320472717285156, 0.5326695442199707, 0.5382700562477112, 0.5388923287391663, 0.5320472717285156, 0.5345364212989807, 0.5357809662818909, 0.5332918763160706, 0.5320472717285156, 0.5301804542541504, 0.5326695442199707, 0.5345364212989807, 0.5345364212989807, 0.5339141488075256, 0.5308027267456055, 0.5289359092712402, 0.5376477837562561, 0.5357809662818909, 0.5395146012306213, 0.5332918763160706, 0.5332918763160706, 0.5332918763160706, 0.5376477837562561, 0.536403238773346, 0.536403238773346, 0.5339141488075256, 0.5395146012306213, 0.5357809662818909, 0.5345364212989807, 0.5357809662818909, 0.5339141488075256, 0.5357809662818909, 0.5320472717285156, 0.5357809662818909, 0.5382700562477112, 0.536403238773346, 0.5395146012306213, 0.5395146012306213, 0.537025511264801, 0.537025511264801, 0.5388923287391663, 0.5395146012306213, 0.5413814783096313, 0.537025511264801, 0.5407592058181763, 0.5413814783096313, 0.5395146012306213, 0.5426260232925415], [0.29728880524635315, 0.1327991932630539, 0.0011883539846166968, 0.12326453626155853, 0.04913953319191933, 0.15660393238067627, 0.014833454042673111, 0.0351136140525341, 0.02588542364537716, 0.01490135956555605, 0.09863195568323135, 0.1321026235818863, 0.18413563072681427, 0.23143409192562103, 0.2714833617210388, 0.2975509464740753, 0.3083488345146179, 0.33519673347473145, 0.34075507521629333, 0.3790355622768402, 0.38349416851997375, 0.4010258913040161, 0.3977215588092804, 0.39782342314720154, 0.40681028366088867, 0.4171818792819977, 0.40649500489234924, 0.41036248207092285, 0.41675299406051636, 0.41673940420150757, 0.4149564504623413, 0.41342276334762573, 0.41207581758499146, 0.4139869511127472, 0.41883453726768494, 0.42064133286476135, 0.4175805449485779, 0.4159258008003235, 0.41222405433654785, 0.4189724624156952, 0.4148085415363312, 0.42427757382392883, 0.4194158911705017, 0.41758525371551514, 0.4165697991847992, 0.4211921691894531, 0.41222912073135376, 0.41719159483909607, 0.42121177911758423, 0.42732664942741394, 0.4233860373497009, 0.42454788088798523, 0.4208313524723053, 0.42669498920440674, 0.42345401644706726, 0.42865297198295593, 0.42220309376716614, 0.42155104875564575, 0.4223296642303467, 0.42346519231796265, 0.42291587591171265, 0.42703863978385925, 0.4279726445674896, 0.4295281767845154, 0.4231444001197815, 0.43127182126045227, 0.42677515745162964, 0.4335858225822449, 0.4371702969074249, 0.4313560128211975, 0.4343535900115967, 0.4278222918510437, 0.43725499510765076, 0.43348270654678345, 0.43578651547431946, 0.4317490756511688, 0.4372141361236572, 0.4340347945690155, 0.4366333782672882, 0.43827229738235474, 0.438041090965271, 0.4348633587360382, 0.44333210587501526, 0.4327738285064697, 0.4373927414417267, 0.4375995099544525, 0.4394238293170929, 0.43986672163009644, 0.444396436214447, 0.44170862436294556, 0.44098910689353943, 0.4411764442920685, 0.4447137415409088, 0.4440732002258301, 0.442118763923645, 0.4396882951259613, 0.44236260652542114, 0.4397950768470764, 0.4396422505378723, 0.4507785737514496], [0.2221638709306717, 0.0803571417927742, 0.0006127451197244227, 0.0882352963089943, 0.02757352963089943, 0.10547968745231628, 0.007965686731040478, 0.018995098769664764, 0.014705882407724857, 0.007965686731040478, 0.05707282945513725, 0.0788690447807312, 0.11563374847173691, 0.15239845216274261, 0.1781337559223175, 0.19590336084365845, 0.2017682045698166, 0.22925420105457306, 0.23792016506195068, 0.2691701650619507, 0.2703956663608551, 0.2912289798259735, 0.2838760316371918, 0.28755250573158264, 0.2961309552192688, 0.307160347700119, 0.2949054539203644, 0.29980742931365967, 0.305934876203537, 0.30348387360572815, 0.30348387360572815, 0.3010329008102417, 0.29980742931365967, 0.30409663915634155, 0.30409663915634155, 0.3077731132507324, 0.3053221106529236, 0.30287113785743713, 0.29858192801475525, 0.306547611951828, 0.30287113785743713, 0.3126750588417053, 0.305934876203537, 0.30470937490463257, 0.3053221106529236, 0.30838584899902344, 0.3004201650619507, 0.30470937490463257, 0.30838584899902344, 0.3132878243923187, 0.30899858474731445, 0.3114495873451233, 0.30899858474731445, 0.31451329588890076, 0.30961135029792786, 0.3126750588417053, 0.30838584899902344, 0.30961135029792786, 0.30899858474731445, 0.30899858474731445, 0.30838584899902344, 0.31390056014060974, 0.3132878243923187, 0.3163515329360962, 0.306547611951828, 0.31880250573158264, 0.3132878243923187, 0.3206407427787781, 0.3212535083293915, 0.3163515329360962, 0.3218662440776825, 0.3132878243923187, 0.32370448112487793, 0.3181897699832916, 0.31941527128219604, 0.3163515329360962, 0.32431721687316895, 0.32002800703048706, 0.3224789798259735, 0.32370448112487793, 0.32370448112487793, 0.32002800703048706, 0.32860642671585083, 0.31880250573158264, 0.3212535083293915, 0.3224789798259735, 0.32554271817207336, 0.3261554539203644, 0.32921919226646423, 0.3279936909675598, 0.32554271817207336, 0.3261554539203644, 0.32983192801475525, 0.3267681896686554, 0.32492995262145996, 0.32554271817207336, 0.3267681896686554, 0.32492995262145996, 0.32370448112487793, 0.3335084021091461], [0.9626100659370422, 0.9849438667297363, 0.9998250007629395, 0.9539815783500671, 0.9909838438034058, 0.9684374332427979, 0.9929971694946289, 0.9946103692054749, 0.9845939874649048, 0.9943976998329163, 0.9895832538604736, 0.9893205761909485, 0.9854689836502075, 0.9804296493530273, 0.9841936230659485, 0.9858940839767456, 0.9888702630996704, 0.9819051623344421, 0.980379581451416, 0.9806421995162964, 0.9821302890777588, 0.9791540503501892, 0.9814301133155823, 0.9795921444892883, 0.9794169664382935, 0.9781913757324219, 0.979591965675354, 0.9790669083595276, 0.9788042902946472, 0.9802048802375793, 0.9789792895317078, 0.9797670841217041, 0.9798545837402344, 0.9784539341926575, 0.9809048771858215, 0.9796795845031738, 0.9795045256614685, 0.98011714220047, 0.9805549383163452, 0.9797670245170593, 0.9795045256614685, 0.9789793491363525, 0.9802922010421753, 0.9801172614097595, 0.9788041710853577, 0.9795920252799988, 0.9795045852661133, 0.9796795845031738, 0.9797671437263489, 0.9797670245170593, 0.9805173873901367, 0.9799421429634094, 0.9789792895317078, 0.979066789150238, 0.9800295829772949, 0.981217622756958, 0.9802047610282898, 0.9794170260429382, 0.9796796441078186, 0.9802923202514648, 0.9801671504974365, 0.9797671437263489, 0.9805172681808472, 0.9795919060707092, 0.9815678596496582, 0.979066789150238, 0.9799419641494751, 0.9791542291641235, 0.9804673194885254, 0.9801671504974365, 0.9789792895317078, 0.98011714220047, 0.9793294072151184, 0.9804673194885254, 0.9806923866271973, 0.9803422689437866, 0.9787165522575378, 0.9796794056892395, 0.9797669053077698, 0.9798544049263, 0.9799420237541199, 0.979992151260376, 0.979379415512085, 0.9796420931816101, 0.9806923866271973, 0.9800796508789062, 0.9793292880058289, 0.9790667295455933, 0.9799420833587646, 0.9793293476104736, 0.98011714220047, 0.9800295829772949, 0.9797670245170593, 0.9809550046920776, 0.9806923270225525, 0.979504406452179, 0.980204701423645, 0.9798544049263, 0.9803422093391418, 0.9806923866271973], [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0003162278, 0.0003162278, 0.0003162278, 0.000100000005, 0.000100000005, 0.000100000005, 0.000100000005, 3.1622778e-05, 3.1622778e-05, 3.1622778e-05, 3.1622778e-05, 3.1622778e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 1.0000001e-05, 3.1622778e-06, 3.1622778e-06, 1.0000001e-06, 1.0000001e-06, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07]])\n",
            "[1.3700655698776245, 0.5426260232925415, 0.4507785737514496, 0.3335084021091461, 0.9806923866271973]\n",
            "Test loss: 1.3700655698776245\n",
            "Test accuracy: 0.5426260232925415\n",
            "Test f1: 0.4507785737514496\n",
            "Test sensitivity: 0.3335084021091461\n",
            "Test specificity: 0.9806923866271973\n",
            "Max Test accuracy 0.5426260232925415\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print('------------Training time is seconds:%s',time.time()-start)\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "#history keys and values\n",
        "print(history.history.keys())\n",
        "print(history.history.values())\n",
        "\n",
        "#Metrics for testing\n",
        "print(scores)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "print('Test f1:',scores[2])\n",
        "print('Test sensitivity:',scores[3])\n",
        "print('Test specificity:',scores[4])\n",
        "print(\"Max Test accuracy\", max(history.history['val_accuracy']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "o8do8qKLOPWk",
        "outputId": "07e3cc10-8a30-4947-a82c-010daef69a87"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# summarize history for accuracy\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig(METRICS_PATH+MODEL_NAME+\"_acc.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "5DDu3Xc8OPT1",
        "outputId": "1bb5dde2-205f-44cb-9c5d-c1feb6257609"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# summarize history for loss\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig(METRICS_PATH+MODEL_NAME+\"_loss.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without Augmentation"
      ],
      "metadata": {
        "id": "F0C3Xs_sKt6g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMVtt6FcZYTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0054a46-0258-4301-ebf9-a77de021c027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PWD /content/drive/MyDrive/datasetnew/\n"
          ]
        }
      ],
      "source": [
        "PATH = \"/content/drive/MyDrive/datasetnew/\"\n",
        "METRICS_PATH=\"/content/drive/MyDrive/metro/\"\n",
        "MODEL_NAME=\"ResNet_wag\"\n",
        "print(\"PWD\", PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_conv(Input, filters, stride=1, kernel_size=7):\n",
        "    x = Conv2D(filters, kernel_size=(kernel_size, kernel_size), strides=(stride, stride), padding=\"same\")(Input)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv_basic_block(Input, filters, stride=1, dropout=0.0):\n",
        "    Init = Input\n",
        "\n",
        "    # First conv which is used to downsample the image\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(Input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Optional Dropout layer\n",
        "    if (dropout > 0.0):\n",
        "        x = Dropout(dropout)(x)\n",
        "\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Projection shortcut to make skip connection(Paper terminology)\n",
        "    skip_conv = Conv2D(filters, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n",
        "    skip = BatchNormalization()(skip_conv)\n",
        "\n",
        "    # Skip connection\n",
        "    x = Add()([x, skip])\n",
        "    return x\n",
        "\n",
        "\n",
        "def normal_conv_basic_block(Input, filters, stride=1, dropout=0.0):\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(Input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Optional Dropout layer\n",
        "    if (dropout > 0.0):\n",
        "        x = Dropout(dropout)(x)\n",
        "\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Identity skip connection\n",
        "    x = Add()([x, Input])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def expand_conv_bottleneck_block(Input, filters, stride=1, dropout=0.0):\n",
        "    # Contracting 1*1 conv\n",
        "    x = Conv2D(filters, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # if(dropout > 0.0):\n",
        "    #   x = Dropout(dropout)(x)\n",
        "\n",
        "    # Depth preserving 3*3 conv\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # if(Dropout > 0.0):\n",
        "    #   x = Dropout(dropout)(x)\n",
        "\n",
        "    # Expanding 1*1 Conv\n",
        "    x = Conv2D(filters * 4, kernel_size=(1, 1), strides=(1, 1), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Projection shortcut\n",
        "    skip_conv = Conv2D(filters * 4, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n",
        "    skip = BatchNormalization()(skip_conv)\n",
        "\n",
        "    # Skip connection\n",
        "    x = Add()([x, skip])\n",
        "\n",
        "    return x\n",
        "\n",
        "def normal_conv_bottleneck_block(Input, filters, stride=1, dropout=0.0):\n",
        "    # Contracting 1*1 conv\n",
        "    x = Conv2D(filters, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(Input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # if(dropout > 0.0):\n",
        "    #   x = Dropout(dropout)(x)\n",
        "\n",
        "    # Depth preserving 3*3 Conv\n",
        "    x = Conv2D(filters, kernel_size=(3, 3), strides=(stride, stride), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # if(Dropout > 0.0):\n",
        "    #    x = Dropout(dropout)(x)\n",
        "\n",
        "    # Expanding 1*1 Conv\n",
        "    x = Conv2D(filters * 4, kernel_size=(1, 1), strides=(stride, stride), padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Identity skip connection\n",
        "    x = Add()([x, Input])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_basic_resnet(h, w, no_of_outputs, r1, r2, r3, r4, first_conv_stride=2, first_max_pool=True,\n",
        "                       first_conv_kernel_size=7):\n",
        "    # Creating input tensor\n",
        "    inputs = Input(shape=(h, w, 3), name=\"image_input\")\n",
        "\n",
        "    # Inital Conv block\n",
        "    x = initial_conv(inputs, 64, first_conv_stride, first_conv_kernel_size)\n",
        "\n",
        "    # Optional Max pooling layer\n",
        "    if (first_max_pool):\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Expanding block1 with projection shortcut\n",
        "    x = expand_conv_basic_block(x, 64, 1)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv1\n",
        "    for i in range(r1 - 1):\n",
        "        x = normal_conv_basic_block(x, 64)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    # Expanding block2 with projection shortcut\n",
        "    x = expand_conv_basic_block(x, 128, 2)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv2\n",
        "    for i in range(r2 - 1):\n",
        "        x = normal_conv_basic_block(x, 128)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    # Expanding block3 with projection shortcut\n",
        "    x = expand_conv_basic_block(x, 256, 2)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv3\n",
        "    for i in range(r3 - 1):\n",
        "        x = normal_conv_basic_block(x, 256)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    # Expanding block4 with projection shortcut\n",
        "    x = expand_conv_basic_block(x, 512, 2)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv3\n",
        "    for i in range(r4 - 1):\n",
        "        x = normal_conv_basic_block(x, 512)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    shape = K.int_shape(x)\n",
        "\n",
        "    # Average pooling layer\n",
        "    x = AveragePooling2D(pool_size=(shape[1], shape[2]),\n",
        "                         strides=(1, 1))(x)\n",
        "    # x = GlobalAveragePooling2D()(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # Classifier Block\n",
        "    x = Dense(no_of_outputs, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_bottleneck_resnet(h, w, no_of_outputs, r1, r2, r3, r4, first_conv_stride=2, first_max_pool=True,\n",
        "                            first_conv_kernel_size=7):\n",
        "    # Creating input tensor\n",
        "    inputs = Input(shape=(h, w, 3), name=\"image_input\")\n",
        "\n",
        "    # Inital Conv block\n",
        "    x = initial_conv(inputs, 64, first_conv_stride, first_conv_kernel_size)\n",
        "\n",
        "    # Optional Max pooling layer\n",
        "    if (first_max_pool):\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Expanding block1 with projection shortcut\n",
        "    x = expand_conv_bottleneck_block(x, 64, 1)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv1\n",
        "    for i in range(r1 - 1):\n",
        "        x = normal_conv_bottleneck_block(x, 64)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    # Expanding block2 with projection shortcut\n",
        "    x = expand_conv_bottleneck_block(x, 128, 2)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv2\n",
        "    for i in range(r2 - 1):\n",
        "        x = normal_conv_bottleneck_block(x, 128)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    # Expanding block3 with projection shortcut\n",
        "    x = expand_conv_bottleneck_block(x, 256, 2)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv3\n",
        "    for i in range(r3 - 1):\n",
        "        x = normal_conv_bottleneck_block(x, 256)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    # Expanding block4 with projection shortcut\n",
        "    x = expand_conv_bottleneck_block(x, 512, 2)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # Repeating block of Conv4\n",
        "    for i in range(r4 - 1):\n",
        "        x = normal_conv_bottleneck_block(x, 512)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    shape = K.int_shape(x)\n",
        "\n",
        "    # Average pooling layer\n",
        "    x = AveragePooling2D(pool_size=(shape[1], shape[2]),\n",
        "                         strides=(1, 1))(x)\n",
        "    # x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Classifier Block\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(no_of_outputs, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "5O0lIpOiFHxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_bottleneck_resnet(32,32,8,3,4,6,3,2,True,7)\n",
        "model.summary()\n",
        "plot_model(model,\"ResNet50_wag.png\",show_shapes=True)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "        optimizer=\"Adam\",\n",
        "        metrics=['accuracy',f1,sensitivity,specificity])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjnD_jhlFTKm",
        "outputId": "6dae8712-332b-4852-eb04-9c8702d8f84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " image_input (InputLayer)    [(None, 32, 32, 3)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_106 (Conv2D)         (None, 16, 16, 64)           9472      ['image_input[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_106 (B  (None, 16, 16, 64)           256       ['conv2d_106[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_98 (Activation)  (None, 16, 16, 64)           0         ['batch_normalization_106[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPoolin  (None, 8, 8, 64)             0         ['activation_98[0][0]']       \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_107 (Conv2D)         (None, 8, 8, 64)             4160      ['max_pooling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_107 (B  (None, 8, 8, 64)             256       ['conv2d_107[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_99 (Activation)  (None, 8, 8, 64)             0         ['batch_normalization_107[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " conv2d_108 (Conv2D)         (None, 8, 8, 64)             36928     ['activation_99[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_108 (B  (None, 8, 8, 64)             256       ['conv2d_108[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_100 (Activation  (None, 8, 8, 64)             0         ['batch_normalization_108[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_109 (Conv2D)         (None, 8, 8, 256)            16640     ['activation_100[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_110 (Conv2D)         (None, 8, 8, 256)            16640     ['max_pooling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_109 (B  (None, 8, 8, 256)            1024      ['conv2d_109[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_110 (B  (None, 8, 8, 256)            1024      ['conv2d_110[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_32 (Add)                (None, 8, 8, 256)            0         ['batch_normalization_109[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_110[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_101 (Activation  (None, 8, 8, 256)            0         ['add_32[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_111 (Conv2D)         (None, 8, 8, 64)             16448     ['activation_101[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_111 (B  (None, 8, 8, 64)             256       ['conv2d_111[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_102 (Activation  (None, 8, 8, 64)             0         ['batch_normalization_111[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_112 (Conv2D)         (None, 8, 8, 64)             36928     ['activation_102[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_112 (B  (None, 8, 8, 64)             256       ['conv2d_112[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_103 (Activation  (None, 8, 8, 64)             0         ['batch_normalization_112[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_113 (Conv2D)         (None, 8, 8, 256)            16640     ['activation_103[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_113 (B  (None, 8, 8, 256)            1024      ['conv2d_113[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_33 (Add)                (None, 8, 8, 256)            0         ['batch_normalization_113[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'activation_101[0][0]']      \n",
            "                                                                                                  \n",
            " activation_104 (Activation  (None, 8, 8, 256)            0         ['add_33[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_114 (Conv2D)         (None, 8, 8, 64)             16448     ['activation_104[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_114 (B  (None, 8, 8, 64)             256       ['conv2d_114[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_105 (Activation  (None, 8, 8, 64)             0         ['batch_normalization_114[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_115 (Conv2D)         (None, 8, 8, 64)             36928     ['activation_105[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_115 (B  (None, 8, 8, 64)             256       ['conv2d_115[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_106 (Activation  (None, 8, 8, 64)             0         ['batch_normalization_115[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_116 (Conv2D)         (None, 8, 8, 256)            16640     ['activation_106[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_116 (B  (None, 8, 8, 256)            1024      ['conv2d_116[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_34 (Add)                (None, 8, 8, 256)            0         ['batch_normalization_116[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'activation_104[0][0]']      \n",
            "                                                                                                  \n",
            " activation_107 (Activation  (None, 8, 8, 256)            0         ['add_34[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_117 (Conv2D)         (None, 4, 4, 128)            32896     ['activation_107[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_117 (B  (None, 4, 4, 128)            512       ['conv2d_117[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_108 (Activation  (None, 4, 4, 128)            0         ['batch_normalization_117[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_118 (Conv2D)         (None, 4, 4, 128)            147584    ['activation_108[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_118 (B  (None, 4, 4, 128)            512       ['conv2d_118[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_109 (Activation  (None, 4, 4, 128)            0         ['batch_normalization_118[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_119 (Conv2D)         (None, 4, 4, 512)            66048     ['activation_109[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_120 (Conv2D)         (None, 4, 4, 512)            131584    ['activation_107[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_119 (B  (None, 4, 4, 512)            2048      ['conv2d_119[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_120 (B  (None, 4, 4, 512)            2048      ['conv2d_120[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_35 (Add)                (None, 4, 4, 512)            0         ['batch_normalization_119[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_120[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_110 (Activation  (None, 4, 4, 512)            0         ['add_35[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_121 (Conv2D)         (None, 4, 4, 128)            65664     ['activation_110[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_121 (B  (None, 4, 4, 128)            512       ['conv2d_121[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_111 (Activation  (None, 4, 4, 128)            0         ['batch_normalization_121[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_122 (Conv2D)         (None, 4, 4, 128)            147584    ['activation_111[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_122 (B  (None, 4, 4, 128)            512       ['conv2d_122[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_112 (Activation  (None, 4, 4, 128)            0         ['batch_normalization_122[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_123 (Conv2D)         (None, 4, 4, 512)            66048     ['activation_112[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_123 (B  (None, 4, 4, 512)            2048      ['conv2d_123[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_36 (Add)                (None, 4, 4, 512)            0         ['batch_normalization_123[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'activation_110[0][0]']      \n",
            "                                                                                                  \n",
            " activation_113 (Activation  (None, 4, 4, 512)            0         ['add_36[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_124 (Conv2D)         (None, 4, 4, 128)            65664     ['activation_113[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_124 (B  (None, 4, 4, 128)            512       ['conv2d_124[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_114 (Activation  (None, 4, 4, 128)            0         ['batch_normalization_124[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_125 (Conv2D)         (None, 4, 4, 128)            147584    ['activation_114[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_125 (B  (None, 4, 4, 128)            512       ['conv2d_125[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_115 (Activation  (None, 4, 4, 128)            0         ['batch_normalization_125[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_126 (Conv2D)         (None, 4, 4, 512)            66048     ['activation_115[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_126 (B  (None, 4, 4, 512)            2048      ['conv2d_126[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_37 (Add)                (None, 4, 4, 512)            0         ['batch_normalization_126[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'activation_113[0][0]']      \n",
            "                                                                                                  \n",
            " activation_116 (Activation  (None, 4, 4, 512)            0         ['add_37[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_127 (Conv2D)         (None, 4, 4, 128)            65664     ['activation_116[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_127 (B  (None, 4, 4, 128)            512       ['conv2d_127[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_117 (Activation  (None, 4, 4, 128)            0         ['batch_normalization_127[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_128 (Conv2D)         (None, 4, 4, 128)            147584    ['activation_117[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_128 (B  (None, 4, 4, 128)            512       ['conv2d_128[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_118 (Activation  (None, 4, 4, 128)            0         ['batch_normalization_128[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_129 (Conv2D)         (None, 4, 4, 512)            66048     ['activation_118[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_129 (B  (None, 4, 4, 512)            2048      ['conv2d_129[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_38 (Add)                (None, 4, 4, 512)            0         ['batch_normalization_129[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'activation_116[0][0]']      \n",
            "                                                                                                  \n",
            " activation_119 (Activation  (None, 4, 4, 512)            0         ['add_38[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_130 (Conv2D)         (None, 2, 2, 256)            131328    ['activation_119[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_130 (B  (None, 2, 2, 256)            1024      ['conv2d_130[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_120 (Activation  (None, 2, 2, 256)            0         ['batch_normalization_130[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_131 (Conv2D)         (None, 2, 2, 256)            590080    ['activation_120[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_131 (B  (None, 2, 2, 256)            1024      ['conv2d_131[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_121 (Activation  (None, 2, 2, 256)            0         ['batch_normalization_131[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_132 (Conv2D)         (None, 2, 2, 1024)           263168    ['activation_121[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_133 (Conv2D)         (None, 2, 2, 1024)           525312    ['activation_119[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_132 (B  (None, 2, 2, 1024)           4096      ['conv2d_132[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_133 (B  (None, 2, 2, 1024)           4096      ['conv2d_133[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_39 (Add)                (None, 2, 2, 1024)           0         ['batch_normalization_132[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_133[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_122 (Activation  (None, 2, 2, 1024)           0         ['add_39[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_134 (Conv2D)         (None, 2, 2, 256)            262400    ['activation_122[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_134 (B  (None, 2, 2, 256)            1024      ['conv2d_134[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_123 (Activation  (None, 2, 2, 256)            0         ['batch_normalization_134[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_135 (Conv2D)         (None, 2, 2, 256)            590080    ['activation_123[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_135 (B  (None, 2, 2, 256)            1024      ['conv2d_135[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_124 (Activation  (None, 2, 2, 256)            0         ['batch_normalization_135[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_136 (Conv2D)         (None, 2, 2, 1024)           263168    ['activation_124[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_136 (B  (None, 2, 2, 1024)           4096      ['conv2d_136[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_40 (Add)                (None, 2, 2, 1024)           0         ['batch_normalization_136[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'activation_122[0][0]']      \n",
            "                                                                                                  \n",
            " activation_125 (Activation  (None, 2, 2, 1024)           0         ['add_40[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_137 (Conv2D)         (None, 2, 2, 256)            262400    ['activation_125[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_137 (B  (None, 2, 2, 256)            1024      ['conv2d_137[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_126 (Activation  (None, 2, 2, 256)            0         ['batch_normalization_137[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_138 (Conv2D)         (None, 2, 2, 256)            590080    ['activation_126[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_138 (B  (None, 2, 2, 256)            1024      ['conv2d_138[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_127 (Activation  (None, 2, 2, 256)            0         ['batch_normalization_138[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_139 (Conv2D)         (None, 2, 2, 1024)           263168    ['activation_127[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_139 (B  (None, 2, 2, 1024)           4096      ['conv2d_139[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_41 (Add)                (None, 2, 2, 1024)           0         ['batch_normalization_139[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'activation_125[0][0]']      \n",
            "                                                                                                  \n",
            " activation_128 (Activation  (None, 2, 2, 1024)           0         ['add_41[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_140 (Conv2D)         (None, 2, 2, 256)            262400    ['activation_128[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_140 (B  (None, 2, 2, 256)            1024      ['conv2d_140[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_129 (Activation  (None, 2, 2, 256)            0         ['batch_normalization_140[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_141 (Conv2D)         (None, 2, 2, 256)            590080    ['activation_129[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_141 (B  (None, 2, 2, 256)            1024      ['conv2d_141[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_130 (Activation  (None, 2, 2, 256)            0         ['batch_normalization_141[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_142 (Conv2D)         (None, 2, 2, 1024)           263168    ['activation_130[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_142 (B  (None, 2, 2, 1024)           4096      ['conv2d_142[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_42 (Add)                (None, 2, 2, 1024)           0         ['batch_normalization_142[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'activation_128[0][0]']      \n",
            "                                                                                                  \n",
            " activation_131 (Activation  (None, 2, 2, 1024)           0         ['add_42[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_143 (Conv2D)         (None, 2, 2, 256)            262400    ['activation_131[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_143 (B  (None, 2, 2, 256)            1024      ['conv2d_143[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_132 (Activation  (None, 2, 2, 256)            0         ['batch_normalization_143[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_144 (Conv2D)         (None, 2, 2, 256)            590080    ['activation_132[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_144 (B  (None, 2, 2, 256)            1024      ['conv2d_144[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_133 (Activation  (None, 2, 2, 256)            0         ['batch_normalization_144[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_145 (Conv2D)         (None, 2, 2, 1024)           263168    ['activation_133[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_145 (B  (None, 2, 2, 1024)           4096      ['conv2d_145[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_43 (Add)                (None, 2, 2, 1024)           0         ['batch_normalization_145[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'activation_131[0][0]']      \n",
            "                                                                                                  \n",
            " activation_134 (Activation  (None, 2, 2, 1024)           0         ['add_43[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_146 (Conv2D)         (None, 2, 2, 256)            262400    ['activation_134[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_146 (B  (None, 2, 2, 256)            1024      ['conv2d_146[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_135 (Activation  (None, 2, 2, 256)            0         ['batch_normalization_146[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_147 (Conv2D)         (None, 2, 2, 256)            590080    ['activation_135[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_147 (B  (None, 2, 2, 256)            1024      ['conv2d_147[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_136 (Activation  (None, 2, 2, 256)            0         ['batch_normalization_147[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_148 (Conv2D)         (None, 2, 2, 1024)           263168    ['activation_136[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_148 (B  (None, 2, 2, 1024)           4096      ['conv2d_148[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_44 (Add)                (None, 2, 2, 1024)           0         ['batch_normalization_148[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'activation_134[0][0]']      \n",
            "                                                                                                  \n",
            " activation_137 (Activation  (None, 2, 2, 1024)           0         ['add_44[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_149 (Conv2D)         (None, 1, 1, 512)            524800    ['activation_137[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_149 (B  (None, 1, 1, 512)            2048      ['conv2d_149[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_138 (Activation  (None, 1, 1, 512)            0         ['batch_normalization_149[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_150 (Conv2D)         (None, 1, 1, 512)            2359808   ['activation_138[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_150 (B  (None, 1, 1, 512)            2048      ['conv2d_150[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_139 (Activation  (None, 1, 1, 512)            0         ['batch_normalization_150[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_151 (Conv2D)         (None, 1, 1, 2048)           1050624   ['activation_139[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_152 (Conv2D)         (None, 1, 1, 2048)           2099200   ['activation_137[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_151 (B  (None, 1, 1, 2048)           8192      ['conv2d_151[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " batch_normalization_152 (B  (None, 1, 1, 2048)           8192      ['conv2d_152[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_45 (Add)                (None, 1, 1, 2048)           0         ['batch_normalization_151[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'batch_normalization_152[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " activation_140 (Activation  (None, 1, 1, 2048)           0         ['add_45[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_153 (Conv2D)         (None, 1, 1, 512)            1049088   ['activation_140[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_153 (B  (None, 1, 1, 512)            2048      ['conv2d_153[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_141 (Activation  (None, 1, 1, 512)            0         ['batch_normalization_153[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_154 (Conv2D)         (None, 1, 1, 512)            2359808   ['activation_141[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_154 (B  (None, 1, 1, 512)            2048      ['conv2d_154[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_142 (Activation  (None, 1, 1, 512)            0         ['batch_normalization_154[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_155 (Conv2D)         (None, 1, 1, 2048)           1050624   ['activation_142[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_155 (B  (None, 1, 1, 2048)           8192      ['conv2d_155[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_46 (Add)                (None, 1, 1, 2048)           0         ['batch_normalization_155[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'activation_140[0][0]']      \n",
            "                                                                                                  \n",
            " activation_143 (Activation  (None, 1, 1, 2048)           0         ['add_46[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_156 (Conv2D)         (None, 1, 1, 512)            1049088   ['activation_143[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_156 (B  (None, 1, 1, 512)            2048      ['conv2d_156[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_144 (Activation  (None, 1, 1, 512)            0         ['batch_normalization_156[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_157 (Conv2D)         (None, 1, 1, 512)            2359808   ['activation_144[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_157 (B  (None, 1, 1, 512)            2048      ['conv2d_157[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " activation_145 (Activation  (None, 1, 1, 512)            0         ['batch_normalization_157[0][0\n",
            " )                                                                  ]']                           \n",
            "                                                                                                  \n",
            " conv2d_158 (Conv2D)         (None, 1, 1, 2048)           1050624   ['activation_145[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_158 (B  (None, 1, 1, 2048)           8192      ['conv2d_158[0][0]']          \n",
            " atchNormalization)                                                                               \n",
            "                                                                                                  \n",
            " add_47 (Add)                (None, 1, 1, 2048)           0         ['batch_normalization_158[0][0\n",
            "                                                                    ]',                           \n",
            "                                                                     'activation_143[0][0]']      \n",
            "                                                                                                  \n",
            " activation_146 (Activation  (None, 1, 1, 2048)           0         ['add_47[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " average_pooling2d_2 (Avera  (None, 1, 1, 2048)           0         ['activation_146[0][0]']      \n",
            " gePooling2D)                                                                                     \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)         (None, 2048)                 0         ['average_pooling2d_2[0][0]'] \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 8)                    16392     ['flatten_2[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23604104 (90.04 MB)\n",
            "Trainable params: 23550984 (89.84 MB)\n",
            "Non-trainable params: 53120 (207.50 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.5e-6)\n",
        "csv_logger = CSVLogger(METRICS_PATH+MODEL_NAME+\".csv\")\n",
        "model_chekpoint = ModelCheckpoint(\"ResNet50_F1score_DA_aug.hdf5\",monitor = 'val_loss',verbose = 1,save_best_only=True)\n"
      ],
      "metadata": {
        "id": "Se5-8T6iM03A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 16\n",
        "data_augmentation = False\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "pDP7SmY6FdVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "if data_augmentation:\n",
        "    print(\"-------------Using Data augmentation------------\")\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True)  # randomly flip images\n",
        "\n",
        "    datagen.fit(x_train)\n",
        "    history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                        epochs=epochs, verbose=1, validation_data=(x_test, y_test),\n",
        "                        callbacks=[lr_reducer,  csv_logger, model_chekpoint])\n",
        "\n",
        "else:\n",
        "    print(\"-----Not Using Data augmentation---------------\")\n",
        "    history = model.fit(x_train, y_train,\n",
        "              batch_size=batch_size * 4,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True, callbacks=[lr_reducer,  csv_logger, model_chekpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkzCryBGFkyy",
        "outputId": "c90bf4dc-6780-4750-d749-d84e40187985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Not Using Data augmentation---------------\n",
            "Epoch 1/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 2.2309 - accuracy: 0.3136 - f1: 0.1361 - sensitivity: 0.0868 - specificity: 0.9816\n",
            "Epoch 1: val_loss improved from inf to 2.01674, saving model to ResNet50_F1score_DA_aug.hdf5\n",
            "101/101 [==============================] - 45s 97ms/step - loss: 2.2309 - accuracy: 0.3136 - f1: 0.1361 - sensitivity: 0.0868 - specificity: 0.9816 - val_loss: 2.0167 - val_accuracy: 0.2110 - val_f1: 0.0000e+00 - val_sensitivity: 0.0000e+00 - val_specificity: 1.0000 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 1.6763 - accuracy: 0.4830 - f1: 0.3892 - sensitivity: 0.2820 - specificity: 0.9784\n",
            "Epoch 2: val_loss did not improve from 2.01674\n",
            "101/101 [==============================] - 5s 51ms/step - loss: 1.6784 - accuracy: 0.4829 - f1: 0.3895 - sensitivity: 0.2830 - specificity: 0.9779 - val_loss: 2.2093 - val_accuracy: 0.2993 - val_f1: 0.2284 - val_sensitivity: 0.1629 - val_specificity: 0.9635 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 1.8437 - accuracy: 0.4069 - f1: 0.2855 - sensitivity: 0.1919 - specificity: 0.9825\n",
            "Epoch 3: val_loss did not improve from 2.01674\n",
            "101/101 [==============================] - 8s 77ms/step - loss: 1.8437 - accuracy: 0.4069 - f1: 0.2855 - sensitivity: 0.1919 - specificity: 0.9825 - val_loss: 2.9864 - val_accuracy: 0.3454 - val_f1: 0.2905 - val_sensitivity: 0.2124 - val_specificity: 0.9660 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 1.1824 - accuracy: 0.5963 - f1: 0.5506 - sensitivity: 0.4425 - specificity: 0.9777\n",
            "Epoch 4: val_loss improved from 2.01674 to 1.47494, saving model to ResNet50_F1score_DA_aug.hdf5\n",
            "101/101 [==============================] - 9s 94ms/step - loss: 1.1824 - accuracy: 0.5963 - f1: 0.5506 - sensitivity: 0.4425 - specificity: 0.9777 - val_loss: 1.4749 - val_accuracy: 0.4829 - val_f1: 0.4267 - val_sensitivity: 0.3248 - val_specificity: 0.9724 - lr: 3.1623e-04\n",
            "Epoch 5/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.8980 - accuracy: 0.7006 - f1: 0.6868 - sensitivity: 0.6058 - specificity: 0.9776\n",
            "Epoch 5: val_loss improved from 1.47494 to 1.28168, saving model to ResNet50_F1score_DA_aug.hdf5\n",
            "101/101 [==============================] - 7s 73ms/step - loss: 0.8985 - accuracy: 0.7003 - f1: 0.6860 - sensitivity: 0.6047 - specificity: 0.9776 - val_loss: 1.2817 - val_accuracy: 0.5582 - val_f1: 0.5170 - val_sensitivity: 0.4130 - val_specificity: 0.9742 - lr: 3.1623e-04\n",
            "Epoch 6/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.6557 - accuracy: 0.7763 - f1: 0.7731 - sensitivity: 0.7198 - specificity: 0.9800\n",
            "Epoch 6: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 51ms/step - loss: 0.6553 - accuracy: 0.7761 - f1: 0.7727 - sensitivity: 0.7196 - specificity: 0.9799 - val_loss: 1.2904 - val_accuracy: 0.5725 - val_f1: 0.5583 - val_sensitivity: 0.4852 - val_specificity: 0.9641 - lr: 3.1623e-04\n",
            "Epoch 7/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.5230 - accuracy: 0.8260 - f1: 0.8258 - sensitivity: 0.7898 - specificity: 0.9826\n",
            "Epoch 7: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 60ms/step - loss: 0.5230 - accuracy: 0.8260 - f1: 0.8258 - sensitivity: 0.7898 - specificity: 0.9826 - val_loss: 1.4466 - val_accuracy: 0.5881 - val_f1: 0.5912 - val_sensitivity: 0.5515 - val_specificity: 0.9554 - lr: 3.1623e-04\n",
            "Epoch 8/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 0.9326 - f1: 0.9332 - sensitivity: 0.9199 - specificity: 0.9927\n",
            "Epoch 8: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 51ms/step - loss: 0.2111 - accuracy: 0.9326 - f1: 0.9332 - sensitivity: 0.9199 - specificity: 0.9927 - val_loss: 1.2975 - val_accuracy: 0.6441 - val_f1: 0.6636 - val_sensitivity: 0.6309 - val_specificity: 0.9616 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.1048 - accuracy: 0.9656 - f1: 0.9656 - sensitivity: 0.9601 - specificity: 0.9959\n",
            "Epoch 9: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 61ms/step - loss: 0.1048 - accuracy: 0.9656 - f1: 0.9656 - sensitivity: 0.9601 - specificity: 0.9959 - val_loss: 1.4576 - val_accuracy: 0.6615 - val_f1: 0.6707 - val_sensitivity: 0.6471 - val_specificity: 0.9596 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0629 - accuracy: 0.9823 - f1: 0.9818 - sensitivity: 0.9791 - specificity: 0.9978\n",
            "Epoch 10: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 51ms/step - loss: 0.0627 - accuracy: 0.9824 - f1: 0.9820 - sensitivity: 0.9793 - specificity: 0.9978 - val_loss: 1.6137 - val_accuracy: 0.6640 - val_f1: 0.6751 - val_sensitivity: 0.6544 - val_specificity: 0.9592 - lr: 3.1623e-05\n",
            "Epoch 11/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9852 - f1: 0.9856 - sensitivity: 0.9838 - specificity: 0.9982\n",
            "Epoch 11: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 57ms/step - loss: 0.0464 - accuracy: 0.9852 - f1: 0.9856 - sensitivity: 0.9838 - specificity: 0.9982 - val_loss: 1.6790 - val_accuracy: 0.6627 - val_f1: 0.6702 - val_sensitivity: 0.6538 - val_specificity: 0.9577 - lr: 3.1623e-05\n",
            "Epoch 12/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9893 - f1: 0.9895 - sensitivity: 0.9885 - specificity: 0.9987\n",
            "Epoch 12: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 56ms/step - loss: 0.0369 - accuracy: 0.9893 - f1: 0.9895 - sensitivity: 0.9885 - specificity: 0.9987 - val_loss: 1.7642 - val_accuracy: 0.6584 - val_f1: 0.6667 - val_sensitivity: 0.6526 - val_specificity: 0.9565 - lr: 1.0000e-05\n",
            "Epoch 13/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9916 - f1: 0.9915 - sensitivity: 0.9905 - specificity: 0.9989\n",
            "Epoch 13: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 51ms/step - loss: 0.0316 - accuracy: 0.9916 - f1: 0.9915 - sensitivity: 0.9905 - specificity: 0.9989 - val_loss: 1.8104 - val_accuracy: 0.6646 - val_f1: 0.6709 - val_sensitivity: 0.6604 - val_specificity: 0.9561 - lr: 1.0000e-05\n",
            "Epoch 14/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9927 - f1: 0.9919 - sensitivity: 0.9909 - specificity: 0.9990\n",
            "Epoch 14: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 64ms/step - loss: 0.0277 - accuracy: 0.9927 - f1: 0.9919 - sensitivity: 0.9909 - specificity: 0.9990 - val_loss: 1.8197 - val_accuracy: 0.6609 - val_f1: 0.6701 - val_sensitivity: 0.6592 - val_specificity: 0.9560 - lr: 3.1623e-06\n",
            "Epoch 15/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9924 - f1: 0.9918 - sensitivity: 0.9914 - specificity: 0.9989\n",
            "Epoch 15: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 58ms/step - loss: 0.0236 - accuracy: 0.9924 - f1: 0.9918 - sensitivity: 0.9914 - specificity: 0.9989 - val_loss: 1.8369 - val_accuracy: 0.6646 - val_f1: 0.6734 - val_sensitivity: 0.6622 - val_specificity: 0.9566 - lr: 3.1623e-06\n",
            "Epoch 16/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0271 - accuracy: 0.9917 - f1: 0.9916 - sensitivity: 0.9911 - specificity: 0.9989\n",
            "Epoch 16: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 61ms/step - loss: 0.0278 - accuracy: 0.9916 - f1: 0.9913 - sensitivity: 0.9908 - specificity: 0.9988 - val_loss: 1.8389 - val_accuracy: 0.6652 - val_f1: 0.6701 - val_sensitivity: 0.6598 - val_specificity: 0.9559 - lr: 1.0000e-06\n",
            "Epoch 17/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9946 - f1: 0.9940 - sensitivity: 0.9934 - specificity: 0.9992\n",
            "Epoch 17: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 51ms/step - loss: 0.0193 - accuracy: 0.9946 - f1: 0.9940 - sensitivity: 0.9934 - specificity: 0.9992 - val_loss: 1.8397 - val_accuracy: 0.6640 - val_f1: 0.6698 - val_sensitivity: 0.6598 - val_specificity: 0.9558 - lr: 1.0000e-06\n",
            "Epoch 18/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9907 - f1: 0.9899 - sensitivity: 0.9886 - specificity: 0.9988\n",
            "Epoch 18: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 59ms/step - loss: 0.0322 - accuracy: 0.9907 - f1: 0.9899 - sensitivity: 0.9886 - specificity: 0.9988 - val_loss: 1.8559 - val_accuracy: 0.6633 - val_f1: 0.6700 - val_sensitivity: 0.6592 - val_specificity: 0.9560 - lr: 5.0000e-07\n",
            "Epoch 19/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9944 - f1: 0.9942 - sensitivity: 0.9935 - specificity: 0.9993\n",
            "Epoch 19: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 53ms/step - loss: 0.0216 - accuracy: 0.9944 - f1: 0.9942 - sensitivity: 0.9935 - specificity: 0.9993 - val_loss: 1.8485 - val_accuracy: 0.6640 - val_f1: 0.6726 - val_sensitivity: 0.6634 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 20/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9935 - f1: 0.9932 - sensitivity: 0.9924 - specificity: 0.9991\n",
            "Epoch 20: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 52ms/step - loss: 0.0277 - accuracy: 0.9935 - f1: 0.9932 - sensitivity: 0.9924 - specificity: 0.9991 - val_loss: 1.8509 - val_accuracy: 0.6640 - val_f1: 0.6707 - val_sensitivity: 0.6610 - val_specificity: 0.9558 - lr: 5.0000e-07\n",
            "Epoch 21/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9938 - f1: 0.9938 - sensitivity: 0.9930 - specificity: 0.9992\n",
            "Epoch 21: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 60ms/step - loss: 0.0206 - accuracy: 0.9938 - f1: 0.9938 - sensitivity: 0.9930 - specificity: 0.9992 - val_loss: 1.8542 - val_accuracy: 0.6627 - val_f1: 0.6710 - val_sensitivity: 0.6610 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 22/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0262 - accuracy: 0.9927 - f1: 0.9927 - sensitivity: 0.9927 - specificity: 0.9990\n",
            "Epoch 22: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 51ms/step - loss: 0.0265 - accuracy: 0.9925 - f1: 0.9924 - sensitivity: 0.9923 - specificity: 0.9989 - val_loss: 1.8461 - val_accuracy: 0.6627 - val_f1: 0.6701 - val_sensitivity: 0.6598 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 23/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0200 - accuracy: 0.9944 - f1: 0.9943 - sensitivity: 0.9941 - specificity: 0.9992\n",
            "Epoch 23: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 61ms/step - loss: 0.0217 - accuracy: 0.9939 - f1: 0.9934 - sensitivity: 0.9930 - specificity: 0.9991 - val_loss: 1.8416 - val_accuracy: 0.6627 - val_f1: 0.6699 - val_sensitivity: 0.6610 - val_specificity: 0.9554 - lr: 5.0000e-07\n",
            "Epoch 24/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9927 - f1: 0.9929 - sensitivity: 0.9923 - specificity: 0.9991\n",
            "Epoch 24: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 51ms/step - loss: 0.0215 - accuracy: 0.9927 - f1: 0.9929 - sensitivity: 0.9923 - specificity: 0.9991 - val_loss: 1.8574 - val_accuracy: 0.6627 - val_f1: 0.6697 - val_sensitivity: 0.6604 - val_specificity: 0.9555 - lr: 5.0000e-07\n",
            "Epoch 25/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9930 - f1: 0.9930 - sensitivity: 0.9927 - specificity: 0.9991\n",
            "Epoch 25: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 62ms/step - loss: 0.0248 - accuracy: 0.9930 - f1: 0.9930 - sensitivity: 0.9927 - specificity: 0.9991 - val_loss: 1.8583 - val_accuracy: 0.6640 - val_f1: 0.6719 - val_sensitivity: 0.6628 - val_specificity: 0.9558 - lr: 5.0000e-07\n",
            "Epoch 26/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9949 - f1: 0.9939 - sensitivity: 0.9935 - specificity: 0.9992\n",
            "Epoch 26: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 9s 86ms/step - loss: 0.0193 - accuracy: 0.9949 - f1: 0.9939 - sensitivity: 0.9935 - specificity: 0.9992 - val_loss: 1.8454 - val_accuracy: 0.6646 - val_f1: 0.6725 - val_sensitivity: 0.6634 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 27/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9941 - f1: 0.9938 - sensitivity: 0.9934 - specificity: 0.9992\n",
            "Epoch 27: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 7s 69ms/step - loss: 0.0194 - accuracy: 0.9941 - f1: 0.9938 - sensitivity: 0.9934 - specificity: 0.9992 - val_loss: 1.8424 - val_accuracy: 0.6627 - val_f1: 0.6710 - val_sensitivity: 0.6622 - val_specificity: 0.9556 - lr: 5.0000e-07\n",
            "Epoch 28/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0180 - accuracy: 0.9952 - f1: 0.9951 - sensitivity: 0.9942 - specificity: 0.9994\n",
            "Epoch 28: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 51ms/step - loss: 0.0180 - accuracy: 0.9952 - f1: 0.9951 - sensitivity: 0.9943 - specificity: 0.9994 - val_loss: 1.8444 - val_accuracy: 0.6627 - val_f1: 0.6701 - val_sensitivity: 0.6616 - val_specificity: 0.9553 - lr: 5.0000e-07\n",
            "Epoch 29/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9936 - f1: 0.9927 - sensitivity: 0.9924 - specificity: 0.9990\n",
            "Epoch 29: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 55ms/step - loss: 0.0225 - accuracy: 0.9936 - f1: 0.9927 - sensitivity: 0.9924 - specificity: 0.9990 - val_loss: 1.8568 - val_accuracy: 0.6640 - val_f1: 0.6713 - val_sensitivity: 0.6622 - val_specificity: 0.9557 - lr: 5.0000e-07\n",
            "Epoch 30/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9949 - f1: 0.9953 - sensitivity: 0.9941 - specificity: 0.9995\n",
            "Epoch 30: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 57ms/step - loss: 0.0169 - accuracy: 0.9949 - f1: 0.9953 - sensitivity: 0.9941 - specificity: 0.9995 - val_loss: 1.8642 - val_accuracy: 0.6646 - val_f1: 0.6717 - val_sensitivity: 0.6628 - val_specificity: 0.9557 - lr: 5.0000e-07\n",
            "Epoch 31/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9935 - f1: 0.9934 - sensitivity: 0.9933 - specificity: 0.9991\n",
            "Epoch 31: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 51ms/step - loss: 0.0202 - accuracy: 0.9935 - f1: 0.9934 - sensitivity: 0.9933 - specificity: 0.9991 - val_loss: 1.8687 - val_accuracy: 0.6640 - val_f1: 0.6717 - val_sensitivity: 0.6622 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 32/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0220 - accuracy: 0.9942 - f1: 0.9944 - sensitivity: 0.9934 - specificity: 0.9993\n",
            "Epoch 32: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 60ms/step - loss: 0.0220 - accuracy: 0.9942 - f1: 0.9944 - sensitivity: 0.9935 - specificity: 0.9993 - val_loss: 1.8699 - val_accuracy: 0.6646 - val_f1: 0.6733 - val_sensitivity: 0.6646 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 33/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0205 - accuracy: 0.9942 - f1: 0.9943 - sensitivity: 0.9934 - specificity: 0.9993\n",
            "Epoch 33: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 52ms/step - loss: 0.0214 - accuracy: 0.9939 - f1: 0.9934 - sensitivity: 0.9924 - specificity: 0.9992 - val_loss: 1.8635 - val_accuracy: 0.6658 - val_f1: 0.6745 - val_sensitivity: 0.6652 - val_specificity: 0.9562 - lr: 5.0000e-07\n",
            "Epoch 34/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9950 - f1: 0.9948 - sensitivity: 0.9942 - specificity: 0.9993\n",
            "Epoch 34: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 60ms/step - loss: 0.0192 - accuracy: 0.9950 - f1: 0.9948 - sensitivity: 0.9942 - specificity: 0.9993 - val_loss: 1.8635 - val_accuracy: 0.6658 - val_f1: 0.6751 - val_sensitivity: 0.6658 - val_specificity: 0.9563 - lr: 5.0000e-07\n",
            "Epoch 35/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0235 - accuracy: 0.9931 - f1: 0.9932 - sensitivity: 0.9925 - specificity: 0.9991\n",
            "Epoch 35: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 52ms/step - loss: 0.0234 - accuracy: 0.9932 - f1: 0.9933 - sensitivity: 0.9926 - specificity: 0.9991 - val_loss: 1.8689 - val_accuracy: 0.6671 - val_f1: 0.6753 - val_sensitivity: 0.6658 - val_specificity: 0.9564 - lr: 5.0000e-07\n",
            "Epoch 36/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9938 - f1: 0.9928 - sensitivity: 0.9921 - specificity: 0.9991\n",
            "Epoch 36: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 63ms/step - loss: 0.0252 - accuracy: 0.9938 - f1: 0.9928 - sensitivity: 0.9921 - specificity: 0.9991 - val_loss: 1.8704 - val_accuracy: 0.6658 - val_f1: 0.6730 - val_sensitivity: 0.6634 - val_specificity: 0.9561 - lr: 5.0000e-07\n",
            "Epoch 37/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9930 - f1: 0.9929 - sensitivity: 0.9920 - specificity: 0.9991\n",
            "Epoch 37: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 59ms/step - loss: 0.0254 - accuracy: 0.9930 - f1: 0.9929 - sensitivity: 0.9920 - specificity: 0.9991 - val_loss: 1.8743 - val_accuracy: 0.6646 - val_f1: 0.6733 - val_sensitivity: 0.6628 - val_specificity: 0.9564 - lr: 5.0000e-07\n",
            "Epoch 38/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0183 - accuracy: 0.9959 - f1: 0.9958 - sensitivity: 0.9955 - specificity: 0.9995\n",
            "Epoch 38: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 51ms/step - loss: 0.0184 - accuracy: 0.9960 - f1: 0.9957 - sensitivity: 0.9951 - specificity: 0.9995 - val_loss: 1.8686 - val_accuracy: 0.6646 - val_f1: 0.6732 - val_sensitivity: 0.6622 - val_specificity: 0.9565 - lr: 5.0000e-07\n",
            "Epoch 39/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9947 - f1: 0.9947 - sensitivity: 0.9943 - specificity: 0.9993\n",
            "Epoch 39: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 61ms/step - loss: 0.0185 - accuracy: 0.9947 - f1: 0.9947 - sensitivity: 0.9943 - specificity: 0.9993 - val_loss: 1.8591 - val_accuracy: 0.6658 - val_f1: 0.6747 - val_sensitivity: 0.6640 - val_specificity: 0.9566 - lr: 5.0000e-07\n",
            "Epoch 40/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0225 - accuracy: 0.9934 - f1: 0.9935 - sensitivity: 0.9930 - specificity: 0.9992\n",
            "Epoch 40: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 52ms/step - loss: 0.0236 - accuracy: 0.9928 - f1: 0.9920 - sensitivity: 0.9915 - specificity: 0.9989 - val_loss: 1.8721 - val_accuracy: 0.6640 - val_f1: 0.6740 - val_sensitivity: 0.6640 - val_specificity: 0.9564 - lr: 5.0000e-07\n",
            "Epoch 41/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9969 - f1: 0.9967 - sensitivity: 0.9963 - specificity: 0.9996\n",
            "Epoch 41: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 62ms/step - loss: 0.0158 - accuracy: 0.9969 - f1: 0.9967 - sensitivity: 0.9963 - specificity: 0.9996 - val_loss: 1.8685 - val_accuracy: 0.6665 - val_f1: 0.6740 - val_sensitivity: 0.6640 - val_specificity: 0.9564 - lr: 5.0000e-07\n",
            "Epoch 42/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9942 - f1: 0.9947 - sensitivity: 0.9940 - specificity: 0.9994\n",
            "Epoch 42: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 51ms/step - loss: 0.0190 - accuracy: 0.9942 - f1: 0.9947 - sensitivity: 0.9940 - specificity: 0.9994 - val_loss: 1.8787 - val_accuracy: 0.6677 - val_f1: 0.6753 - val_sensitivity: 0.6658 - val_specificity: 0.9564 - lr: 5.0000e-07\n",
            "Epoch 43/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0197 - accuracy: 0.9944 - f1: 0.9946 - sensitivity: 0.9940 - specificity: 0.9993\n",
            "Epoch 43: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 53ms/step - loss: 0.0197 - accuracy: 0.9944 - f1: 0.9946 - sensitivity: 0.9940 - specificity: 0.9993 - val_loss: 1.8799 - val_accuracy: 0.6671 - val_f1: 0.6760 - val_sensitivity: 0.6658 - val_specificity: 0.9566 - lr: 5.0000e-07\n",
            "Epoch 44/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9961 - f1: 0.9959 - sensitivity: 0.9956 - specificity: 0.9995\n",
            "Epoch 44: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 58ms/step - loss: 0.0154 - accuracy: 0.9961 - f1: 0.9959 - sensitivity: 0.9956 - specificity: 0.9995 - val_loss: 1.8848 - val_accuracy: 0.6633 - val_f1: 0.6736 - val_sensitivity: 0.6646 - val_specificity: 0.9560 - lr: 5.0000e-07\n",
            "Epoch 45/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9947 - f1: 0.9943 - sensitivity: 0.9937 - specificity: 0.9993\n",
            "Epoch 45: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 51ms/step - loss: 0.0234 - accuracy: 0.9947 - f1: 0.9943 - sensitivity: 0.9937 - specificity: 0.9993 - val_loss: 1.8898 - val_accuracy: 0.6665 - val_f1: 0.6757 - val_sensitivity: 0.6652 - val_specificity: 0.9567 - lr: 5.0000e-07\n",
            "Epoch 46/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 0.9969 - f1: 0.9967 - sensitivity: 0.9961 - specificity: 0.9996\n",
            "Epoch 46: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 61ms/step - loss: 0.0144 - accuracy: 0.9969 - f1: 0.9967 - sensitivity: 0.9961 - specificity: 0.9996 - val_loss: 1.8834 - val_accuracy: 0.6665 - val_f1: 0.6745 - val_sensitivity: 0.6652 - val_specificity: 0.9562 - lr: 5.0000e-07\n",
            "Epoch 47/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0165 - accuracy: 0.9947 - f1: 0.9947 - sensitivity: 0.9945 - specificity: 0.9993\n",
            "Epoch 47: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 52ms/step - loss: 0.0165 - accuracy: 0.9947 - f1: 0.9947 - sensitivity: 0.9946 - specificity: 0.9993 - val_loss: 1.8923 - val_accuracy: 0.6658 - val_f1: 0.6747 - val_sensitivity: 0.6652 - val_specificity: 0.9563 - lr: 5.0000e-07\n",
            "Epoch 48/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9960 - f1: 0.9958 - sensitivity: 0.9954 - specificity: 0.9995\n",
            "Epoch 48: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 61ms/step - loss: 0.0162 - accuracy: 0.9960 - f1: 0.9958 - sensitivity: 0.9954 - specificity: 0.9995 - val_loss: 1.9039 - val_accuracy: 0.6671 - val_f1: 0.6754 - val_sensitivity: 0.6658 - val_specificity: 0.9564 - lr: 5.0000e-07\n",
            "Epoch 49/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9944 - f1: 0.9940 - sensitivity: 0.9934 - specificity: 0.9992\n",
            "Epoch 49: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 8s 74ms/step - loss: 0.0201 - accuracy: 0.9944 - f1: 0.9940 - sensitivity: 0.9934 - specificity: 0.9992 - val_loss: 1.8998 - val_accuracy: 0.6677 - val_f1: 0.6751 - val_sensitivity: 0.6646 - val_specificity: 0.9566 - lr: 5.0000e-07\n",
            "Epoch 50/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9967 - f1: 0.9967 - sensitivity: 0.9964 - specificity: 0.9996\n",
            "Epoch 50: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 7s 67ms/step - loss: 0.0147 - accuracy: 0.9967 - f1: 0.9967 - sensitivity: 0.9964 - specificity: 0.9996 - val_loss: 1.9026 - val_accuracy: 0.6696 - val_f1: 0.6761 - val_sensitivity: 0.6658 - val_specificity: 0.9567 - lr: 5.0000e-07\n",
            "Epoch 51/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0177 - accuracy: 0.9952 - f1: 0.9953 - sensitivity: 0.9948 - specificity: 0.9994\n",
            "Epoch 51: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 52ms/step - loss: 0.0180 - accuracy: 0.9949 - f1: 0.9946 - sensitivity: 0.9941 - specificity: 0.9993 - val_loss: 1.9019 - val_accuracy: 0.6677 - val_f1: 0.6763 - val_sensitivity: 0.6670 - val_specificity: 0.9565 - lr: 5.0000e-07\n",
            "Epoch 52/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9952 - f1: 0.9947 - sensitivity: 0.9943 - specificity: 0.9993\n",
            "Epoch 52: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 7s 71ms/step - loss: 0.0178 - accuracy: 0.9952 - f1: 0.9947 - sensitivity: 0.9943 - specificity: 0.9993 - val_loss: 1.9116 - val_accuracy: 0.6646 - val_f1: 0.6739 - val_sensitivity: 0.6646 - val_specificity: 0.9561 - lr: 5.0000e-07\n",
            "Epoch 53/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9960 - f1: 0.9958 - sensitivity: 0.9956 - specificity: 0.9994\n",
            "Epoch 53: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 53ms/step - loss: 0.0188 - accuracy: 0.9960 - f1: 0.9958 - sensitivity: 0.9956 - specificity: 0.9994 - val_loss: 1.9084 - val_accuracy: 0.6683 - val_f1: 0.6765 - val_sensitivity: 0.6676 - val_specificity: 0.9564 - lr: 5.0000e-07\n",
            "Epoch 54/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0173 - accuracy: 0.9942 - f1: 0.9943 - sensitivity: 0.9936 - specificity: 0.9993\n",
            "Epoch 54: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 52ms/step - loss: 0.0173 - accuracy: 0.9942 - f1: 0.9943 - sensitivity: 0.9937 - specificity: 0.9993 - val_loss: 1.9036 - val_accuracy: 0.6683 - val_f1: 0.6760 - val_sensitivity: 0.6658 - val_specificity: 0.9566 - lr: 5.0000e-07\n",
            "Epoch 55/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9962 - f1: 0.9962 - sensitivity: 0.9962 - specificity: 0.9995\n",
            "Epoch 55: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 60ms/step - loss: 0.0150 - accuracy: 0.9960 - f1: 0.9955 - sensitivity: 0.9955 - specificity: 0.9994 - val_loss: 1.9115 - val_accuracy: 0.6677 - val_f1: 0.6768 - val_sensitivity: 0.6688 - val_specificity: 0.9561 - lr: 5.0000e-07\n",
            "Epoch 56/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9961 - f1: 0.9960 - sensitivity: 0.9953 - specificity: 0.9995\n",
            "Epoch 56: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 52ms/step - loss: 0.0161 - accuracy: 0.9961 - f1: 0.9960 - sensitivity: 0.9953 - specificity: 0.9995 - val_loss: 1.9144 - val_accuracy: 0.6671 - val_f1: 0.6756 - val_sensitivity: 0.6676 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 57/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0137 - accuracy: 0.9967 - f1: 0.9967 - sensitivity: 0.9962 - specificity: 0.9996\n",
            "Epoch 57: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 61ms/step - loss: 0.0144 - accuracy: 0.9966 - f1: 0.9964 - sensitivity: 0.9959 - specificity: 0.9995 - val_loss: 1.9118 - val_accuracy: 0.6696 - val_f1: 0.6757 - val_sensitivity: 0.6664 - val_specificity: 0.9564 - lr: 5.0000e-07\n",
            "Epoch 58/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9953 - f1: 0.9955 - sensitivity: 0.9950 - specificity: 0.9994\n",
            "Epoch 58: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 53ms/step - loss: 0.0152 - accuracy: 0.9953 - f1: 0.9955 - sensitivity: 0.9950 - specificity: 0.9994 - val_loss: 1.9170 - val_accuracy: 0.6696 - val_f1: 0.6786 - val_sensitivity: 0.6694 - val_specificity: 0.9567 - lr: 5.0000e-07\n",
            "Epoch 59/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9936 - f1: 0.9938 - sensitivity: 0.9933 - specificity: 0.9992\n",
            "Epoch 59: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 64ms/step - loss: 0.0207 - accuracy: 0.9936 - f1: 0.9938 - sensitivity: 0.9933 - specificity: 0.9992 - val_loss: 1.9277 - val_accuracy: 0.6665 - val_f1: 0.6741 - val_sensitivity: 0.6640 - val_specificity: 0.9564 - lr: 5.0000e-07\n",
            "Epoch 60/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9955 - f1: 0.9948 - sensitivity: 0.9943 - specificity: 0.9993\n",
            "Epoch 60: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 63ms/step - loss: 0.0164 - accuracy: 0.9955 - f1: 0.9948 - sensitivity: 0.9943 - specificity: 0.9993 - val_loss: 1.9060 - val_accuracy: 0.6683 - val_f1: 0.6769 - val_sensitivity: 0.6670 - val_specificity: 0.9567 - lr: 5.0000e-07\n",
            "Epoch 61/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9942 - f1: 0.9941 - sensitivity: 0.9935 - specificity: 0.9992\n",
            "Epoch 61: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 53ms/step - loss: 0.0204 - accuracy: 0.9942 - f1: 0.9941 - sensitivity: 0.9935 - specificity: 0.9992 - val_loss: 1.9233 - val_accuracy: 0.6665 - val_f1: 0.6750 - val_sensitivity: 0.6658 - val_specificity: 0.9562 - lr: 5.0000e-07\n",
            "Epoch 62/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0124 - accuracy: 0.9966 - f1: 0.9967 - sensitivity: 0.9964 - specificity: 0.9996\n",
            "Epoch 62: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 61ms/step - loss: 0.0146 - accuracy: 0.9958 - f1: 0.9948 - sensitivity: 0.9945 - specificity: 0.9993 - val_loss: 1.9291 - val_accuracy: 0.6677 - val_f1: 0.6760 - val_sensitivity: 0.6658 - val_specificity: 0.9566 - lr: 5.0000e-07\n",
            "Epoch 63/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0160 - accuracy: 0.9953 - f1: 0.9955 - sensitivity: 0.9948 - specificity: 0.9995\n",
            "Epoch 63: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 52ms/step - loss: 0.0160 - accuracy: 0.9953 - f1: 0.9956 - sensitivity: 0.9949 - specificity: 0.9995 - val_loss: 1.9210 - val_accuracy: 0.6677 - val_f1: 0.6760 - val_sensitivity: 0.6664 - val_specificity: 0.9565 - lr: 5.0000e-07\n",
            "Epoch 64/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9960 - f1: 0.9961 - sensitivity: 0.9960 - specificity: 0.9995\n",
            "Epoch 64: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 62ms/step - loss: 0.0143 - accuracy: 0.9960 - f1: 0.9961 - sensitivity: 0.9960 - specificity: 0.9995 - val_loss: 1.9327 - val_accuracy: 0.6677 - val_f1: 0.6745 - val_sensitivity: 0.6652 - val_specificity: 0.9562 - lr: 5.0000e-07\n",
            "Epoch 65/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9970 - f1: 0.9968 - sensitivity: 0.9967 - specificity: 0.9996\n",
            "Epoch 65: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 52ms/step - loss: 0.0108 - accuracy: 0.9970 - f1: 0.9968 - sensitivity: 0.9967 - specificity: 0.9996 - val_loss: 1.9322 - val_accuracy: 0.6683 - val_f1: 0.6753 - val_sensitivity: 0.6658 - val_specificity: 0.9564 - lr: 5.0000e-07\n",
            "Epoch 66/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9955 - f1: 0.9953 - sensitivity: 0.9947 - specificity: 0.9994\n",
            "Epoch 66: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 63ms/step - loss: 0.0190 - accuracy: 0.9955 - f1: 0.9953 - sensitivity: 0.9947 - specificity: 0.9994 - val_loss: 1.9366 - val_accuracy: 0.6683 - val_f1: 0.6744 - val_sensitivity: 0.6652 - val_specificity: 0.9561 - lr: 5.0000e-07\n",
            "Epoch 67/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0157 - accuracy: 0.9956 - f1: 0.9957 - sensitivity: 0.9955 - specificity: 0.9994\n",
            "Epoch 67: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 53ms/step - loss: 0.0158 - accuracy: 0.9956 - f1: 0.9957 - sensitivity: 0.9955 - specificity: 0.9994 - val_loss: 1.9478 - val_accuracy: 0.6640 - val_f1: 0.6733 - val_sensitivity: 0.6646 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 68/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9953 - f1: 0.9952 - sensitivity: 0.9950 - specificity: 0.9993\n",
            "Epoch 68: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 53ms/step - loss: 0.0159 - accuracy: 0.9953 - f1: 0.9952 - sensitivity: 0.9950 - specificity: 0.9993 - val_loss: 1.9411 - val_accuracy: 0.6665 - val_f1: 0.6743 - val_sensitivity: 0.6652 - val_specificity: 0.9561 - lr: 5.0000e-07\n",
            "Epoch 69/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0155 - accuracy: 0.9958 - f1: 0.9959 - sensitivity: 0.9958 - specificity: 0.9994\n",
            "Epoch 69: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 7s 71ms/step - loss: 0.0157 - accuracy: 0.9956 - f1: 0.9956 - sensitivity: 0.9954 - specificity: 0.9994 - val_loss: 1.9322 - val_accuracy: 0.6671 - val_f1: 0.6753 - val_sensitivity: 0.6664 - val_specificity: 0.9562 - lr: 5.0000e-07\n",
            "Epoch 70/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0164 - accuracy: 0.9961 - f1: 0.9955 - sensitivity: 0.9953 - specificity: 0.9994\n",
            "Epoch 70: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 52ms/step - loss: 0.0164 - accuracy: 0.9961 - f1: 0.9955 - sensitivity: 0.9953 - specificity: 0.9994 - val_loss: 1.9340 - val_accuracy: 0.6683 - val_f1: 0.6745 - val_sensitivity: 0.6658 - val_specificity: 0.9560 - lr: 5.0000e-07\n",
            "Epoch 71/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9950 - f1: 0.9949 - sensitivity: 0.9942 - specificity: 0.9994\n",
            "Epoch 71: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 8s 75ms/step - loss: 0.0177 - accuracy: 0.9950 - f1: 0.9949 - sensitivity: 0.9942 - specificity: 0.9994 - val_loss: 1.9469 - val_accuracy: 0.6658 - val_f1: 0.6735 - val_sensitivity: 0.6652 - val_specificity: 0.9558 - lr: 5.0000e-07\n",
            "Epoch 72/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0181 - accuracy: 0.9937 - f1: 0.9941 - sensitivity: 0.9937 - specificity: 0.9992\n",
            "Epoch 72: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 7s 71ms/step - loss: 0.0185 - accuracy: 0.9935 - f1: 0.9934 - sensitivity: 0.9931 - specificity: 0.9991 - val_loss: 1.9342 - val_accuracy: 0.6671 - val_f1: 0.6751 - val_sensitivity: 0.6670 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 73/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9958 - f1: 0.9958 - sensitivity: 0.9953 - specificity: 0.9995\n",
            "Epoch 73: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 61ms/step - loss: 0.0128 - accuracy: 0.9958 - f1: 0.9958 - sensitivity: 0.9954 - specificity: 0.9995 - val_loss: 1.9492 - val_accuracy: 0.6665 - val_f1: 0.6769 - val_sensitivity: 0.6688 - val_specificity: 0.9562 - lr: 5.0000e-07\n",
            "Epoch 74/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0177 - accuracy: 0.9948 - f1: 0.9949 - sensitivity: 0.9947 - specificity: 0.9993\n",
            "Epoch 74: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 52ms/step - loss: 0.0182 - accuracy: 0.9947 - f1: 0.9944 - sensitivity: 0.9940 - specificity: 0.9993 - val_loss: 1.9426 - val_accuracy: 0.6689 - val_f1: 0.6758 - val_sensitivity: 0.6676 - val_specificity: 0.9560 - lr: 5.0000e-07\n",
            "Epoch 75/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9963 - f1: 0.9961 - sensitivity: 0.9951 - specificity: 0.9996\n",
            "Epoch 75: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 61ms/step - loss: 0.0144 - accuracy: 0.9963 - f1: 0.9961 - sensitivity: 0.9951 - specificity: 0.9996 - val_loss: 1.9342 - val_accuracy: 0.6683 - val_f1: 0.6758 - val_sensitivity: 0.6670 - val_specificity: 0.9562 - lr: 5.0000e-07\n",
            "Epoch 76/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0142 - accuracy: 0.9964 - f1: 0.9963 - sensitivity: 0.9962 - specificity: 0.9995\n",
            "Epoch 76: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 53ms/step - loss: 0.0144 - accuracy: 0.9963 - f1: 0.9960 - sensitivity: 0.9959 - specificity: 0.9994 - val_loss: 1.9442 - val_accuracy: 0.6658 - val_f1: 0.6739 - val_sensitivity: 0.6664 - val_specificity: 0.9556 - lr: 5.0000e-07\n",
            "Epoch 77/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9949 - f1: 0.9951 - sensitivity: 0.9946 - specificity: 0.9994\n",
            "Epoch 77: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 63ms/step - loss: 0.0166 - accuracy: 0.9949 - f1: 0.9951 - sensitivity: 0.9946 - specificity: 0.9994 - val_loss: 1.9542 - val_accuracy: 0.6633 - val_f1: 0.6731 - val_sensitivity: 0.6658 - val_specificity: 0.9554 - lr: 5.0000e-07\n",
            "Epoch 78/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9958 - f1: 0.9959 - sensitivity: 0.9957 - specificity: 0.9994\n",
            "Epoch 78: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 53ms/step - loss: 0.0163 - accuracy: 0.9958 - f1: 0.9959 - sensitivity: 0.9957 - specificity: 0.9994 - val_loss: 1.9588 - val_accuracy: 0.6665 - val_f1: 0.6742 - val_sensitivity: 0.6664 - val_specificity: 0.9557 - lr: 5.0000e-07\n",
            "Epoch 79/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9952 - f1: 0.9954 - sensitivity: 0.9949 - specificity: 0.9994\n",
            "Epoch 79: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 59ms/step - loss: 0.0173 - accuracy: 0.9952 - f1: 0.9954 - sensitivity: 0.9949 - specificity: 0.9994 - val_loss: 1.9581 - val_accuracy: 0.6677 - val_f1: 0.6744 - val_sensitivity: 0.6658 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 80/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0142 - accuracy: 0.9964 - f1: 0.9966 - sensitivity: 0.9964 - specificity: 0.9995\n",
            "Epoch 80: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 58ms/step - loss: 0.0143 - accuracy: 0.9963 - f1: 0.9962 - sensitivity: 0.9961 - specificity: 0.9995 - val_loss: 1.9591 - val_accuracy: 0.6677 - val_f1: 0.6729 - val_sensitivity: 0.6634 - val_specificity: 0.9560 - lr: 5.0000e-07\n",
            "Epoch 81/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0137 - accuracy: 0.9972 - f1: 0.9972 - sensitivity: 0.9969 - specificity: 0.9996\n",
            "Epoch 81: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 53ms/step - loss: 0.0137 - accuracy: 0.9972 - f1: 0.9972 - sensitivity: 0.9969 - specificity: 0.9996 - val_loss: 1.9580 - val_accuracy: 0.6677 - val_f1: 0.6735 - val_sensitivity: 0.6646 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 82/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0118 - accuracy: 0.9970 - f1: 0.9969 - sensitivity: 0.9966 - specificity: 0.9996\n",
            "Epoch 82: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 62ms/step - loss: 0.0130 - accuracy: 0.9966 - f1: 0.9958 - sensitivity: 0.9955 - specificity: 0.9995 - val_loss: 1.9596 - val_accuracy: 0.6652 - val_f1: 0.6725 - val_sensitivity: 0.6634 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 83/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9970 - f1: 0.9966 - sensitivity: 0.9965 - specificity: 0.9995\n",
            "Epoch 83: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 53ms/step - loss: 0.0122 - accuracy: 0.9970 - f1: 0.9966 - sensitivity: 0.9965 - specificity: 0.9995 - val_loss: 1.9676 - val_accuracy: 0.6671 - val_f1: 0.6757 - val_sensitivity: 0.6682 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 84/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9952 - f1: 0.9955 - sensitivity: 0.9950 - specificity: 0.9994\n",
            "Epoch 84: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 61ms/step - loss: 0.0178 - accuracy: 0.9952 - f1: 0.9955 - sensitivity: 0.9950 - specificity: 0.9994 - val_loss: 1.9640 - val_accuracy: 0.6652 - val_f1: 0.6736 - val_sensitivity: 0.6664 - val_specificity: 0.9554 - lr: 5.0000e-07\n",
            "Epoch 85/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.9953 - f1: 0.9951 - sensitivity: 0.9946 - specificity: 0.9994\n",
            "Epoch 85: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 53ms/step - loss: 0.0159 - accuracy: 0.9953 - f1: 0.9951 - sensitivity: 0.9946 - specificity: 0.9994 - val_loss: 1.9710 - val_accuracy: 0.6714 - val_f1: 0.6762 - val_sensitivity: 0.6688 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 86/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9961 - f1: 0.9961 - sensitivity: 0.9960 - specificity: 0.9994\n",
            "Epoch 86: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 59ms/step - loss: 0.0142 - accuracy: 0.9961 - f1: 0.9961 - sensitivity: 0.9960 - specificity: 0.9994 - val_loss: 1.9619 - val_accuracy: 0.6696 - val_f1: 0.6762 - val_sensitivity: 0.6676 - val_specificity: 0.9562 - lr: 5.0000e-07\n",
            "Epoch 87/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0184 - accuracy: 0.9945 - f1: 0.9948 - sensitivity: 0.9945 - specificity: 0.9993\n",
            "Epoch 87: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 56ms/step - loss: 0.0185 - accuracy: 0.9946 - f1: 0.9948 - sensitivity: 0.9946 - specificity: 0.9993 - val_loss: 1.9708 - val_accuracy: 0.6671 - val_f1: 0.6744 - val_sensitivity: 0.6664 - val_specificity: 0.9558 - lr: 5.0000e-07\n",
            "Epoch 88/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9967 - f1: 0.9964 - sensitivity: 0.9961 - specificity: 0.9995\n",
            "Epoch 88: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 53ms/step - loss: 0.0158 - accuracy: 0.9967 - f1: 0.9964 - sensitivity: 0.9961 - specificity: 0.9995 - val_loss: 1.9738 - val_accuracy: 0.6671 - val_f1: 0.6750 - val_sensitivity: 0.6670 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 89/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9961 - f1: 0.9961 - sensitivity: 0.9958 - specificity: 0.9995\n",
            "Epoch 89: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 7s 65ms/step - loss: 0.0130 - accuracy: 0.9961 - f1: 0.9961 - sensitivity: 0.9958 - specificity: 0.9995 - val_loss: 1.9638 - val_accuracy: 0.6658 - val_f1: 0.6747 - val_sensitivity: 0.6664 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 90/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0095 - accuracy: 0.9977 - f1: 0.9977 - sensitivity: 0.9977 - specificity: 0.9997\n",
            "Epoch 90: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 60ms/step - loss: 0.0120 - accuracy: 0.9972 - f1: 0.9963 - sensitivity: 0.9962 - specificity: 0.9995 - val_loss: 1.9736 - val_accuracy: 0.6683 - val_f1: 0.6776 - val_sensitivity: 0.6688 - val_specificity: 0.9565 - lr: 5.0000e-07\n",
            "Epoch 91/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9964 - f1: 0.9962 - sensitivity: 0.9961 - specificity: 0.9995\n",
            "Epoch 91: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 62ms/step - loss: 0.0138 - accuracy: 0.9964 - f1: 0.9962 - sensitivity: 0.9961 - specificity: 0.9995 - val_loss: 1.9784 - val_accuracy: 0.6677 - val_f1: 0.6747 - val_sensitivity: 0.6664 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 92/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0157 - accuracy: 0.9958 - f1: 0.9957 - sensitivity: 0.9955 - specificity: 0.9994\n",
            "Epoch 92: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 64ms/step - loss: 0.0162 - accuracy: 0.9955 - f1: 0.9950 - sensitivity: 0.9948 - specificity: 0.9993 - val_loss: 1.9755 - val_accuracy: 0.6689 - val_f1: 0.6770 - val_sensitivity: 0.6694 - val_specificity: 0.9560 - lr: 5.0000e-07\n",
            "Epoch 93/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9960 - f1: 0.9960 - sensitivity: 0.9958 - specificity: 0.9994\n",
            "Epoch 93: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 7s 68ms/step - loss: 0.0161 - accuracy: 0.9960 - f1: 0.9960 - sensitivity: 0.9958 - specificity: 0.9994 - val_loss: 1.9740 - val_accuracy: 0.6671 - val_f1: 0.6752 - val_sensitivity: 0.6658 - val_specificity: 0.9563 - lr: 5.0000e-07\n",
            "Epoch 94/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9964 - f1: 0.9961 - sensitivity: 0.9958 - specificity: 0.9995\n",
            "Epoch 94: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 8s 75ms/step - loss: 0.0128 - accuracy: 0.9964 - f1: 0.9961 - sensitivity: 0.9958 - specificity: 0.9995 - val_loss: 1.9889 - val_accuracy: 0.6696 - val_f1: 0.6765 - val_sensitivity: 0.6682 - val_specificity: 0.9562 - lr: 5.0000e-07\n",
            "Epoch 95/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9958 - f1: 0.9956 - sensitivity: 0.9953 - specificity: 0.9994\n",
            "Epoch 95: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 7s 65ms/step - loss: 0.0134 - accuracy: 0.9958 - f1: 0.9957 - sensitivity: 0.9954 - specificity: 0.9994 - val_loss: 1.9784 - val_accuracy: 0.6677 - val_f1: 0.6773 - val_sensitivity: 0.6682 - val_specificity: 0.9565 - lr: 5.0000e-07\n",
            "Epoch 96/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9959 - f1: 0.9958 - sensitivity: 0.9956 - specificity: 0.9994\n",
            "Epoch 96: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 54ms/step - loss: 0.0139 - accuracy: 0.9956 - f1: 0.9952 - sensitivity: 0.9949 - specificity: 0.9994 - val_loss: 1.9754 - val_accuracy: 0.6652 - val_f1: 0.6737 - val_sensitivity: 0.6652 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 97/100\n",
            "100/101 [============================>.] - ETA: 0s - loss: 0.0173 - accuracy: 0.9948 - f1: 0.9950 - sensitivity: 0.9947 - specificity: 0.9993\n",
            "Epoch 97: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 62ms/step - loss: 0.0173 - accuracy: 0.9949 - f1: 0.9950 - sensitivity: 0.9947 - specificity: 0.9993 - val_loss: 1.9704 - val_accuracy: 0.6696 - val_f1: 0.6754 - val_sensitivity: 0.6658 - val_specificity: 0.9564 - lr: 5.0000e-07\n",
            "Epoch 98/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9963 - f1: 0.9962 - sensitivity: 0.9958 - specificity: 0.9995\n",
            "Epoch 98: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 5s 54ms/step - loss: 0.0130 - accuracy: 0.9963 - f1: 0.9962 - sensitivity: 0.9958 - specificity: 0.9995 - val_loss: 1.9671 - val_accuracy: 0.6683 - val_f1: 0.6750 - val_sensitivity: 0.6664 - val_specificity: 0.9561 - lr: 5.0000e-07\n",
            "Epoch 99/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9953 - f1: 0.9955 - sensitivity: 0.9954 - specificity: 0.9994\n",
            "Epoch 99: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 64ms/step - loss: 0.0149 - accuracy: 0.9953 - f1: 0.9955 - sensitivity: 0.9954 - specificity: 0.9994 - val_loss: 1.9760 - val_accuracy: 0.6658 - val_f1: 0.6739 - val_sensitivity: 0.6652 - val_specificity: 0.9559 - lr: 5.0000e-07\n",
            "Epoch 100/100\n",
            "101/101 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9964 - f1: 0.9961 - sensitivity: 0.9958 - specificity: 0.9995\n",
            "Epoch 100: val_loss did not improve from 1.28168\n",
            "101/101 [==============================] - 6s 55ms/step - loss: 0.0134 - accuracy: 0.9964 - f1: 0.9961 - sensitivity: 0.9958 - specificity: 0.9995 - val_loss: 1.9838 - val_accuracy: 0.6665 - val_f1: 0.6740 - val_sensitivity: 0.6658 - val_specificity: 0.9559 - lr: 5.0000e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EMIwaVENpaBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('------------Training time is seconds:%s',time.time()-start)\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "\n",
        "#history keys and values\n",
        "print(history.history.keys())\n",
        "print(history.history.values())\n",
        "\n",
        "#Metrics for testing\n",
        "print(scores)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "print('Test f1:',scores[2])\n",
        "print('Test sensitivity:',scores[3])\n",
        "print('Test specificity:',scores[4])\n",
        "print(\"Max Test accuracy\", max(history.history['val_accuracy']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1zyGGAAFwJx",
        "outputId": "02760e4c-6cc8-461b-ab53-1f4bc6e84016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------Training time is seconds:%s 639.5264360904694\n",
            "51/51 [==============================] - 1s 14ms/step - loss: 1.9838 - accuracy: 0.6665 - f1: 0.6702 - sensitivity: 0.6620 - specificity: 0.9554\n",
            "dict_keys(['loss', 'accuracy', 'f1', 'sensitivity', 'specificity', 'val_loss', 'val_accuracy', 'val_f1', 'val_sensitivity', 'val_specificity', 'lr'])\n",
            "dict_values([[2.2308740615844727, 1.678444743156433, 1.8436875343322754, 1.1823880672454834, 0.8984947204589844, 0.6553383469581604, 0.5230430364608765, 0.21110355854034424, 0.10484760254621506, 0.06273987889289856, 0.04644503444433212, 0.03689739108085632, 0.031567174941301346, 0.027706464752554893, 0.023647209629416466, 0.027841966599225998, 0.019263980910182, 0.03221552073955536, 0.021648282185196877, 0.0276666060090065, 0.02064565196633339, 0.02647976018488407, 0.021669210866093636, 0.021545320749282837, 0.02477939799427986, 0.019284304231405258, 0.019447172060608864, 0.018026279285550117, 0.022492239251732826, 0.016869578510522842, 0.02015579864382744, 0.022003021091222763, 0.02139558084309101, 0.019190514460206032, 0.02340889163315296, 0.02522466331720352, 0.025387214496731758, 0.01844312623143196, 0.01849425956606865, 0.0236325953155756, 0.01578517071902752, 0.019041582942008972, 0.01974964514374733, 0.015369324013590813, 0.023402396589517593, 0.014433512464165688, 0.016473619267344475, 0.016232989728450775, 0.020051494240760803, 0.014669130556285381, 0.01802085153758526, 0.01784995011985302, 0.018797926604747772, 0.01730470173060894, 0.015035755932331085, 0.016099929809570312, 0.014372210949659348, 0.015225728042423725, 0.02066703699529171, 0.016448214650154114, 0.020382188260555267, 0.014569049701094627, 0.016028396785259247, 0.014252929948270321, 0.010813136585056782, 0.01896616257727146, 0.015777137130498886, 0.015876611694693565, 0.015729857608675957, 0.016369668766856194, 0.017651392146945, 0.01851147972047329, 0.012756221927702427, 0.01821543462574482, 0.01442507654428482, 0.014443822205066681, 0.016574112698435783, 0.0163104385137558, 0.017266983166337013, 0.014298622496426105, 0.013702482916414738, 0.012994498945772648, 0.012184567749500275, 0.01782846264541149, 0.015894675627350807, 0.014187143184244633, 0.018484745174646378, 0.01578764244914055, 0.01299020741134882, 0.012011184357106686, 0.013756879605352879, 0.016221830621361732, 0.016083940863609314, 0.012787450104951859, 0.01338426023721695, 0.0138666070997715, 0.017252780497074127, 0.013004505075514317, 0.014877171255648136, 0.013391704298555851], [0.3135698735713959, 0.4828820526599884, 0.4069405496120453, 0.5963274240493774, 0.7002801299095154, 0.77606600522995, 0.826019287109375, 0.9326174855232239, 0.9656084775924683, 0.9824151992797852, 0.9852163195610046, 0.9892623424530029, 0.9915966391563416, 0.9926859736442566, 0.9923747181892395, 0.9915966391563416, 0.9945533871650696, 0.9906629323959351, 0.994397759437561, 0.9934640526771545, 0.9937753081321716, 0.992530345916748, 0.9939308762550354, 0.9926859736442566, 0.9929971694946289, 0.9948645830154419, 0.994086503982544, 0.995175838470459, 0.9936196804046631, 0.9948645830154419, 0.9934640526771545, 0.9942421317100525, 0.9939308762550354, 0.9950202107429504, 0.9931527972221375, 0.9937753081321716, 0.9929971694946289, 0.9959539175033569, 0.9947090148925781, 0.9928416013717651, 0.9968876242637634, 0.9942421317100525, 0.994397759437561, 0.9961095452308655, 0.9947090148925781, 0.9968876242637634, 0.9947090148925781, 0.9959539175033569, 0.994397759437561, 0.9967319965362549, 0.9948645830154419, 0.995175838470459, 0.9959539175033569, 0.9942421317100525, 0.9959539175033569, 0.9961095452308655, 0.9965764284133911, 0.9953314661979675, 0.9936196804046631, 0.9954870939254761, 0.9942421317100525, 0.9957982897758484, 0.9953314661979675, 0.9959539175033569, 0.997043251991272, 0.9954870939254761, 0.9956427216529846, 0.9953314661979675, 0.9956427216529846, 0.9961095452308655, 0.9950202107429504, 0.9934640526771545, 0.9957982897758484, 0.9947090148925781, 0.996265172958374, 0.996265172958374, 0.9948645830154419, 0.9957982897758484, 0.995175838470459, 0.996265172958374, 0.9971988797187805, 0.9965764284133911, 0.997043251991272, 0.995175838470459, 0.9953314661979675, 0.9961095452308655, 0.9945533871650696, 0.9967319965362549, 0.9961095452308655, 0.9971988797187805, 0.9964208006858826, 0.9954870939254761, 0.9959539175033569, 0.9964208006858826, 0.9957982897758484, 0.9956427216529846, 0.9948645830154419, 0.996265172958374, 0.9953314661979675, 0.9964208006858826], [0.13605333864688873, 0.3894955515861511, 0.2854895293712616, 0.5506069660186768, 0.6859599351882935, 0.7727499604225159, 0.8257584571838379, 0.933205783367157, 0.9655802249908447, 0.9819615483283997, 0.9856398105621338, 0.9895464181900024, 0.9914771914482117, 0.9918986558914185, 0.9918119311332703, 0.9912555813789368, 0.9939663410186768, 0.9898717403411865, 0.9941871762275696, 0.9931821227073669, 0.9937578439712524, 0.9924236536026001, 0.9933784604072571, 0.9928566217422485, 0.9930400848388672, 0.9939088225364685, 0.9938136339187622, 0.9951127767562866, 0.9927401542663574, 0.9953381419181824, 0.9934262037277222, 0.99440997838974, 0.9933649301528931, 0.9948173761367798, 0.9932589530944824, 0.9927505850791931, 0.992902934551239, 0.9956923127174377, 0.9946656227111816, 0.9920473694801331, 0.9966659545898438, 0.9947291612625122, 0.9946450591087341, 0.9959015250205994, 0.9943471550941467, 0.9967415928840637, 0.9947364926338196, 0.9958131909370422, 0.9940377473831177, 0.9966708421707153, 0.9945924878120422, 0.9946656227111816, 0.9958173632621765, 0.9943405985832214, 0.9955255389213562, 0.9960027933120728, 0.9963632225990295, 0.9955075979232788, 0.9938033223152161, 0.9947673678398132, 0.9941128492355347, 0.994843602180481, 0.9955794215202332, 0.9961312413215637, 0.9968321323394775, 0.9952753782272339, 0.9957426190376282, 0.9952059388160706, 0.9955957531929016, 0.9955235123634338, 0.994883120059967, 0.993354856967926, 0.9958119988441467, 0.9943830966949463, 0.9960831999778748, 0.9959830641746521, 0.995121419429779, 0.9958985447883606, 0.9953516721725464, 0.9962145090103149, 0.9972129464149475, 0.9958354234695435, 0.9966060519218445, 0.9955062866210938, 0.9950565099716187, 0.996054470539093, 0.9948119521141052, 0.9963619709014893, 0.9960513114929199, 0.9963205456733704, 0.9962121248245239, 0.9949823021888733, 0.9959753155708313, 0.9960627555847168, 0.9956523776054382, 0.9952345490455627, 0.9950434565544128, 0.9962042570114136, 0.9955026507377625, 0.9961292743682861], [0.08684787154197693, 0.2830469310283661, 0.1918673813343048, 0.44246238470077515, 0.6047338843345642, 0.7195711135864258, 0.789794385433197, 0.9198876619338989, 0.9601104259490967, 0.979269802570343, 0.9838395118713379, 0.9884805679321289, 0.9904917478561401, 0.9908963441848755, 0.9914318323135376, 0.9908011555671692, 0.9933596849441528, 0.9885757565498352, 0.9935024976730347, 0.9924195408821106, 0.9929670095443726, 0.9923481941223145, 0.9929788708686829, 0.9922648668289185, 0.9926576018333435, 0.9935262799263, 0.9934310913085938, 0.9942759871482849, 0.9923600554466248, 0.9941213130950928, 0.9932764172554016, 0.9935024976730347, 0.9923600554466248, 0.994204580783844, 0.9925742745399475, 0.9920625686645508, 0.9919673800468445, 0.9951328039169312, 0.99428790807724, 0.9915151596069336, 0.9962871074676514, 0.9939665794372559, 0.9939665794372559, 0.9955969452857971, 0.9937404990196228, 0.9961324334144592, 0.994585394859314, 0.9953588843345642, 0.9934310913085938, 0.9964418411254883, 0.9941332340240479, 0.99428790807724, 0.9955969452857971, 0.9936571717262268, 0.9955255389213562, 0.9952875375747681, 0.9959063529968262, 0.9950494766235352, 0.9933477640151978, 0.9942997694015503, 0.9935024976730347, 0.9945378303527832, 0.994894802570343, 0.9959776997566223, 0.9966798424720764, 0.99466872215271, 0.9955136179924011, 0.994978129863739, 0.9954422116279602, 0.9952993988990784, 0.994204580783844, 0.9930502772331238, 0.9953588843345642, 0.9939785003662109, 0.9951328039169312, 0.9959063529968262, 0.994585394859314, 0.9956682920455933, 0.994894802570343, 0.9960610270500183, 0.9969059228897095, 0.9954540729522705, 0.9964537620544434, 0.9950494766235352, 0.994597315788269, 0.9959776997566223, 0.994585394859314, 0.9960610270500183, 0.9957516193389893, 0.9961562156677246, 0.9960610270500183, 0.994752049446106, 0.9958230257034302, 0.9958349466323853, 0.9953588843345642, 0.9949067234992981, 0.9947401285171509, 0.9958230257034302, 0.9953588843345642, 0.9957516193389893], [0.981612503528595, 0.9779316186904907, 0.9824774861335754, 0.977652907371521, 0.9776375889778137, 0.9799036979675293, 0.9825984239578247, 0.9926760792732239, 0.9959468245506287, 0.9978338479995728, 0.998221755027771, 0.9986635446548462, 0.9989287853240967, 0.9989966750144958, 0.9988861680030823, 0.9988182187080383, 0.9992280602455139, 0.9987536668777466, 0.9992706179618835, 0.999137818813324, 0.9992263913154602, 0.9989287853240967, 0.9991174936294556, 0.9990717172622681, 0.9990615248680115, 0.9991854429244995, 0.999172031879425, 0.9994252920150757, 0.9990189075469971, 0.9995136857032776, 0.9990835189819336, 0.999336838722229, 0.9992059469223022, 0.9993489384651184, 0.9991379380226135, 0.9990751147270203, 0.9991277456283569, 0.9994693994522095, 0.9992944002151489, 0.9989423155784607, 0.999579906463623, 0.9993588328361511, 0.9993370175361633, 0.9994592666625977, 0.9992824792861938, 0.9996241927146912, 0.9992706775665283, 0.9994695782661438, 0.9992383718490601, 0.9995577931404114, 0.9992943406105042, 0.9992944002151489, 0.9994370937347412, 0.99929279088974, 0.9993607401847839, 0.9995357394218445, 0.9995477795600891, 0.9994252324104309, 0.9991821646690369, 0.9993283748626709, 0.9992485642433167, 0.9993079900741577, 0.999469518661499, 0.999469518661499, 0.999569833278656, 0.9994150996208191, 0.9994252920150757, 0.9993487596511841, 0.9993930459022522, 0.9993946552276611, 0.9993709325790405, 0.9990954995155334, 0.999469518661499, 0.999260425567627, 0.999579906463623, 0.9994372129440308, 0.9993810653686523, 0.9994473457336426, 0.999403178691864, 0.9994814395904541, 0.9996462464332581, 0.9994609355926514, 0.9995377063751221, 0.9994252920150757, 0.9993607401847839, 0.9994473457336426, 0.9992927312850952, 0.9995256066322327, 0.9994813799858093, 0.9995052218437195, 0.9994814991950989, 0.999316394329071, 0.9994473457336426, 0.9994712471961975, 0.9994252920150757, 0.999370813369751, 0.999336838722229, 0.9995136260986328, 0.9993811845779419, 0.9995035529136658], [2.016738176345825, 2.209285259246826, 2.986431121826172, 1.4749443531036377, 1.2816805839538574, 1.2903929948806763, 1.4466230869293213, 1.2974934577941895, 1.4576352834701538, 1.6137129068374634, 1.6790226697921753, 1.7642027139663696, 1.8104007244110107, 1.8197245597839355, 1.836870789527893, 1.8388590812683105, 1.8397470712661743, 1.8558987379074097, 1.8485147953033447, 1.8509323596954346, 1.854215383529663, 1.8461157083511353, 1.841597557067871, 1.857369065284729, 1.8583401441574097, 1.8454324007034302, 1.842369794845581, 1.84439218044281, 1.8567534685134888, 1.8642338514328003, 1.8686516284942627, 1.8698914051055908, 1.8634521961212158, 1.8634552955627441, 1.8689290285110474, 1.8703968524932861, 1.8743382692337036, 1.868563175201416, 1.8590987920761108, 1.8721026182174683, 1.8684906959533691, 1.8786650896072388, 1.879859447479248, 1.8847988843917847, 1.8898046016693115, 1.8834038972854614, 1.8922513723373413, 1.9039416313171387, 1.8997726440429688, 1.902557373046875, 1.9019285440444946, 1.9116188287734985, 1.9083642959594727, 1.9036426544189453, 1.9114632606506348, 1.9144279956817627, 1.9118207693099976, 1.9170429706573486, 1.9276705980300903, 1.906036138534546, 1.9232901334762573, 1.9291326999664307, 1.9210261106491089, 1.9326868057250977, 1.9322457313537598, 1.9366326332092285, 1.94778311252594, 1.9411439895629883, 1.932228446006775, 1.9339971542358398, 1.9469125270843506, 1.9341795444488525, 1.94922935962677, 1.942565679550171, 1.9342352151870728, 1.9441813230514526, 1.954153060913086, 1.9588481187820435, 1.9580919742584229, 1.9590575695037842, 1.958040475845337, 1.9596476554870605, 1.9676305055618286, 1.9639818668365479, 1.970988392829895, 1.9619430303573608, 1.9708280563354492, 1.973831057548523, 1.9637824296951294, 1.9736073017120361, 1.9784435033798218, 1.975493311882019, 1.9739974737167358, 1.9889484643936157, 1.978449821472168, 1.9753903150558472, 1.9704419374465942, 1.9671344757080078, 1.9759821891784668, 1.9838413000106812], [0.21095208823680878, 0.299315482378006, 0.3453640341758728, 0.4828873574733734, 0.558182954788208, 0.5724953413009644, 0.5880522727966309, 0.6440572738647461, 0.6614810228347778, 0.6639701128005981, 0.662725567817688, 0.6583696603775024, 0.6645923852920532, 0.6608587503433228, 0.6645923852920532, 0.6652146577835083, 0.6639701128005981, 0.6633478403091431, 0.6639701128005981, 0.6639701128005981, 0.662725567817688, 0.662725567817688, 0.662725567817688, 0.662725567817688, 0.6639701128005981, 0.6645923852920532, 0.662725567817688, 0.662725567817688, 0.6639701128005981, 0.6645923852920532, 0.6639701128005981, 0.6645923852920532, 0.6658369898796082, 0.6658369898796082, 0.6670815348625183, 0.6658369898796082, 0.6645923852920532, 0.6645923852920532, 0.6658369898796082, 0.6639701128005981, 0.6664592623710632, 0.6677038073539734, 0.6670815348625183, 0.6633478403091431, 0.6664592623710632, 0.6664592623710632, 0.6658369898796082, 0.6670815348625183, 0.6677038073539734, 0.6695706248283386, 0.6677038073539734, 0.6645923852920532, 0.6683260798454285, 0.6683260798454285, 0.6677038073539734, 0.6670815348625183, 0.6695706248283386, 0.6695706248283386, 0.6664592623710632, 0.6683260798454285, 0.6664592623710632, 0.6677038073539734, 0.6677038073539734, 0.6677038073539734, 0.6683260798454285, 0.6683260798454285, 0.6639701128005981, 0.6664592623710632, 0.6670815348625183, 0.6683260798454285, 0.6658369898796082, 0.6670815348625183, 0.6664592623710632, 0.6689483523368835, 0.6683260798454285, 0.6658369898796082, 0.6633478403091431, 0.6664592623710632, 0.6677038073539734, 0.6677038073539734, 0.6677038073539734, 0.6652146577835083, 0.6670815348625183, 0.6652146577835083, 0.6714374423027039, 0.6695706248283386, 0.6670815348625183, 0.6670815348625183, 0.6658369898796082, 0.6683260798454285, 0.6677038073539734, 0.6689483523368835, 0.6670815348625183, 0.6695706248283386, 0.6677038073539734, 0.6652146577835083, 0.6695706248283386, 0.6683260798454285, 0.6658369898796082, 0.6664592623710632], [0.0, 0.22839237749576569, 0.29046571254730225, 0.42671263217926025, 0.5170124769210815, 0.558340311050415, 0.5911662578582764, 0.6636350154876709, 0.6707354784011841, 0.6750665307044983, 0.6702178716659546, 0.6667064428329468, 0.6709363460540771, 0.6700617671012878, 0.6734434962272644, 0.6700559854507446, 0.6698497533798218, 0.6700165867805481, 0.6725633144378662, 0.6707033514976501, 0.6709944009780884, 0.6700887680053711, 0.6698659658432007, 0.6696916818618774, 0.6719435453414917, 0.6724901795387268, 0.6710379719734192, 0.670106053352356, 0.6712777614593506, 0.6716741919517517, 0.6716517806053162, 0.6732978224754333, 0.6744699478149414, 0.6751455664634705, 0.675349235534668, 0.6730401515960693, 0.6732736825942993, 0.6732334494590759, 0.674674391746521, 0.6740165948867798, 0.6740347743034363, 0.6752569675445557, 0.6760047674179077, 0.6736446022987366, 0.6756633520126343, 0.6745041608810425, 0.6746776103973389, 0.6753568053245544, 0.6750730276107788, 0.6760746836662292, 0.6762815117835999, 0.6738570928573608, 0.6765149831771851, 0.675959587097168, 0.6767764687538147, 0.6755685210227966, 0.6757382750511169, 0.6786226034164429, 0.6741195917129517, 0.6769457459449768, 0.6749630570411682, 0.6759580969810486, 0.675981879234314, 0.6745140552520752, 0.6753016710281372, 0.6743617653846741, 0.6733028888702393, 0.6743411421775818, 0.6752978563308716, 0.6744677424430847, 0.6734750270843506, 0.6751136779785156, 0.6769184470176697, 0.6757944822311401, 0.6757646203041077, 0.6738720536231995, 0.6730777025222778, 0.6741927862167358, 0.6743900775909424, 0.6728525161743164, 0.6734780073165894, 0.6724520325660706, 0.67573481798172, 0.6735655069351196, 0.6762078404426575, 0.6761616468429565, 0.6743600368499756, 0.6750112771987915, 0.6747279763221741, 0.6776459813117981, 0.6746891736984253, 0.676958441734314, 0.675167977809906, 0.6765487194061279, 0.6773443222045898, 0.6737254858016968, 0.6753532886505127, 0.675026535987854, 0.6739104986190796, 0.6740388870239258], [0.0, 0.16286057233810425, 0.21239696443080902, 0.3247767984867096, 0.41303226351737976, 0.48523351550102234, 0.5515109896659851, 0.6309238076210022, 0.6471497416496277, 0.6543613076210022, 0.6537603139877319, 0.6525583863258362, 0.6603708863258362, 0.6591689586639404, 0.6621738076210022, 0.6597699522972107, 0.6597699522972107, 0.6591689586639404, 0.663375735282898, 0.6609718799591064, 0.6609718799591064, 0.6597699522972107, 0.6609718799591064, 0.6603708863258362, 0.6627747416496277, 0.663375735282898, 0.6621738076210022, 0.6615728139877319, 0.6621738076210022, 0.6627747416496277, 0.6621738076210022, 0.6645776033401489, 0.6651785969734192, 0.6657795310020447, 0.6657795310020447, 0.663375735282898, 0.6627747416496277, 0.6621738076210022, 0.6639766693115234, 0.6639766693115234, 0.6639766693115234, 0.6657795310020447, 0.6657795310020447, 0.6645776033401489, 0.6651785969734192, 0.6651785969734192, 0.6651785969734192, 0.6657795310020447, 0.6645776033401489, 0.6657795310020447, 0.6669814586639404, 0.6645776033401489, 0.6675824522972107, 0.6657795310020447, 0.6687843799591064, 0.6675824522972107, 0.6663805246353149, 0.6693853139877319, 0.6639766693115234, 0.6669814586639404, 0.6657795310020447, 0.6657795310020447, 0.6663805246353149, 0.6651785969734192, 0.6657795310020447, 0.6651785969734192, 0.6645776033401489, 0.6651785969734192, 0.6663805246353149, 0.6657795310020447, 0.6651785969734192, 0.6669814586639404, 0.6687843799591064, 0.6675824522972107, 0.6669814586639404, 0.6663805246353149, 0.6657795310020447, 0.6663805246353149, 0.6657795310020447, 0.663375735282898, 0.6645776033401489, 0.663375735282898, 0.6681833863258362, 0.6663805246353149, 0.6687843799591064, 0.6675824522972107, 0.6663805246353149, 0.6669814586639404, 0.6663805246353149, 0.6687843799591064, 0.6663805246353149, 0.6693853139877319, 0.6657795310020447, 0.6681833863258362, 0.6681833863258362, 0.6651785969734192, 0.6657795310020447, 0.6663805246353149, 0.6651785969734192, 0.6657795310020447], [1.0, 0.9634761810302734, 0.9659906029701233, 0.9724293947219849, 0.9742322564125061, 0.9640772342681885, 0.9554185271263123, 0.9616120457649231, 0.9596497416496277, 0.9592205882072449, 0.9576628804206848, 0.9565468430519104, 0.956117570400238, 0.9560317993164062, 0.956632673740387, 0.9558600783348083, 0.9557740092277527, 0.9560316801071167, 0.9558600187301636, 0.955774188041687, 0.9558600187301636, 0.9558601379394531, 0.9554306864738464, 0.9555165767669678, 0.9557742476463318, 0.9558600783348083, 0.9556025862693787, 0.9553449153900146, 0.9556883573532104, 0.9556884169578552, 0.9558600783348083, 0.9558600783348083, 0.9562035202980042, 0.9562892913818359, 0.9563751816749573, 0.9561176896095276, 0.956375241279602, 0.9565470218658447, 0.9566327333450317, 0.9563751816749573, 0.9563751220703125, 0.9563751816749573, 0.9566327333450317, 0.9560317993164062, 0.9567185044288635, 0.9562034606933594, 0.9562892913818359, 0.9563751816749573, 0.9566327333450317, 0.9567185640335083, 0.9564610719680786, 0.9561176300048828, 0.9563751816749573, 0.9566327333450317, 0.9561176896095276, 0.9559460282325745, 0.9563751816749573, 0.9567185044288635, 0.9563750624656677, 0.9567185044288635, 0.9562034606933594, 0.9566327333450317, 0.9564610719680786, 0.9562034606933594, 0.9563750624656677, 0.956117570400238, 0.9558600187301636, 0.956117570400238, 0.9562034606933594, 0.9560316801071167, 0.9557740688323975, 0.9559459090232849, 0.9562034010887146, 0.9560316801071167, 0.9562034606933594, 0.9556023478507996, 0.9554306864738464, 0.9556882381439209, 0.9559459090232849, 0.9560317397117615, 0.9559459090232849, 0.9558600187301636, 0.9558600187301636, 0.9554307460784912, 0.9558600783348083, 0.9562034010887146, 0.955774188041687, 0.9558600187301636, 0.9559458494186401, 0.9564610123634338, 0.9559458494186401, 0.9560317993164062, 0.9562892913818359, 0.9562034010887146, 0.9565468430519104, 0.955859899520874, 0.9563750624656677, 0.9561175107955933, 0.9559457302093506, 0.955859899520874], [0.001, 0.001, 0.001, 0.0003162278, 0.0003162278, 0.0003162278, 0.0003162278, 0.000100000005, 0.000100000005, 3.1622778e-05, 3.1622778e-05, 1.0000001e-05, 1.0000001e-05, 3.1622778e-06, 3.1622778e-06, 1.0000001e-06, 1.0000001e-06, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07, 5e-07]])\n",
            "[1.983841896057129, 0.6664592623710632, 0.6702178716659546, 0.6620272994041443, 0.955394446849823]\n",
            "Test loss: 1.983841896057129\n",
            "Test accuracy: 0.6664592623710632\n",
            "Test f1: 0.6702178716659546\n",
            "Test sensitivity: 0.6620272994041443\n",
            "Test specificity: 0.955394446849823\n",
            "Max Test accuracy 0.6714374423027039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# summarize history for accuracy\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig(METRICS_PATH+MODEL_NAME+\"_acc.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "reepHdo4LBXt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "7d014ec7-c43f-4c2e-9e7a-d25054ba6204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for loss\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig(METRICS_PATH+MODEL_NAME+\"_loss.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2w5C3a5RLBes",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "3628defe-e2a4-4951-edfd-ba7cf9f8c7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WSp3RqLpNUa1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}